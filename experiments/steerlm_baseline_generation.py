# steerlm_nemotron_baseline.py - ä½¿ç”¨Nemotronæ¨¡å‹è¿›è¡Œå®éªŒ
import os
import numpy as np
import pandas as pd
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification
from tqdm.auto import tqdm
import torch
import time
import random
import json

# =============================================================================
# ğŸŒ ç½‘ç»œé…ç½® - æ”¯æŒå›½å†…é•œåƒ
# =============================================================================

# å›½å†…é•œåƒé€‰é¡¹
MIRROR_OPTIONS = {
    "official": {
        "hf_endpoint": None,
        "name": "Official HuggingFace",
        "description": "ç›´æ¥è®¿é—®å®˜æ–¹HuggingFace"
    },
    "hf_mirror": {
        "hf_endpoint": "https://hf-mirror.com", 
        "name": "HF Mirror",
        "description": "å›½å†…HFé•œåƒ (æ¨è)"
    },
    "modelfun": {
        "hf_endpoint": "https://www.modelfun.cn",
        "name": "ModelFun",
        "description": "æ¨¡å‹ä¹å›­é•œåƒ"
    }
}

def setup_mirror(mirror_choice="hf_mirror"):
    """è®¾ç½®é•œåƒé…ç½®"""
    if mirror_choice in MIRROR_OPTIONS:
        mirror = MIRROR_OPTIONS[mirror_choice]
        if mirror["hf_endpoint"]:
            os.environ['HF_ENDPOINT'] = mirror["hf_endpoint"]
        elif 'HF_ENDPOINT' in os.environ:
            del os.environ['HF_ENDPOINT']
        
        print(f"ğŸŒ ä½¿ç”¨é•œåƒ: {mirror['name']} - {mirror['description']}")
        return mirror["name"]
    else:
        print(f"âŒ æœªçŸ¥é•œåƒé€‰æ‹©: {mirror_choice}")
        return "Unknown"

# è®¾ç½®ç½‘ç»œè¶…æ—¶å’Œé‡è¯•
os.environ['HF_HUB_DOWNLOAD_TIMEOUT'] = '300'
os.environ['TRANSFORMERS_CACHE'] = '/root/.cache/huggingface'

# é»˜è®¤ä½¿ç”¨å›½å†…é•œåƒ
current_mirror = setup_mirror("hf_mirror")  # å¯æ”¹ä¸º "official" æˆ– "modelfun"

print("ğŸŒ Network configuration:")
print(f"Current Mirror: {current_mirror}")
print(f"HF_ENDPOINT: {os.environ.get('HF_ENDPOINT', 'Official HuggingFace')}")
print(f"Cache dir: {os.environ.get('TRANSFORMERS_CACHE', 'Default')}")

# =============================================================================

# å®šä¹‰æ–¹å‘å‘é‡
PREFERENCE_DIRECTIONS = {
    "v3": {"vector": (0.9848, 0.1736), "angle": 10},
    "v4": {"vector": (0.9659, 0.2588), "angle": 15},
    "v5": {"vector": (0.9397, 0.3420), "angle": 20},
    "v6": {"vector": (0.9063, 0.4226), "angle": 25},
    "v7": {"vector": (0.8660, 0.5000), "angle": 30},
    "v8": {"vector": (0.8192, 0.5736), "angle": 35},
    "v9": {"vector": (0.7660, 0.6428), "angle": 40},
    "v10": {"vector": (0.7071, 0.7071), "angle": 45},
}

def load_nemotron_steerlm(device):
    """åŠ è½½Nemotronæ¨¡å‹ - ä½¿ç”¨å¯ç”¨çš„HuggingFaceæ ¼å¼ç‰ˆæœ¬"""
    print("ğŸ¤– Loading Nemotron model...")
    
    # ä½¿ç”¨å¯ç”¨çš„Nemotronæ¨¡å‹
    model_options = [
        "nvidia/nemotron-3-8b-base-4k-hf",  # Base model - HFæ ¼å¼
        "nvidia/Llama-3.1-Nemotron-Nano-8B-v1",  # æ–°çš„Nemotronæ¨¡å‹
        "nvidia/Nemotron-4-340B-Instruct"  # æ›´å¤§çš„æ¨¡å‹
    ]
    
    for model_name in model_options:
        try:
            print(f"ğŸ”„ å°è¯•åŠ è½½æ¨¡å‹: {model_name}")
            
            # å°è¯•åŠ è½½æ¨¡å‹
            steerlm_model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=torch.float16,
                device_map="auto",
                trust_remote_code=True,
                cache_dir="/root/.cache/huggingface",
                force_download=False,
                resume_download=True
            )
            steerlm_tokenizer = AutoTokenizer.from_pretrained(
                model_name,
                cache_dir="/root/.cache/huggingface",
                force_download=False,
                resume_download=True
            )
            
            # è®¾ç½®pad_token
            if steerlm_tokenizer.pad_token is None:
                steerlm_tokenizer.pad_token = steerlm_tokenizer.eos_token
            
            print(f"âœ… æˆåŠŸåŠ è½½æ¨¡å‹: {model_name}")
            break
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹ {model_name} åŠ è½½å¤±è´¥: {str(e)[:100]}...")
            continue
    
    else:
        raise Exception("æ‰€æœ‰æ¨¡å‹éƒ½åŠ è½½å¤±è´¥")
    
    # åŠ è½½rewardæ¨¡å‹
    print("ğŸ† Loading reward model...")
    try:
        reward_model = AutoModelForSequenceClassification.from_pretrained(
            "Haoxiang-Wang/RewardModel-Mistral-7B-for-DPA-v1", 
            trust_remote_code=True,
            cache_dir="/root/.cache/huggingface"
        ).to(device)
        reward_tokenizer = AutoTokenizer.from_pretrained(
            "Haoxiang-Wang/RewardModel-Mistral-7B-for-DPA-v1",
            cache_dir="/root/.cache/huggingface"
        )
        print("âœ… Reward model loaded successfully!")
    except Exception as e:
        print(f"âŒ Failed to load reward model: {e}")
        reward_model, reward_tokenizer = None, None
    
    return steerlm_model, steerlm_tokenizer, reward_model, reward_tokenizer

def dpa_vector_to_nemotron_attributes(v1, v2):
    """æ˜ å°„DPAå‘é‡åˆ°Nemotronå±æ€§æ ¼å¼"""
    helpfulness = max(0, min(4, round(v1 * 4)))
    verbosity = max(0, min(4, round(v2 * 4)))
    
    return {
        "quality": 4,
        "understanding": 4,
        "correctness": 4,
        "coherence": 4,
        "complexity": 4,
        "verbosity": verbosity,
        "toxicity": 0,
        "humor": 0,
        "creativity": 0,
        "violence": 0,
        "helpfulness": helpfulness,
        "not_appropriate": 0,
        "hate_speech": 0,
        "sexual_content": 0,
        "fails_task": 0,
        "political_content": 0,
        "moral_judgement": 0,
        "lang": "en"
    }

def build_nemotron_prompt(prompt, v1, v2, model_name=""):
    """æ„å»ºé€‚é…ä¸åŒNemotronæ¨¡å‹çš„promptæ ¼å¼"""
    
    # åˆ¤æ–­æ¨¡å‹ç±»å‹å¹¶æ„å»ºç›¸åº”æ ¼å¼çš„prompt
    if "steerlm" in model_name.lower():
        # SteerLMæ ¼å¼ (å¦‚æœæœ‰çš„è¯)
        attrs = dpa_vector_to_nemotron_attributes(v1, v2)
    attr_string = f"quality:{attrs['quality']},understanding:{attrs['understanding']},correctness:{attrs['correctness']},coherence:{attrs['coherence']},complexity:{attrs['complexity']},verbosity:{attrs['verbosity']},toxicity:{attrs['toxicity']},humor:{attrs['humor']},creativity:{attrs['creativity']},violence:{attrs['violence']},helpfulness:{attrs['helpfulness']},not_appropriate:{attrs['not_appropriate']},hate_speech:{attrs['hate_speech']},sexual_content:{attrs['sexual_content']},fails_task:{attrs['fails_task']},political_content:{attrs['political_content']},moral_judgement:{attrs['moral_judgement']},lang:{attrs['lang']}"
    
    nemotron_prompt = f"""<extra_id_0>System
A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.

<extra_id_1>User
{prompt}
<extra_id_1>Assistant
<extra_id_2>{attr_string}
"""
    return nemotron_prompt, attr_string

    elif "llama" in model_name.lower():
        # Llama-Nemotronæ ¼å¼
        # ä½¿ç”¨reasoning on/offæ¥æ§åˆ¶è¯¦ç»†ç¨‹åº¦
        reasoning_mode = "on" if v1 > 0.5 else "off"
        
        nemotron_prompt = f"""<|start_header_id|>system<|end_header_id|>

detailed thinking {reasoning_mode}<|eot_id|><|start_header_id|>user<|end_header_id|>

{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>

"""
        return nemotron_prompt, f"reasoning={reasoning_mode}"
        
    else:
        # æ ‡å‡†Nemotronæ ¼å¼
        nemotron_prompt = f"""<extra_id_0>System
A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.

<extra_id_1>User
{prompt}
<extra_id_1>Assistant
"""
        return nemotron_prompt, "standard_format"

def generate_responses_for_direction(prompts, v1, v2, steerlm_model, steerlm_tokenizer, device, direction_name, model_name=""):
    """ä¸ºæŒ‡å®šæ–¹å‘ç”Ÿæˆå“åº”"""
    print(f"ğŸ¯ Generating responses for {direction_name} (v1={v1:.4f}, v2={v2:.4f})")
    
    results = []
    
    for i, prompt in enumerate(tqdm(prompts, desc=f"Generating {direction_name}")):
        prompt_results = []
        
        # ä¸ºæ¯ä¸ªpromptç”Ÿæˆ3ä¸ªå“åº”
        for sample_id in range(3):
            try:
                nemotron_prompt, attr_string = build_nemotron_prompt(prompt, v1, v2, model_name)
                
                # ç”Ÿæˆå“åº”
                inputs = steerlm_tokenizer(nemotron_prompt, return_tensors="pt", truncation=True, max_length=3072).to(device)
                
                with torch.no_grad():
                    outputs = steerlm_model.generate(
                        **inputs,
                        max_new_tokens=512,
                        temperature=0.7,
                        do_sample=True,
                        pad_token_id=steerlm_tokenizer.pad_token_id
                    )
                
                response = steerlm_tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
                
                # æ¸…ç†è¾“å‡ºï¼ˆç§»é™¤ç‰¹æ®Šæ ‡è®°ï¼‰
                if "llama" in model_name.lower():
                    # Llamaæ ¼å¼æ¸…ç†
                    response = response.split("<|eot_id|>")[0].strip()
                else:
                    # Nemotronæ ¼å¼æ¸…ç†
                response = response.split("<extra_id_1>")[0].strip()
                
                prompt_results.append({
                    "prompt_id": i,
                    "sample_id": sample_id,
                    "prompt": prompt,
                    "response": response,
                    "direction": direction_name,
                    "v1": v1,
                    "v2": v2,
                    "attributes": attr_string,
                    "model_name": model_name
                })
                
            except Exception as e:
                print(f"âŒ Error generating response for prompt {i}, sample {sample_id}: {e}")
                prompt_results.append({
                    "prompt_id": i,
                    "sample_id": sample_id,
                    "prompt": prompt,
                    "response": f"ERROR: {str(e)}",
                    "direction": direction_name,
                    "v1": v1,
                    "v2": v2,
                    "attributes": "ERROR",
                    "model_name": model_name
                })
        
        results.extend(prompt_results)
    
    return results

def main():
    """ä¸»å‡½æ•° - æµ‹è¯•Nemotronæ¨¡å‹"""
    print("ğŸš€ Starting Nemotron experiment!")
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # æ ¹æ®æ˜¾å­˜è®¾ç½®batch_size
    if torch.cuda.is_available():
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3  # GB
        if gpu_memory >= 80:  # A100 80GB
            batch_size = 4
        elif gpu_memory >= 48:  # L40S 48GB
            batch_size = 2
        elif gpu_memory >= 24:  # RTX 4090ç­‰
            batch_size = 1
        else:
            batch_size = 1
        print(f"ğŸš€ GPU Memory: {gpu_memory:.1f}GB, using batch_size={batch_size}")
    else:
        batch_size = 1
        print("ğŸ’» Using CPU, batch_size=1")
    
    # åŠ è½½æ•°æ®
    print("ğŸ“¦ Loading UltraFeedback dataset...")
    ds = load_dataset("HuggingFaceH4/ultrafeedback_binarized", split="test_prefs")
    prompts = ds["prompt"][:10]  # å…ˆæµ‹è¯•10ä¸ª
    
    # åŠ è½½æ¨¡å‹
    try:
        steerlm_model, steerlm_tokenizer, reward_model, reward_tokenizer = load_nemotron_steerlm(device)
        # è·å–å®é™…åŠ è½½çš„æ¨¡å‹åç§°
        model_name = steerlm_model.config.name_or_path if hasattr(steerlm_model.config, 'name_or_path') else "unknown"
        print(f"ğŸ“ ä½¿ç”¨æ¨¡å‹: {model_name}")
    except Exception as e:
        print("ğŸ’¡ æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œå¯èƒ½çš„è§£å†³æ–¹æ¡ˆ:")
        print("1. æ£€æŸ¥ç½‘ç»œè¿æ¥")
        print("2. ç¡®ä¿å·²ç™»å½•HuggingFace: huggingface-cli login")
        print("3. è®¿é—®æ¨¡å‹é¡µé¢æ¥å—è®¸å¯è¯")
        return
    
    # æµ‹è¯•ä¸€ä¸ªprompt
    test_prompt = prompts[0]
    v1, v2 = 0.8, 0.4  # é«˜helpfulnessï¼Œä¸­ç­‰verbosity
    
    print(f"\nğŸ§ª Testing with prompt: {test_prompt[:100]}...")
    print(f"ğŸ¯ DPA vector: ({v1}, {v2})")
    
    nemotron_prompt, attr_string = build_nemotron_prompt(test_prompt, v1, v2, model_name)
    
    print(f"\nğŸ“ Generated prompt:")
    print(f"```\n{nemotron_prompt}\n```")
    
    # ç”Ÿæˆå“åº”
    inputs = steerlm_tokenizer(nemotron_prompt, return_tensors="pt", truncation=True, max_length=3072).to(device)
    
    with torch.no_grad():
        outputs = steerlm_model.generate(
            **inputs,
            max_new_tokens=512,
            temperature=0.7,
            do_sample=True,
            pad_token_id=steerlm_tokenizer.pad_token_id
        )
    
    response = steerlm_tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
    
    # æ¸…ç†è¾“å‡º
    if "llama" in model_name.lower():
        response = response.split("<|eot_id|>")[0].strip()
    else:
    response = response.split("<extra_id_1>")[0].strip()
    
    print(f"\nğŸ¯ Model Response:")
    print(f"```\n{response}\n```")
    
    print(f"\nâœ… Nemotron model test successful!")
    print(f"ğŸ“Š Control attributes: {attr_string}")
    print(f"ğŸ”§ Model used: {model_name}")

if __name__ == "__main__":
    main()