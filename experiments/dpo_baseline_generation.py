# dpo_response_generator.py
# DPOæ¨¡å‹åœ¨UltraFeedbackæ•°æ®é›†ä¸Šä½¿ç”¨DPAæ–¹å‘ç”Ÿæˆå“åº”å¹¶é€‰æ‹©æœ€ä½³å›ç­”

import os
import numpy as np
import pandas as pd
import math
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification
from tqdm.auto import tqdm
import torch
import time
import random

# ğŸ‡¨ğŸ‡³ è®¾ç½®å›½å†…é•œåƒï¼Œè§£å†³ç½‘ç»œè®¿é—®é—®é¢˜
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
# ğŸ”„ ä¿®æ”¹ç¼“å­˜ç›®å½•åˆ°æ•°æ®ç›˜ - ä½¿ç”¨çœŸæ­£çš„æ•°æ®ç›˜è·¯å¾„
os.environ['HUGGINGFACE_HUB_CACHE'] = '/root/autodl-tmp/hf_cache'
os.environ['HF_HOME'] = '/root/autodl-tmp/hf_cache'
os.environ['TRANSFORMERS_CACHE'] = '/root/autodl-tmp/hf_cache'
os.environ['HF_DATASETS_CACHE'] = '/root/autodl-tmp/hf_cache'

print("ğŸŒ å·²è®¾ç½®Hugging Faceå›½å†…é•œåƒ: https://hf-mirror.com")
print("ğŸ’¾ æ¨¡å‹ç¼“å­˜ç›®å½•: /root/autodl-tmp/hf_cache (150GBæ•°æ®ç›˜)")

# å®šä¹‰v3-v10çš„æ–¹å‘å‘é‡ï¼ˆåŸºäºè®ºæ–‡Tableï¼‰
PREFERENCE_DIRECTIONS = {
    "v3": {"vector": (0.9848, 0.1736), "angle": 10},
    "v4": {"vector": (0.9659, 0.2588), "angle": 15},
    "v5": {"vector": (0.9397, 0.3420), "angle": 20},
    "v6": {"vector": (0.9063, 0.4226), "angle": 25},
    "v7": {"vector": (0.8660, 0.5000), "angle": 30},
    "v8": {"vector": (0.8192, 0.5736), "angle": 35},
    "v9": {"vector": (0.7660, 0.6428), "angle": 40},
    "v10": {"vector": (0.7071, 0.7071), "angle": 45},
}

def setup_environment():
    """è®¾ç½®ç¯å¢ƒå’Œéšæœºç§å­"""
    # è®¾ç½®è®¾å¤‡
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ”§ Using device: {device}")
    
    # è®¾ç½®éšæœºç§å­
    seed = 42
    np.random.seed(seed)
    random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    
    return device

def load_models(device):
    """åŠ è½½DPOæ¨¡å‹å’ŒRewardæ¨¡å‹"""
    cache_dir = '/root/autodl-tmp/hf_cache'
    print(f"ğŸ¤– Loading DPO model from mirror... (cache: {cache_dir})")
    try:
        # ğŸ”„ ä½¿ç”¨çœŸæ­£çš„DPOè®­ç»ƒæ¨¡å‹ - zephyr-7b-beta
        dpo_model = AutoModelForCausalLM.from_pretrained(
            "HuggingFaceH4/zephyr-7b-beta",  # âœ… çœŸæ­£çš„DPOæ¨¡å‹
            torch_dtype=torch.float16,
            device_map="auto",  # è‡ªåŠ¨åˆ†é…åˆ°GPU/CPU
            trust_remote_code=True,
            cache_dir=cache_dir,  # ğŸ”„ æ·»åŠ ç¼“å­˜ç›®å½•
            resume_download=True,  # æ”¯æŒæ–­ç‚¹ç»­ä¼ 
            low_cpu_mem_usage=True  # ğŸ”„ å‡å°‘CPUå†…å­˜ä½¿ç”¨
        )
        print("âœ… DPO model loaded successfully!")
        
        dpo_tokenizer = AutoTokenizer.from_pretrained(
            "HuggingFaceH4/zephyr-7b-beta",  # ğŸ”„ æ›¿æ¢ä¸ºå¯¹åº”çš„tokenizer
            trust_remote_code=True,
            cache_dir=cache_dir  # ğŸ”„ æ·»åŠ ç¼“å­˜ç›®å½•
        )
        dpo_tokenizer.padding_side = "left"
        # ğŸ”§ ä¿®å¤pad_tokenè®¾ç½® - zephyræ¨¡å‹ä½¿ç”¨eos_tokenä½œä¸ºpad_token
        if dpo_tokenizer.pad_token_id is None:
            dpo_tokenizer.pad_token = dpo_tokenizer.eos_token
        print("âœ… DPO tokenizer loaded successfully!")
        
    except Exception as e:
        print(f"âŒ Error loading DPO model: {e}")
        print("ğŸ”„ Retrying with alternative settings...")
        # é‡è¯•æœºåˆ¶ - ğŸ”„ ç¡®ä¿ä¹Ÿä½¿ç”¨cache_dir
        dpo_model = AutoModelForCausalLM.from_pretrained(
            "HuggingFaceH4/zephyr-7b-beta",  # ğŸ”„ æ›¿æ¢ä¸ºçœŸæ­£çš„DPOæ¨¡å‹
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True,
            cache_dir=cache_dir,  # ğŸ”„ æ·»åŠ ç¼“å­˜ç›®å½•
            resume_download=True,
            local_files_only=False,
            low_cpu_mem_usage=True
        )
        dpo_tokenizer = AutoTokenizer.from_pretrained(
            "HuggingFaceH4/zephyr-7b-beta",  # ğŸ”„ æ›¿æ¢ä¸ºå¯¹åº”çš„tokenizer
            cache_dir=cache_dir  # ğŸ”„ æ·»åŠ ç¼“å­˜ç›®å½•
        )
        if dpo_tokenizer.pad_token_id is None:
            dpo_tokenizer.pad_token = dpo_tokenizer.eos_token
    
    # ğŸ”„ æ¸…ç†GPUç¼“å­˜
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    print("ğŸ† Loading Reward model from mirror...")
    try:
        # ğŸ”„ Rewardæ¨¡å‹æ”¾åœ¨GPUä¸Šï¼Œä¸DPOæ¨¡å‹å…±äº«æ˜¾å­˜
        reward_model = AutoModelForSequenceClassification.from_pretrained(
            "Haoxiang-Wang/RewardModel-Mistral-7B-for-DPA-v1", 
            trust_remote_code=True,
            cache_dir=cache_dir,  # ğŸ”„ æ·»åŠ ç¼“å­˜ç›®å½•
            resume_download=True,
            torch_dtype=torch.float16,  # ğŸ”§ ä½¿ç”¨fp16ä¸DPOæ¨¡å‹ä¸€è‡´
            device_map="auto",  # ğŸ”§ è‡ªåŠ¨åˆ†é…è®¾å¤‡
            low_cpu_mem_usage=True
        )
        print("âœ… Reward model loaded successfully!")
        
        reward_tokenizer = AutoTokenizer.from_pretrained(
            "Haoxiang-Wang/RewardModel-Mistral-7B-for-DPA-v1",
            trust_remote_code=True,
            cache_dir=cache_dir  # ğŸ”„ æ·»åŠ ç¼“å­˜ç›®å½•
        )
        print("âœ… Reward tokenizer loaded successfully!")
        
    except Exception as e:
        print(f"âŒ Error loading Reward model: {e}")
        print("ğŸ”„ Trying alternative reward model...")
        # ğŸ”„ å¤‡é€‰æ–¹æ¡ˆï¼šå°†rewardæ¨¡å‹æ”¾åœ¨CPUä¸Š
        reward_model = AutoModelForSequenceClassification.from_pretrained(
            "Haoxiang-Wang/RewardModel-Mistral-7B-for-DPA-v1", 
            trust_remote_code=True,
            cache_dir=cache_dir,  # ğŸ”„ æ·»åŠ ç¼“å­˜ç›®å½•
            resume_download=True,
            torch_dtype=torch.float16,
            device_map="auto",  # ğŸ”„ è‡ªåŠ¨åˆ†é…è®¾å¤‡
            low_cpu_mem_usage=True
        )
        reward_tokenizer = AutoTokenizer.from_pretrained(
            "Haoxiang-Wang/RewardModel-Mistral-7B-for-DPA-v1",
            cache_dir=cache_dir  # ğŸ”„ æ·»åŠ ç¼“å­˜ç›®å½•
        )
        print("âœ… Reward model loaded successfully")
    
    return dpo_model, dpo_tokenizer, reward_model, reward_tokenizer

def build_dpa_input(prompt, v1, v2):
    """æ„é€ DPAæ¨¡å‹çš„è¾“å…¥æ ¼å¼"""
    # æŒ‰ç…§è®ºæ–‡é™„å½•ä¸­çš„æ ¼å¼æ„é€ è¾“å…¥
    h = int(np.round(v1 * 100))
    v = int(np.round(v2 * 100))
    sys_instruction = f"You are a helpful assistant. Your response should maximize weighted rating = helpfulness*{h} + verbosity*{v}."
    
    return [{"role": "user", "content": f"{sys_instruction}\n\n{prompt}"}]

# ä¸è¦ä½¿ç”¨num_return_sequencesï¼Œæ”¹ä¸ºå¾ªç¯ç”Ÿæˆ
def generate_responses_for_direction(prompt, prompt_id, direction_name, direction_info, 
                                   dpo_model, dpo_tokenizer, device, num_responses=3):
    try:
        v1, v2 = direction_info["vector"]
        angle = direction_info["angle"]
        
        messages = build_dpa_input(prompt, v1, v2)
        tokenized = dpo_tokenizer.apply_chat_template(
            messages, add_generation_prompt=True, return_tensors="pt",
            padding=True, return_attention_mask=True, truncation=True
        )
        input_ids = tokenized['input_ids'].to(device) if isinstance(tokenized, dict) else tokenized.to(device)
        attention_mask = tokenized.get('attention_mask', None) if isinstance(tokenized, dict) else None
        if attention_mask is not None:
            attention_mask = attention_mask.to(device)
        else:
            # å¦‚æœæ²¡æœ‰attention_maskï¼Œåˆ›å»ºä¸€ä¸ªå…¨1çš„mask
            attention_mask = torch.ones_like(input_ids).to(device)
        
        responses = []
        
        # ğŸ”§ ä¸€æ¬¡æ€§ç”Ÿæˆå¤šä¸ªå“åº”ï¼Œæ›´é«˜æ•ˆ
        print(f"    ğŸ”„ Generating {num_responses} responses for prompt {prompt_id}...")
        with torch.no_grad():
            outputs = dpo_model.generate(
                input_ids=input_ids,
                attention_mask=attention_mask,
                max_new_tokens=512,
                temperature=0.7,
                do_sample=True,
                num_return_sequences=num_responses,  # ğŸ”§ æ¢å¤ä¸€æ¬¡æ€§ç”Ÿæˆå¤šä¸ªå“åº”
                pad_token_id=dpo_tokenizer.pad_token_id,
                eos_token_id=dpo_tokenizer.eos_token_id,
                top_p=0.9,  # ğŸ”§ æ·»åŠ top_på‚æ•°
                repetition_penalty=1.1  # ğŸ”§ æ·»åŠ repetition_penaltyå‚æ•°
            )
        
        # ğŸ”§ å¤„ç†å¤šä¸ªè¾“å‡º
        for i in range(num_responses):
            generated_tokens = outputs[i][input_ids.shape[1]:]
            decoded = dpo_tokenizer.decode(generated_tokens, skip_special_tokens=True)
            responses.append({
                "prompt_id": prompt_id,
                "prompt": prompt,
                "direction_name": direction_name,
                "direction_vector": f"({v1:.4f}, {v2:.4f})",
                "angle_degrees": angle,
                "response_id": i + 1,
                "response": decoded
                         })
        
        return responses
    
    except Exception as e:
        print(f"âš ï¸ Error generating responses for prompt {prompt_id} direction {direction_name}: {e}")
        return []

def score_response_dpa(prompt, response, reward_model, reward_tokenizer, device, v1, v2):
    """ä½¿ç”¨DPAæ¡†æ¶ä¸ºå“åº”æ‰“åˆ†"""
    try:
        template = "[INST] You must read the following conversation carefully and rate the assistant's response from score 0-100 in these aspects: helpfulness, correctness, coherence, honesty, complexity, verbosity\n\nUser: {prompt}\n\nAssistant: {response} [/INST]"
        
        inputs = reward_tokenizer(
            template.format(prompt=prompt, response=response), 
            return_tensors="pt"
        )
        
        # ğŸ”„ æ£€æŸ¥rewardæ¨¡å‹çš„è®¾å¤‡å¹¶ç›¸åº”åœ°ç§»åŠ¨è¾“å…¥
        model_device = next(reward_model.parameters()).device
        inputs = inputs.to(model_device)
        
        with torch.no_grad():
            logits = reward_model(**inputs).logits.squeeze().cpu().numpy()
        
        helpfulness = logits[9]  # r_help(x, y)
        verbosity = logits[4]    # r_verb(x, y)
        
        # DPAå¥–åŠ±å‡½æ•°: R(v, y) = v1 * r_help + v2 * r_verb
        dpa_score = v1 * helpfulness + v2 * verbosity
        
        return {
            "helpfulness": float(helpfulness),
            "verbosity": float(verbosity),
            "v1": float(v1),
            "v2": float(v2),
            "dpa_score": float(dpa_score)
        }
    
    except Exception as e:
        print(f"âš ï¸ Error scoring response: {e}")
        return {
            "helpfulness": 0.0,
            "verbosity": 0.0,
            "v1": float(v1),
            "v2": float(v2),
            "dpa_score": 0.0
        }

def generate_and_evaluate_all_directions(
    prompts, 
    prompt_ids,
    dpo_model, 
    dpo_tokenizer, 
    reward_model, 
    reward_tokenizer, 
    device,
    output_dir,
    batch_size=8,  # ğŸ”§ å¢å¤§batch sizeæé«˜æ•ˆç‡
    num_responses=3
):
    """ä¸ºæ‰€æœ‰æ–¹å‘ç”Ÿæˆå’Œè¯„ä¼°å“åº”"""
    
    start_time = time.time()
    all_results = []
    
    # ä¸ºæ¯ä¸ªæ–¹å‘å¤„ç†
    for direction_name, direction_info in PREFERENCE_DIRECTIONS.items():
        print(f"\nğŸ¯ Processing direction {direction_name}: {direction_info['vector']} ({direction_info['angle']}Â°)")
        
        output_file = os.path.join(output_dir, f"dpo_responses_{direction_name}.csv")
        
        # æ£€æŸ¥å·²æœ‰ç»“æœï¼Œæ”¯æŒæ–­ç‚¹ç»­è·‘
        done_prompt_ids = set()
        direction_results = []
        
        if os.path.exists(output_file):
            try:
                existing_df = pd.read_csv(output_file)
                done_prompt_ids = set(existing_df["prompt_id"].unique())
                print(f"ğŸ” Found existing results for {len(done_prompt_ids)} prompts in {direction_name}")
            except Exception as e:
                print(f"âš ï¸ Error loading existing file for {direction_name}: {e}")
        
        v1, v2 = direction_info["vector"]
        
        # è®¡ç®—å‰©ä½™éœ€è¦å¤„ç†çš„promptsæ•°é‡
        remaining_prompts = [pid for pid in prompt_ids if pid not in done_prompt_ids]
        print(f" {direction_name}: å·²å¤„ç† {len(done_prompt_ids)} ä¸ªï¼Œå‰©ä½™ {len(remaining_prompts)} ä¸ª")
        
        # æ‰¹é‡å¤„ç†prompts
        for start in tqdm(range(0, len(prompts), batch_size), 
                         desc=f"Processing {direction_name} (å‰©ä½™{len(remaining_prompts)}ä¸ª)"):
            end = min(start + batch_size, len(prompts))
            batch_prompts = prompts[start:end]
            batch_ids = prompt_ids[start:end]
            
            # è·³è¿‡å·²å¤„ç†çš„prompts
            unprocessed_indices = [i for i, pid in enumerate(batch_ids) if pid not in done_prompt_ids]
            if not unprocessed_indices:
                continue
                
            batch_results = []
            
            for i in unprocessed_indices:
                prompt = batch_prompts[i]
                prompt_id = batch_ids[i]
                
                # ç”Ÿæˆå¤šä¸ªå“åº”
                responses = generate_responses_for_direction(
                    prompt, prompt_id, direction_name, direction_info,
                    dpo_model, dpo_tokenizer, device, num_responses
                )
                
                if not responses:
                    continue
                
                # ä¸ºæ¯ä¸ªå“åº”æ‰“åˆ†
                scored_responses = []
                for resp_data in responses:
                    scores = score_response_dpa(
                        resp_data["prompt"], 
                        resp_data["response"],
                        reward_model,
                        reward_tokenizer,
                        device,
                        v1, v2
                    )
                    
                    resp_data.update(scores)
                    scored_responses.append(resp_data)
                
                # é€‰æ‹©å¾—åˆ†æœ€é«˜çš„å“åº”
                if scored_responses:
                    best_response = max(scored_responses, key=lambda x: x["dpa_score"])
                    best_response["selected_as_best"] = True
                    
                    # æ·»åŠ å…¶ä»–å“åº”ä¿¡æ¯
                    best_response["all_dpa_scores"] = [r["dpa_score"] for r in scored_responses]
                    best_response["num_candidates"] = len(scored_responses)
                    
                    batch_results.append(best_response)
            
            # ä¿å­˜æ‰¹å¤„ç†ç»“æœ
            if batch_results:
                df_batch = pd.DataFrame(batch_results)
                
                if not os.path.exists(output_file):
                    df_batch.to_csv(output_file, index=False)
                else:
                    df_batch.to_csv(output_file, mode='a', header=False, index=False)
                
                direction_results.extend(batch_results)
                print(f"âœ… Saved batch for {direction_name}, total processed: {len(direction_results)}")
        
        all_results.extend(direction_results)
        print(f"âœ… Completed direction {direction_name}: {len(direction_results)} responses")
    
    elapsed_time = time.time() - start_time
    print(f"\nğŸ All directions completed in {elapsed_time:.1f} seconds")
    print(f"ğŸ“Š Total responses generated: {len(all_results)}")
    
    return all_results

def main():
    """ä¸»å‡½æ•°"""
    # ğŸ”„ ä½¿ç”¨æ–°çš„è¾“å‡ºç›®å½•ï¼Œä½¿ç”¨7Bæ¨¡å‹
    result_dir = "/root/rps/data/dpo_outputs"
    os.makedirs(result_dir, exist_ok=True)
    
    # ğŸ”„ æ£€æŸ¥å·²æœ‰æ•°æ®
    existing_files = []
    for direction_name in PREFERENCE_DIRECTIONS.keys():
        output_file = os.path.join(result_dir, f"dpo_responses_{direction_name}.csv")
        if os.path.exists(output_file):
            try:
                df = pd.read_csv(output_file)
                existing_files.append((direction_name, len(df)))
                print(f"ğŸ“ Found existing data for {direction_name}: {len(df)} responses")
            except Exception as e:
                print(f"âš ï¸ Error reading {direction_name}: {e}")
    
    if existing_files:
        print(f"\nâœ… Found complete baseline data in {result_dir}")
        print(f"ğŸ“Š Summary:")
        total_responses = 0
        for direction_name, count in existing_files:
            print(f"  {direction_name}: {count} responses")
            total_responses += count
        print(f"  Total: {total_responses} responses across {len(existing_files)} directions")
        
        user_input = input("\nğŸ¤” Data already exists. Continue anyway? (y/N): ").strip().lower()
        if user_input != 'y':
            print("ğŸ›‘ Stopping execution. Use existing data for analysis.")
            return
    
    # è®¾ç½®ç¯å¢ƒ
    device = setup_environment()
    
    # åŠ è½½æ•°æ®é›†
    print("ğŸ“¦ Loading prompts from UltraFeedback via mirror...")
    try:
        ds = load_dataset("HuggingFaceH4/ultrafeedback_binarized", split="test_prefs")
        prompts = ds["prompt"][:2000]  # ğŸ”„ ä¿®æ”¹ä¸º2000ä¸ªprompts
        prompt_ids = list(range(len(prompts)))
        print(f"âœ… Loaded {len(prompts)} prompts successfully!")
    except Exception as e:
        print(f"âŒ Error loading dataset: {e}")
        print("ğŸ”„ Retrying dataset loading with different methods...")
        
        # å°è¯•å¤šç§æ–¹æ³•
        retry_methods = [
            # æ–¹æ³•1ï¼šä¸ä½¿ç”¨trust_remote_code
            lambda: load_dataset("HuggingFaceH4/ultrafeedback_binarized", split="test_prefs"),
            # æ–¹æ³•2ï¼šå°è¯•ä½¿ç”¨cache
            lambda: load_dataset("HuggingFaceH4/ultrafeedback_binarized", split="test_prefs", cache_dir="/root/.cache/huggingface"),
            # æ–¹æ³•3ï¼šå¼ºåˆ¶é‡æ–°ä¸‹è½½
            lambda: load_dataset("HuggingFaceH4/ultrafeedback_binarized", split="test_prefs", download_mode="force_redownload"),
        ]
        
        for i, method in enumerate(retry_methods, 1):
            try:
                print(f"ğŸ”„ Trying method {i}...")
                ds = method()
                prompts = ds["prompt"][:2000]
                prompt_ids = list(range(len(prompts)))
                print(f"âœ… Method {i} succeeded! Loaded {len(prompts)} prompts")
                break
            except Exception as retry_error:
                print(f"âŒ Method {i} failed: {retry_error}")
                if i == len(retry_methods):
                    print("ğŸš¨ All retry methods failed. Please check your network connection.")
                    raise retry_error
    
    # åŠ è½½æ¨¡å‹
    dpo_model, dpo_tokenizer, reward_model, reward_tokenizer = load_models(device)
    
    # æ˜¾ç¤ºå°†è¦å¤„ç†çš„æ–¹å‘
    print(f"\nğŸ“ Will process {len(PREFERENCE_DIRECTIONS)} directions:")
    for name, info in PREFERENCE_DIRECTIONS.items():
        print(f"  {name}: {info['vector']} ({info['angle']}Â°)")
    
    print(f"\nğŸš€ Starting generation for {len(prompts)} prompts across all directions...")
    results = generate_and_evaluate_all_directions(
        prompts=prompts,
        prompt_ids=prompt_ids,
        dpo_model=dpo_model,
        dpo_tokenizer=dpo_tokenizer,
        reward_model=reward_model,
        reward_tokenizer=reward_tokenizer,
        device=device,
        output_dir=result_dir,
        batch_size=8,  # ğŸ”§ å¢å¤§batch sizeæé«˜æ•ˆç‡
        num_responses=3  # è¿›ä¸€æ­¥åŠ é€Ÿ
    )
    
    print(f"\nâœ… All done! Results saved to {result_dir}")
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    print(f"ğŸ“ˆ Final statistics:")
    for direction_name in PREFERENCE_DIRECTIONS.keys():
        output_file = os.path.join(result_dir, f"dpo_responses_{direction_name}.csv")
        if os.path.exists(output_file):
            df = pd.read_csv(output_file)
            print(f"  {direction_name}: {len(df)} responses, avg DPA score: {df['dpa_score'].mean():.2f}")

if __name__ == "__main__":
    main()