# steerlm_nemo_fixed.py - ä½¿ç”¨NeMoæ¡†æ¶æ­£ç¡®åŠ è½½SteerLMæ¨¡å‹
import os
import numpy as np
import pandas as pd
from datasets import load_dataset
from tqdm.auto import tqdm
import torch
import time
import random
import json

# =============================================================================
# ğŸŒ ç½‘ç»œé…ç½® - æ”¯æŒå›½å†…é•œåƒ
# =============================================================================

# å›½å†…é•œåƒé€‰é¡¹
MIRROR_OPTIONS = {
    "official": {
        "hf_endpoint": None,
        "name": "Official HuggingFace",
        "description": "ç›´æ¥è®¿é—®å®˜æ–¹HuggingFace"
    },
    "hf_mirror": {
        "hf_endpoint": "https://hf-mirror.com", 
        "name": "HF Mirror",
        "description": "å›½å†…HFé•œåƒ (æ¨è)"
    },
    "modelfun": {
        "hf_endpoint": "https://www.modelfun.cn",
        "name": "ModelFun",
        "description": "æ¨¡å‹ä¹å›­é•œåƒ"
    }
}

def setup_mirror(mirror_choice="hf_mirror"):
    """è®¾ç½®é•œåƒé…ç½®"""
    if mirror_choice in MIRROR_OPTIONS:
        mirror = MIRROR_OPTIONS[mirror_choice]
        if mirror["hf_endpoint"]:
            os.environ['HF_ENDPOINT'] = mirror["hf_endpoint"]
        elif 'HF_ENDPOINT' in os.environ:
            del os.environ['HF_ENDPOINT']
        
        print(f"ğŸŒ ä½¿ç”¨é•œåƒ: {mirror['name']} - {mirror['description']}")
        return mirror["name"]
    else:
        print(f"âŒ æœªçŸ¥é•œåƒé€‰æ‹©: {mirror_choice}")
        return "Unknown"

# è®¾ç½®ç½‘ç»œè¶…æ—¶å’Œé‡è¯•
os.environ['HF_HUB_DOWNLOAD_TIMEOUT'] = '300'
os.environ['TRANSFORMERS_CACHE'] = '/root/.cache/huggingface'

# é»˜è®¤ä½¿ç”¨å›½å†…é•œåƒ
current_mirror = setup_mirror("hf_mirror")

print("ğŸŒ Network configuration:")
print(f"Current Mirror: {current_mirror}")
print(f"HF_ENDPOINT: {os.environ.get('HF_ENDPOINT', 'Official HuggingFace')}")
print(f"Cache dir: {os.environ.get('TRANSFORMERS_CACHE', 'Default')}")

# =============================================================================

# å¯¼å…¥NeMoç›¸å…³æ¨¡å—
try:
    from nemo.collections.nlp.models.language_modeling.megatron_gpt_model import MegatronGPTModel
    from nemo.collections.nlp.parts.utils_funcs import torch_dtype_from_precision
    from nemo.core.config import hydra_runner
    from omegaconf import DictConfig
    import nemo
    # ä½¿ç”¨æ­£ç¡®çš„PyTorch Lightningå¯¼å…¥
    try:
        from lightning.pytorch import Trainer
    except ImportError:
        from pytorch_lightning import Trainer
    print("âœ… NeMo toolkit imported successfully!")
except ImportError as e:
    print(f"âŒ NeMo import failed: {e}")
    print("ğŸ’¡ è¯·å…ˆè¿è¡Œ: bash install_nemo_steerlm.sh")
    exit(1)

# å®šä¹‰æ–¹å‘å‘é‡
PREFERENCE_DIRECTIONS = {
    "v3": {"vector": (0.9848, 0.1736), "angle": 10},
    "v4": {"vector": (0.9659, 0.2588), "angle": 15},
    "v5": {"vector": (0.9397, 0.3420), "angle": 20},
    "v6": {"vector": (0.9063, 0.4226), "angle": 25},
    "v7": {"vector": (0.8660, 0.5000), "angle": 30},
    "v8": {"vector": (0.8192, 0.5736), "angle": 35},
    "v9": {"vector": (0.7660, 0.6428), "angle": 40},
    "v10": {"vector": (0.7071, 0.7071), "angle": 45},
}

def download_nemo_model():
    """ä¸‹è½½.nemoæ ¼å¼çš„SteerLMæ¨¡å‹"""
    from huggingface_hub import hf_hub_download
    
    print("ğŸ“¥ Downloading Nemotron-3-8B-SteerLM (.nemo format)...")
    
    try:
        model_path = hf_hub_download(
            repo_id="nvidia/nemotron-3-8b-chat-4k-steerlm",
            filename="Nemotron-3-8B-Chat-4k-SteerLM.nemo",
            cache_dir="/root/.cache/huggingface",
            resume_download=True
        )
        print(f"âœ… Model downloaded to: {model_path}")
        return model_path
    except Exception as e:
        print(f"âŒ Download failed: {e}")
        print("ğŸ’¡ ç¡®ä¿å·²ç™»å½•HuggingFaceå¹¶æ¥å—æ¨¡å‹è®¸å¯è¯")
        return None

def load_nemo_steerlm_model(nemo_path, device="cuda"):
    """ä½¿ç”¨NeMoåŠ è½½SteerLMæ¨¡å‹"""
    print(f"ğŸ¤– Loading SteerLM model from: {nemo_path}")
    
    try:
        # åˆ›å»ºPTL trainer
        trainer = Trainer(
            accelerator='gpu' if torch.cuda.is_available() else 'cpu',
            devices=1,
            precision=16 if torch.cuda.is_available() else 32,
            logger=False,
            enable_checkpointing=False,
            enable_progress_bar=False,
            enable_model_summary=False,
        )
        
        # ä½¿ç”¨NeMoåŠ è½½æ¨¡å‹
        model = MegatronGPTModel.restore_from(
            restore_path=nemo_path,
            trainer=trainer,
            map_location=device
        )
        
        # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        model.eval()
        model = model.to(device)
        
        print("âœ… NeMo SteerLM model loaded successfully!")
        return model
        
    except Exception as e:
        print(f"âŒ Failed to load model: {e}")
        return None

def dpa_vector_to_steerlm_attributes(v1, v2):
    """æ˜ å°„DPAå‘é‡åˆ°SteerLMå±æ€§æ ¼å¼ - ä¸¥æ ¼æŒ‰ç…§è®ºæ–‡è§„èŒƒ"""
    helpfulness = max(0, min(4, round(v1 * 4)))
    verbosity = max(0, min(4, round(v2 * 4)))
    
    return {
        "quality": 4,
        "understanding": 4,
        "correctness": 4,
        "coherence": 4,
        "complexity": 2,        # ä¿®æ­£ï¼š4 â†’ 2
        "verbosity": verbosity,
        "toxicity": 0,
        "humor": 0,
        "creativity": 1,        # ä¿®æ­£ï¼š0 â†’ 1
        "violence": 0,
        "helpfulness": helpfulness,
        "not_appropriate": 0,
        "hate_speech": 0,
        "sexual_content": 0,
        "fails_task": 0,
        "political_content": 0,
        "moral_judgement": 0,
        "lang": "en"
    }

def build_steerlm_prompt(prompt, v1, v2):
    """æ„å»ºSteerLMæ ¼å¼çš„prompt"""
    attrs = dpa_vector_to_steerlm_attributes(v1, v2)
    
    # æŒ‰ç…§å®˜æ–¹é¡ºåºæ„å»ºå±æ€§å­—ç¬¦ä¸²
    attr_string = f"quality:{attrs['quality']},understanding:{attrs['understanding']},correctness:{attrs['correctness']},coherence:{attrs['coherence']},complexity:{attrs['complexity']},verbosity:{attrs['verbosity']},toxicity:{attrs['toxicity']},humor:{attrs['humor']},creativity:{attrs['creativity']},violence:{attrs['violence']},helpfulness:{attrs['helpfulness']},not_appropriate:{attrs['not_appropriate']},hate_speech:{attrs['hate_speech']},sexual_content:{attrs['sexual_content']},fails_task:{attrs['fails_task']},political_content:{attrs['political_content']},moral_judgement:{attrs['moral_judgement']},lang:{attrs['lang']}"
    
    # å®˜æ–¹SteerLM promptæ ¼å¼
    steerlm_prompt = f"""<extra_id_0>System
A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.

<extra_id_1>User
{prompt}
<extra_id_1>Assistant
<extra_id_2>{attr_string}
"""
    
    return steerlm_prompt, attr_string

def generate_with_nemo_model(model, prompt_text, max_tokens=512, temperature=0.7):
    """ä½¿ç”¨NeMoæ¨¡å‹ç”Ÿæˆæ–‡æœ¬"""
    try:
        # NeMoç”Ÿæˆå‚æ•°
        length_params = {
            "max_length": max_tokens,
            "min_length": 1,
        }
        
        sampling_params = {
            "use_greedy": temperature == 0.0,
            "temperature": temperature,
            "top_k": 50,
            "top_p": 0.9,
            "repetition_penalty": 1.2,
        }
        
        # ç”Ÿæˆå“åº”
        response = model.generate([prompt_text], length_params, sampling_params)
        
        if response and len(response) > 0:
            return response[0]
        else:
            return "ERROR: Empty response from model"
            
    except Exception as e:
        return f"ERROR: {str(e)}"

def generate_responses_for_direction(prompts, v1, v2, model, direction_name):
    """ä¸ºæŒ‡å®šæ–¹å‘ç”Ÿæˆå“åº”"""
    print(f"ğŸ¯ Generating responses for {direction_name} (v1={v1:.4f}, v2={v2:.4f})")
    
    results = []
    
    for i, prompt in enumerate(tqdm(prompts, desc=f"Generating {direction_name}")):
        prompt_results = []
        
        # ä¸ºæ¯ä¸ªpromptç”Ÿæˆ3ä¸ªå“åº”
        for sample_id in range(3):
            try:
                steerlm_prompt, attr_string = build_steerlm_prompt(prompt, v1, v2)
                
                # ä½¿ç”¨NeMoç”Ÿæˆå“åº”
                response = generate_with_nemo_model(
                    model=model,
                    prompt_text=steerlm_prompt,
                    max_tokens=512,
                    temperature=0.7
                )
                
                # æ¸…ç†è¾“å‡ºï¼ˆç§»é™¤promptéƒ¨åˆ†ï¼‰
                if response.startswith(steerlm_prompt):
                    response = response[len(steerlm_prompt):].strip()
                
                # ç§»é™¤SteerLMç‰¹æ®Šæ ‡è®°
                response = response.split("<extra_id_1>")[0].strip()
                
                prompt_results.append({
                    "prompt_id": i,
                    "sample_id": sample_id,
                    "prompt": prompt,
                    "response": response,
                    "direction": direction_name,
                    "v1": v1,
                    "v2": v2,
                    "attributes": attr_string,
                    "model_name": "nemotron-3-8b-chat-4k-steerlm"
                })
                
            except Exception as e:
                print(f"âŒ Error generating response for prompt {i}, sample {sample_id}: {e}")
                prompt_results.append({
                    "prompt_id": i,
                    "sample_id": sample_id,
                    "prompt": prompt,
                    "response": f"ERROR: {str(e)}",
                    "direction": direction_name,
                    "v1": v1,
                    "v2": v2,
                    "attributes": "ERROR",
                    "model_name": "nemotron-3-8b-chat-4k-steerlm"
                })
        
        results.extend(prompt_results)
    
    return results

def main():
    """ä¸»å‡½æ•° - ä½¿ç”¨NeMoåŠ è½½å’Œæµ‹è¯•SteerLMæ¨¡å‹"""
    print("ğŸš€ Starting NeMo SteerLM experiment!")
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # æ ¹æ®æ˜¾å­˜è®¾ç½®batch_size
    if torch.cuda.is_available():
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3  # GB
        print(f"ğŸš€ GPU Memory: {gpu_memory:.1f}GB")
    else:
        print("ğŸ’» Using CPU")
    
    # ä¸‹è½½.nemoæ¨¡å‹
    nemo_path = download_nemo_model()
    if nemo_path is None:
        return
    
    # åŠ è½½NeMoæ¨¡å‹
    print(f"ğŸ¤– Loading model from: {nemo_path}")
    model = load_nemo_steerlm_model(nemo_path, device)
    if model is None:
        return
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    print("ğŸ“¦ Loading UltraFeedback dataset...")
    ds = load_dataset("HuggingFaceH4/ultrafeedback_binarized", split="test_prefs")
    prompts = ds["prompt"][:5]  # å…ˆæµ‹è¯•5ä¸ª
    
    # æµ‹è¯•ä¸€ä¸ªprompt
    test_prompt = prompts[0]
    v1, v2 = 0.8, 0.4  # é«˜helpfulnessï¼Œä¸­ç­‰verbosity
    
    print(f"\nğŸ§ª Testing with prompt: {test_prompt[:100]}...")
    print(f"ğŸ¯ DPA vector: ({v1}, {v2})")
    
    steerlm_prompt, attr_string = build_steerlm_prompt(test_prompt, v1, v2)
    
    print(f"\nğŸ“ Generated SteerLM prompt:")
    print(f"```\n{steerlm_prompt}\n```")
    
    # ç”Ÿæˆå“åº”
    print("âš¡ Generating response...")
    response = generate_with_nemo_model(
        model=model,
        prompt_text=steerlm_prompt,
        max_tokens=512,
        temperature=0.7
    )
    
    # æ¸…ç†è¾“å‡º
    if response.startswith(steerlm_prompt):
        response = response[len(steerlm_prompt):].strip()
    response = response.split("<extra_id_1>")[0].strip()
    
    print(f"\nğŸ¯ SteerLM Response:")
    print(f"```\n{response}\n```")
    
    print(f"\nâœ… NeMo SteerLM test successful!")
    print(f"ğŸ“Š Attribute string used: {attr_string}")
    print(f"ğŸ”§ Model: nvidia/nemotron-3-8b-chat-4k-steerlm (.nemo format)")

if __name__ == "__main__":
    main() 