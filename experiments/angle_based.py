# -*- coding: utf-8 -*-
"""angle_based.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PsBiAui4lvvGc8YJvieVk6XCqjdopeF6
"""

vectors = {
    "v1":  np.array([1.0000, 0.0000]),
    "v2":  np.array([0.9962, 0.0872]),
    "v3":  np.array([0.9848, 0.1736]),
    "v4":  np.array([0.9659, 0.2588]),
    "v5":  np.array([0.9397, 0.3420]),
    "v6":  np.array([0.9063, 0.4226]),
    "v7":  np.array([0.8660, 0.5000]),
    "v8":  np.array([0.8192, 0.5736]),
    "v9":  np.array([0.7660, 0.6428]),
    "v10": np.array([0.7071, 0.7071])
}

!pip install -U datasets

# ğŸš€ Step 0: é˜²æ­¢ Colab ç©ºé—²æ–­çº¿
from IPython.display import Javascript
Javascript('''
function ClickConnect(){
  console.log("ğŸ”„ ä¿æŒè¿æ¥ä¸­ - ClickConnect è¢«è°ƒç”¨äº†");
  document.querySelector("colab-connect-button").click();
}
setInterval(ClickConnect, 60000);
''')

from google.colab import drive
drive.mount('/content/drive')

# âœ… Step 2: å®‰è£…ä¾èµ–
!pip install -q transformers datasets accelerate

import numpy as np

def get_top_k_angle_perturbations(v_main,
                                   angle_range=(-40, 40),
                                   step=5,
                                   theta_max=30,
                                   top_k=5,
                                   verbose=True):
    """
    è¿”å›ä¸ä¸»æ–¹å‘å¤¹è§’æœ€å°çš„å‰ top_k ä¸ªåˆæ³•æ‰°åŠ¨æ–¹å‘ï¼ˆAngle-Based Perturbationï¼‰

    Args:
        v_main (np.ndarray): ä¸»æ–¹å‘å•ä½å‘é‡ï¼Œä¾‹å¦‚ np.array([0.7071, 0.7071])
        angle_range (tuple): æ‰°åŠ¨è§’åº¦èŒƒå›´ (min_angle, max_angle)ï¼Œå•ä½ä¸ºåº¦
        step (int): æ¯éš”å¤šå°‘åº¦é‡‡æ ·ä¸€ä¸ªæ–¹å‘
        theta_max (float): æœ€å¤§å…è®¸çš„å¤¹è§’ï¼ˆå•ä½ä¸ºåº¦ï¼‰
        top_k (int): ä¿ç•™å¤¹è§’æœ€å°çš„å‰ k ä¸ªæ–¹å‘
        verbose (bool): æ˜¯å¦æ‰“å°ç­›é€‰ç»“æœ

    Returns:
        valid_vs (List[np.ndarray]): æœ‰æ•ˆæ‰°åŠ¨æ–¹å‘å‘é‡
        valid_angles (List[float]): æœ‰æ•ˆæ‰°åŠ¨è§’åº¦ï¼ˆç›¸å¯¹è§’åº¦ï¼‰
    """

    def angle_between(v1, v2):
        cos_angle = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))
        return np.degrees(np.arccos(np.clip(cos_angle, -1.0, 1.0)))

    # æ„é€ å•ä½æ‰°åŠ¨å‘é‡
    angle_offsets = np.arange(angle_range[0], angle_range[1] + 1, step)
    perturbed_vs = []
    perturbed_angles = []
    angle_diffs = []

    for offset in angle_offsets:
        angle_rad = np.radians(offset)
        v = np.array([np.cos(angle_rad), np.sin(angle_rad)])
        angle_diff = angle_between(v, v_main)
        if angle_diff <= theta_max:
            perturbed_vs.append(v)
            perturbed_angles.append(offset)
            angle_diffs.append(angle_diff)

    # æ ¹æ®å¤¹è§’æ’åºå¹¶é€‰å‡º top_k ä¸ª
    sorted_indices = np.argsort(angle_diffs)
    top_indices = sorted_indices[:top_k]
    valid_vs = [perturbed_vs[i] for i in top_indices]
    valid_angles = [perturbed_angles[i] for i in top_indices]

    if verbose:
        print("âœ… æœ€å°å¤¹è§’ Top-K æ‰°åŠ¨æ–¹å‘:")
        for v, a in zip(valid_vs, valid_angles):
            print(f"â†’ angle = {a}Â°, v = {np.round(v, 6)}")

    return valid_vs, valid_angles

#v4
valid_vs, valid_angles = get_top_k_angle_perturbations(
    v_main=vectors["v4"],   # ä½¿ç”¨ä½ å®šä¹‰çš„ä¸»æ–¹å‘ v5 = (0.9397, 0.3420)
    angle_range=(-40, 40),  # åœ¨ä¸»æ–¹å‘ Â±40Â° èŒƒå›´å†…æ„é€ æ‰°åŠ¨
    step=5,                 # æ¯ 5Â° é‡‡æ ·ä¸€æ¬¡
    theta_max=30,           # æœ€å¤§å…è®¸çš„å¤¹è§’ä¸º 30Â°
    top_k=5,                # é€‰æ‹©å¤¹è§’æœ€å°çš„å‰ 5 ä¸ªæ–¹å‘
    verbose=True            # æ‰“å°æœ‰æ•ˆå‘é‡å’Œè§’åº¦
)

#v5
valid_vs, valid_angles = get_top_k_angle_perturbations(
    v_main=vectors["v5"],   # ä½¿ç”¨ä½ å®šä¹‰çš„ä¸»æ–¹å‘ v5 = (0.9397, 0.3420)
    angle_range=(-40, 40),  # åœ¨ä¸»æ–¹å‘ Â±40Â° èŒƒå›´å†…æ„é€ æ‰°åŠ¨
    step=5,                 # æ¯ 5Â° é‡‡æ ·ä¸€æ¬¡
    theta_max=30,           # æœ€å¤§å…è®¸çš„å¤¹è§’ä¸º 30Â°
    top_k=5,                # é€‰æ‹©å¤¹è§’æœ€å°çš„å‰ 5 ä¸ªæ–¹å‘
    verbose=True            # æ‰“å°æœ‰æ•ˆå‘é‡å’Œè§’åº¦
)

#v6
valid_vs, valid_angles = get_top_k_angle_perturbations(
    v_main=vectors["v6"],   # ä½¿ç”¨ä½ å®šä¹‰çš„ä¸»æ–¹å‘ v5 = (0.9397, 0.3420)
    angle_range=(-40, 40),  # åœ¨ä¸»æ–¹å‘ Â±40Â° èŒƒå›´å†…æ„é€ æ‰°åŠ¨
    step=5,                 # æ¯ 5Â° é‡‡æ ·ä¸€æ¬¡
    theta_max=30,           # æœ€å¤§å…è®¸çš„å¤¹è§’ä¸º 30Â°
    top_k=5,                # é€‰æ‹©å¤¹è§’æœ€å°çš„å‰ 5 ä¸ªæ–¹å‘
    verbose=True            # æ‰“å°æœ‰æ•ˆå‘é‡å’Œè§’åº¦
)

#v7
valid_vs, valid_angles = get_top_k_angle_perturbations(
    v_main=vectors["v7"],   # ä½¿ç”¨ä½ å®šä¹‰çš„ä¸»æ–¹å‘ v5 = (0.9397, 0.3420)
    angle_range=(-40, 40),  # åœ¨ä¸»æ–¹å‘ Â±40Â° èŒƒå›´å†…æ„é€ æ‰°åŠ¨
    step=5,                 # æ¯ 5Â° é‡‡æ ·ä¸€æ¬¡
    theta_max=30,           # æœ€å¤§å…è®¸çš„å¤¹è§’ä¸º 30Â°
    top_k=5,                # é€‰æ‹©å¤¹è§’æœ€å°çš„å‰ 5 ä¸ªæ–¹å‘
    verbose=True            # æ‰“å°æœ‰æ•ˆå‘é‡å’Œè§’åº¦
)

#v8
valid_vs, valid_angles = get_top_k_angle_perturbations(
    v_main=vectors["v8"],   # ä½¿ç”¨ä½ å®šä¹‰çš„ä¸»æ–¹å‘ v5 = (0.9397, 0.3420)
    angle_range=(-40, 40),  # åœ¨ä¸»æ–¹å‘ Â±40Â° èŒƒå›´å†…æ„é€ æ‰°åŠ¨
    step=5,                 # æ¯ 5Â° é‡‡æ ·ä¸€æ¬¡
    theta_max=30,           # æœ€å¤§å…è®¸çš„å¤¹è§’ä¸º 30Â°
    top_k=5,                # é€‰æ‹©å¤¹è§’æœ€å°çš„å‰ 5 ä¸ªæ–¹å‘
    verbose=True            # æ‰“å°æœ‰æ•ˆå‘é‡å’Œè§’åº¦
)

#v9
valid_vs, valid_angles = get_top_k_angle_perturbations(
    v_main=vectors["v9"],   # ä½¿ç”¨ä½ å®šä¹‰çš„ä¸»æ–¹å‘ v5 = (0.9397, 0.3420)
    angle_range=(-40, 40),  # åœ¨ä¸»æ–¹å‘ Â±40Â° èŒƒå›´å†…æ„é€ æ‰°åŠ¨
    step=5,                 # æ¯ 5Â° é‡‡æ ·ä¸€æ¬¡
    theta_max=30,           # æœ€å¤§å…è®¸çš„å¤¹è§’ä¸º 30Â°
    top_k=5,                # é€‰æ‹©å¤¹è§’æœ€å°çš„å‰ 5 ä¸ªæ–¹å‘
    verbose=True            # æ‰“å°æœ‰æ•ˆå‘é‡å’Œè§’åº¦
)

#v10
valid_vs, valid_angles = get_top_k_angle_perturbations(
    v_main=vectors["v10"],   # ä½¿ç”¨ä½ å®šä¹‰çš„ä¸»æ–¹å‘ v5 = (0.9397, 0.3420)
    angle_range=(-40, 40),  # åœ¨ä¸»æ–¹å‘ Â±40Â° èŒƒå›´å†…æ„é€ æ‰°åŠ¨
    step=5,                 # æ¯ 5Â° é‡‡æ ·ä¸€æ¬¡
    theta_max=30,           # æœ€å¤§å…è®¸çš„å¤¹è§’ä¸º 30Â°
    top_k=5,                # é€‰æ‹©å¤¹è§’æœ€å°çš„å‰ 5 ä¸ªæ–¹å‘
    verbose=True            # æ‰“å°æœ‰æ•ˆå‘é‡å’Œè§’åº¦
)

#å…ˆæµ‹è¯•ä¸‹v5

def run_dpa_generation(
    result_dir,
    valid_vs,
    valid_angles,
    main_v,
    prompt_limit=500,
    batch_size=8,
    model_name="Haoxiang-Wang/DPA-v1-Mistral-7B",
    tokenizer_name="Haoxiang-Wang/RewardModel-Mistral-7B-for-DPA-v1"
):
    from datasets import load_dataset
    from transformers import AutoTokenizer, AutoModelForCausalLM
    from tqdm.auto import tqdm
    import torch
    import os
    import random
    import numpy as np
    import pandas as pd

    # === è®¾ç½®è®¾å¤‡ä¸ç§å­ ===
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    seed = 42
    np.random.seed(seed)
    random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

    os.makedirs(result_dir, exist_ok=True)

    # === åŠ è½½æ•°æ®é›† ===
    ds = load_dataset("HuggingFaceH4/ultrafeedback_binarized", split="test_prefs")
    prompts = ds["prompt"][:prompt_limit]
    prompt_ids = list(range(prompt_limit))

    # === åŠ è½½æ¨¡å‹ä¸ tokenizer ===
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
        device_map="auto"
    ).to(device)

    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
    tokenizer.padding_side = "left"
    if tokenizer.pad_token_id is None:
        tokenizer.pad_token = tokenizer.eos_token

    def build_input(prompt, v1, v2):
        h = int(np.round(v1 * 100))
        v = int(np.round(v2 * 100))
        sys_instruction = f"You are a helpful assistant. Your response should maximize weighted rating = helpfulness*{h} + verbosity*{v}."
        return [{"role": "user", "content": f"{sys_instruction}\n\n{prompt}"}]

    def generate_response_batch(prompts_batch, prompt_ids_batch, v1, v2):
        input_ids_list = []
        for prompt in prompts_batch:
            messages = build_input(prompt, v1, v2)
            input_ids = tokenizer.apply_chat_template(
                messages, add_generation_prompt=True, return_tensors="pt"
            )[0]
            input_ids_list.append(input_ids.to(device))

        input_ids_padded = torch.nn.utils.rnn.pad_sequence(
            input_ids_list, batch_first=True, padding_value=tokenizer.pad_token_id
        ).to(device)

        attention_mask = (input_ids_padded != tokenizer.pad_token_id).to(device)
        max_input_len = input_ids_padded.shape[1]
        max_new_tokens = min(2048, 4096 - max_input_len)

        with torch.no_grad():
            outputs = model.generate(
                input_ids=input_ids_padded,
                attention_mask=attention_mask,
                max_new_tokens=max_new_tokens,
                temperature=0.7,
                do_sample=True,
                num_return_sequences=1,
                pad_token_id=tokenizer.eos_token_id
            )

        responses = []
        for i, input_ids in enumerate(input_ids_list):
            generated_tokens = outputs[i][input_ids.shape[0]:]
            decoded = tokenizer.decode(generated_tokens, skip_special_tokens=True)
            responses.append({
                "prompt_id": prompt_ids_batch[i],
                "prompt": prompts_batch[i],
                "response": decoded
            })
        return responses

    # === ä¸»å¾ªç¯ ===
    for i, (v_vec, angle_deg) in enumerate(zip(valid_vs, valid_angles)):
        v1, v2 = v_vec[0], v_vec[1]
        output_file = os.path.join(result_dir, f"fdiv_v1_angle{int(angle_deg)}.csv")
        print(f"\nğŸš€ Generating for direction {i}: angle â‰ˆ {angle_deg}Â°, v = ({v1:.4f}, {v2:.4f})")

        if os.path.exists(output_file):
            existing_df = pd.read_csv(output_file)
            done_prompt_ids = set(existing_df["prompt_id"].unique())
            results = existing_df.to_dict("records")
            print(f"ğŸ” Resuming from previous run: {len(done_prompt_ids)} prompts already completed.")
        else:
            done_prompt_ids = set()
            results = []

        for start in tqdm(range(0, len(prompts), batch_size), desc=f"Generating angle {angle_deg}"):
            end = min(start + batch_size, len(prompts))
            batch_prompts_all = prompts[start:end]
            batch_ids_all = prompt_ids[start:end]

            unprocessed_indices = [j for j, pid in enumerate(batch_ids_all) if pid not in done_prompt_ids]
            if not unprocessed_indices:
                continue

            batch_prompts = [batch_prompts_all[j] for j in unprocessed_indices]
            batch_ids = [batch_ids_all[j] for j in unprocessed_indices]

            try:
                batch_outputs = generate_response_batch(batch_prompts, batch_ids, v1, v2)
                for item in batch_outputs:
                    item.update({
                        "v1_p": round(v1, 4),
                        "v2_p": round(v2, 4),
                        "direction_index": i,
                        "valid_angle": round(angle_deg, 1),
                        "main_v1": round(main_v[0], 4),
                        "main_v2": round(main_v[1], 4)
                    })
                    results.append(item)

                pd.DataFrame(batch_outputs).to_csv(
                    output_file, mode='a', index=False,
                    header=not os.path.exists(output_file)
                )

            except Exception as e:
                print(f"âš ï¸ Error at batch {start}-{end}: {e}")

        print(f"âœ… Final saved {len(results)} responses to {output_file}")

#v5

run_dpa_generation(
    result_dir="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v5_anglebased_top5",
    valid_vs=[
        np.array([0.939693, 0.342020]),  # 20Â°
        np.array([0.965926, 0.258819]),  # 15Â°
        np.array([0.906308, 0.422618]),  # 25Â°
        np.array([0.984808, 0.173648]),  # 10Â°
        np.array([0.866025, 0.5     ])   # 30Â°
    ],
    valid_angles=[20, 15, 25, 10, 30],
    main_v=np.array([0.9397, 0.3420]),  # v5
    prompt_limit=2000
)

#ä¿®å¤

import os
import pandas as pd
import numpy as np
import torch
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForCausalLM
from accelerate import Accelerator

def run_response_repair(input_dir, output_subdir="fixed"):
    """
    ä¿®å¤æŒ‡å®šç›®å½•ä¸‹æ‰€æœ‰ CSV æ–‡ä»¶ä¸­æ— æ•ˆçš„ LLM responseï¼Œå¹¶ä¿å­˜åˆ°å­ç›®å½•ä¸­ã€‚

    Args:
        input_dir (str): åŒ…å«å¾…ä¿®å¤ CSV æ–‡ä»¶çš„ç›®å½•è·¯å¾„ã€‚
        output_subdir (str): ä¿®å¤åæ–‡ä»¶ä¿å­˜çš„å­ç›®å½•åç§°ï¼Œé»˜è®¤ "fixed"ã€‚
    """

    # åˆå§‹åŒ–æ¨¡å‹ä¸ tokenizer
    device = "cuda" if torch.cuda.is_available() else "cpu"
    accelerator = Accelerator()

    model = AutoModelForCausalLM.from_pretrained(
        "Haoxiang-Wang/DPA-v1-Mistral-7B",
        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
        device_map="auto" if torch.cuda.is_available() else None
    )
    model = accelerator.prepare(model)

    tokenizer = AutoTokenizer.from_pretrained("Haoxiang-Wang/DPA-v1-Mistral-7B")
    tokenizer.padding_side = "left"
    if tokenizer.pad_token_id is None:
        tokenizer.pad_token = tokenizer.eos_token

    def is_invalid_response(resp):
        if pd.isna(resp):
            return True
        resp = str(resp).strip()
        if not resp:
            return True
        invalid_keywords = [
            "[A:]", "[/SOLUTION]", "[ASS]", "[A]:", "[/CONV]", "[/CODE]", "[/USER]",
            "[OUTPUT]", "[CC]", "[OUT]", "[/DATA]", "[/ASST]", "[/ME]", "[/REST]",
            "[RESP]", "<[user]>", "<|user|>", "[Response]", "[Assistant]", "[Answer]",
            "[End of Response]", "<human>", "<|endoftext|>", "<<", "[INST]", "[/INST]",
            "[]", "[USER]"
        ]
        tokens = resp.split()
        if all(token.strip() in invalid_keywords for token in tokens):
            return True
        tag_like_count = sum(1 for t in tokens if (t.startswith("[") and t.endswith("]")) or (t.startswith("<") and t.endswith(">")))
        return len(tokens) > 0 and tag_like_count / len(tokens) > 0.8

    def build_input(prompt, v1, v2):
        h = int(np.round(v1 * 100))
        v = int(np.round(v2 * 100))
        sys_instruction = f"You are a helpful assistant. Your response should maximize weighted rating = helpfulness*{h} + verbosity*{v}."
        return [{"role": "system", "content": sys_instruction}, {"role": "user", "content": prompt}]

    def regenerate_response(prompt, v1, v2):
        try:
            messages = build_input(prompt, v1, v2)
            input_ids = tokenizer.apply_chat_template(
                messages, add_generation_prompt=True, return_tensors="pt"
            ).to(device)

            if input_ids.ndim != 2 or input_ids.shape[1] == 0:
                print(f"âš ï¸ Invalid input_ids shape {input_ids.shape}, skipping: {prompt[:60]}")
                return ""

            outputs = model.generate(
                input_ids=input_ids,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
            generated = outputs[0][input_ids.shape[1]:]
            return tokenizer.decode(generated, skip_special_tokens=True).strip()

        except Exception as e:
            print(f"âŒ Error: {e} \nPrompt: {prompt[:60]}")
            return ""

    # åˆ›å»ºä¿å­˜ç›®å½•
    output_dir = os.path.join(input_dir, output_subdir)
    os.makedirs(output_dir, exist_ok=True)

    # éå†å¹¶ä¿®å¤æ‰€æœ‰æœªä¿®å¤æ–‡ä»¶
    for file in os.listdir(input_dir):
        if file.endswith(".csv") and not file.endswith("_fixed.csv"):
            file_path = os.path.join(input_dir, file)
            print(f"\nğŸ“‚ æ­£åœ¨æ£€æŸ¥: {file_path}")

            df = pd.read_csv(file_path)
            if "v1_p" not in df.columns or "v2_p" not in df.columns:
                print(f"âŒ ç¼ºå°‘ v1_p/v2_p åˆ—ï¼Œè·³è¿‡: {file}")
                continue

            bad_mask = df["response"].apply(is_invalid_response)
            num_bad = bad_mask.sum()

            if num_bad == 0:
                print("âœ… æ— éœ€ä¿®å¤ï¼Œè·³è¿‡ã€‚")
                continue

            print(f"ğŸ”§ å‘ç° {num_bad} æ¡æ— æ•ˆ responseï¼Œå¼€å§‹é‡ç”Ÿæˆ...")
            for i in tqdm(bad_mask[bad_mask].index, total=num_bad, desc=file, dynamic_ncols=True):
                row = df.loc[i]
                new_resp = regenerate_response(row["prompt"], row["v1_p"], row["v2_p"])
                df.at[i, "response"] = new_resp

            save_name = os.path.basename(file).replace(".csv", "_fixed.csv")
            save_path = os.path.join(output_dir, save_name)
            df.to_csv(save_path, index=False)
            print(f"âœ… ä¿®å¤å®Œæˆ â†’ å·²ä¿å­˜: {save_path}")

#v5
run_response_repair("/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v5_anglebased_top5")



#æ‰“åˆ†

import os
import pandas as pd
import numpy as np
import torch
from tqdm.auto import tqdm
from transformers import AutoTokenizer, AutoModelForSequenceClassification

def run_reward_scoring(input_dir, output_dir):
    """
    å¯¹ input_dir ä¸­æ‰€æœ‰ `_fixed.csv` æ–‡ä»¶è¿›è¡Œæ‰“åˆ†ï¼ˆhelpfulness, verbosityï¼‰ï¼Œæ”¯æŒæ–­ç‚¹ç»­è·‘ï¼Œåªæ‰“ç¼ºçš„ï¼Œä¿å­˜è‡³ output_dirã€‚

    Args:
        input_dir (str): åŒ…å«å¾…æ‰“åˆ†æ–‡ä»¶çš„è·¯å¾„
        output_dir (str): æ‰“åˆ†ç»“æœä¿å­˜è·¯å¾„
    """
    os.makedirs(output_dir, exist_ok=True)

    print("ğŸ¤– Loading Reward Model...")
    device = "cuda" if torch.cuda.is_available() else "cpu"
    rm = AutoModelForSequenceClassification.from_pretrained(
        "Haoxiang-Wang/RewardModel-Mistral-7B-for-DPA-v1", trust_remote_code=True
    ).to(device)
    tokenizer = AutoTokenizer.from_pretrained("Haoxiang-Wang/RewardModel-Mistral-7B-for-DPA-v1")

    def score_response(prompt, response):
        template = "[INST] You must read the following conversation carefully and rate the assistant's response from score 0-100 in these aspects: helpfulness, correctness, coherence, honesty, complexity, verbosity\n\nUser: {prompt}\n\nAssistant: {response} [/INST]"
        inputs = tokenizer(template.format(prompt=prompt, response=response), return_tensors="pt").to(device)
        with torch.no_grad():
            logits = rm(**inputs).logits.squeeze().cpu().numpy()
        return logits[9], logits[4]  # helpfulness, verbosity

    def score_file(file_path):
        print(f"\nğŸ“‚ Scoring file: {file_path}")
        df = pd.read_csv(file_path)

        # ç¡®ä¿ä¸¤åˆ—å­˜åœ¨
        if "helpfulness" not in df.columns:
            df["helpfulness"] = np.nan
        if "verbosity" not in df.columns:
            df["verbosity"] = np.nan

        total = len(df)
        remaining = df["helpfulness"].isna().sum()
        print(f"ğŸ” Total: {total} rows, Remaining to score: {remaining}")

        for i, row in tqdm(df.iterrows(), total=total, desc=os.path.basename(file_path), dynamic_ncols=True):
            if pd.notnull(row["helpfulness"]) and pd.notnull(row["verbosity"]):
                continue  # å·²ç»æ‰“è¿‡åˆ†ï¼Œè·³è¿‡
            try:
                h, v = score_response(row["prompt"], row["response"])
            except Exception as e:
                print(f"âš ï¸ Error on row {i}: {e}")
                h, v = None, None
            df.loc[i, "helpfulness"] = h
            df.loc[i, "verbosity"] = v

            # æ¯æ‰“å®Œä¸€ä¸ªå¯ä»¥å³æ—¶ä¿å­˜ï¼ˆå¯é€‰ï¼Œå¦‚æœä½ æ‹…å¿ƒæ–­ç”µï¼‰ï¼š
            # df.to_csv(save_path, index=False)

        # ä¿å­˜æœ€ç»ˆç»“æœ
        file_name = os.path.basename(file_path).replace(".csv", "_scored.csv")
        save_path = os.path.join(output_dir, file_name)
        df.to_csv(save_path, index=False)
        print(f"âœ… Scoring done â†’ Saved to: {save_path}")
        return df

    for file in os.listdir(input_dir):
        if file.endswith("_fixed.csv"):
            score_file(os.path.join(input_dir, file))

#v5æ‰“åˆ†
run_reward_scoring(
    input_dir="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v5_anglebased_top5/fixed",
    output_dir="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v5_anglebased_top5/scored"
)

#divé€‰å‡ºbest response

import os
import pandas as pd

def merge_and_select_best_responses(input_dir, output_file, main_v1=0.7071, main_v2=0.7071):
    """
    åˆå¹¶æŒ‡å®šç›®å½•ä¸­çš„ `_scored.csv` æ–‡ä»¶ï¼Œè®¡ç®—ä¸»æ–¹å‘åŠ æƒå¾—åˆ†ï¼Œå¹¶ä¸ºæ¯ä¸ª prompt é€‰å‡ºå¾—åˆ†æœ€é«˜çš„å“åº”ã€‚

    Args:
        input_dir (str): åŒ…å« `_scored.csv` æ–‡ä»¶çš„ç›®å½•è·¯å¾„
        output_file (str): æœ€ç»ˆåˆå¹¶åçš„è¾“å‡ºæ–‡ä»¶è·¯å¾„
        main_v1 (float): ä¸»æ–¹å‘åœ¨ helpfulness ç»´åº¦ä¸Šçš„æƒé‡
        main_v2 (float): ä¸»æ–¹å‘åœ¨ verbosity ç»´åº¦ä¸Šçš„æƒé‡
    """
    dfs = []
    for file in os.listdir(input_dir):
        if file.endswith("_scored.csv"):
            df = pd.read_csv(os.path.join(input_dir, file))
            df["main_v1"] = main_v1
            df["main_v2"] = main_v2
            df["score_total"] = df["main_v1"] * df["helpfulness"] + df["main_v2"] * df["verbosity"]
            dfs.append(df)

    df_all = pd.concat(dfs, ignore_index=True)
    df_best = df_all.loc[df_all.groupby("prompt_id")["score_total"].idxmax()]
    df_best = df_best.rename(columns={"response": "f_best_response"})

    cols_to_keep = [
        "prompt_id", "prompt", "f_best_response",
        "v1_p", "v2_p", "valid_angle",
        "main_v1", "main_v2",
        "helpfulness", "verbosity", "score_total"
    ]
    df_best_clean = df_best[cols_to_keep]

    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    df_best_clean.to_csv(output_file, index=False)
    print(f"âœ… å·²ä¿å­˜åˆ°: {output_file}")

#v5 basleienè¡¥å……

from google.colab import drive
import os
import numpy as np
import pandas as pd
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM
from tqdm.auto import tqdm
import torch
import time
import random

# âœ… è®¾ç½® v_index ä¸º5ï¼Œå¯¹åº” v5 = (0.9397, 0.3420)
v_indices = [5]
v_mapping = {5: (0.9397, 0.3420)}

# âœ… æŒ‚è½½ Google Drive
drive.mount('/content/drive')

# âœ… ç¯å¢ƒè®¾ç½®
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
seed = 42
np.random.seed(seed)
random.seed(seed)
torch.manual_seed(seed)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(seed)

# âœ… è¾“å‡ºç›®å½•
result_dir = "/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9"
os.makedirs(result_dir, exist_ok=True)

# âœ… è½½å…¥æ•°æ®é›†
print("ğŸ“¦ Loading prompts from UltraFeedback...")
ds = load_dataset("HuggingFaceH4/ultrafeedback_binarized", split="test_prefs")
prompts = ds["prompt"][:2000]
all_prompt_ids = list(range(len(prompts)))

# âœ… åŠ è½½æ¨¡å‹
print("ğŸ¤– Loading DPA-v1 model...")
model = AutoModelForCausalLM.from_pretrained(
    "Haoxiang-Wang/DPA-v1-Mistral-7B",
    torch_dtype=torch.float16
).to(device)

tokenizer = AutoTokenizer.from_pretrained("Haoxiang-Wang/RewardModel-Mistral-7B-for-DPA-v1")
tokenizer.padding_side = "left"
if tokenizer.pad_token_id is None:
    tokenizer.pad_token = tokenizer.eos_token

# âœ… æ„é€  Prompt
def build_input(prompt, v1, v2):
    h = int(np.round(v1 * 100))
    v = int(np.round(v2 * 100))
    sys_instruction = f"You are a helpful assistant. Your response should maximize weighted rating = helpfulness*{h} + verbosity*{v}."
    return [{"role": "user", "content": f"{sys_instruction}\n\n{prompt}"}]

# âœ… æ‰¹é‡ç”Ÿæˆå‡½æ•°
def generate_response_batch(prompts_batch, prompt_ids_batch, v1, v2):
    input_ids_list = []
    for prompt in prompts_batch:
        messages = build_input(prompt, v1, v2)
        input_ids = tokenizer.apply_chat_template(
            messages, add_generation_prompt=True, return_tensors="pt"
        )[0]
        input_ids_list.append(input_ids.to(device))

    input_ids_padded = torch.nn.utils.rnn.pad_sequence(
        input_ids_list, batch_first=True, padding_value=tokenizer.pad_token_id
    ).to(device)

    attention_mask = (input_ids_padded != tokenizer.pad_token_id).to(device)
    max_input_len = input_ids_padded.shape[1]
    dynamic_max_new_tokens = min(2048, 4096 - max_input_len)

    with torch.no_grad():
        outputs = model.generate(
            input_ids=input_ids_padded,
            attention_mask=attention_mask,
            max_new_tokens=dynamic_max_new_tokens,
            temperature=0.7,
            do_sample=True,
            num_return_sequences=3,
            pad_token_id=tokenizer.eos_token_id
        )

    responses = []
    for i in range(len(prompts_batch)):
        prompt_responses = []
        for j in range(3):
            idx = i * 3 + j
            generated_tokens = outputs[idx][input_ids_padded.shape[1]:]
            decoded = tokenizer.decode(generated_tokens, skip_special_tokens=True)
            prompt_responses.append(decoded)
        responses.append({
            "prompt_id": prompt_ids_batch[i],
            "prompt": prompts_batch[i],
            "responses": prompt_responses
        })
    return responses

# âœ… ä¸»é€»è¾‘ï¼šæ–­ç‚¹ç»­è·‘ + å®Œæ•´å•æ¡ç¼ºè¡¥é€»è¾‘
batch_size = 8

for v_index in v_indices:
    v1, v2 = v_mapping[v_index]
    output_file = os.path.join(result_dir, f"batch_v{v_index}.csv")
    print(f"\nğŸ§­ Running v{v_index}: v = ({v1:.4f}, {v2:.4f})")

    if os.path.exists(output_file):
        existing_df = pd.read_csv(output_file)
        done_prompt_ids = set(existing_df["prompt_id"].unique())
        print(f"ğŸ” Loaded {len(done_prompt_ids)} prompts from previous run.")
    else:
        done_prompt_ids = set()

    remaining_prompt_ids = [pid for pid in all_prompt_ids if pid not in done_prompt_ids]
    print(f"ğŸ” Remaining prompts to process: {len(remaining_prompt_ids)}")

    start_time = time.time()

    for i in tqdm(range(0, len(remaining_prompt_ids), batch_size), desc=f"Generating v{v_index}"):
        batch_prompt_ids = remaining_prompt_ids[i:i+batch_size]
        prompt_batch = [prompts[pid] for pid in batch_prompt_ids]

        try:
            batch_results = generate_response_batch(prompt_batch, batch_prompt_ids, v1, v2)
            new_rows = []
            for item in batch_results:
                for k, resp in enumerate(item["responses"]):
                    row = {
                        "prompt_id": item["prompt_id"],
                        "v1": v1,
                        "v2": v2,
                        "prompt": item["prompt"],
                        "response_id": k + 1,
                        "response": resp
                    }
                    new_rows.append(row)
            pd.DataFrame(new_rows).to_csv(output_file, mode='a', header=not os.path.exists(output_file), index=False)
        except Exception as e:
            print(f"âš ï¸ Error at batch {i}-{i+batch_size}: {e}")

    print(f"âœ… Completed v{v_index}. Total time: {time.time() - start_time:.1f}s")

#ä¿®å¤v5 basleine
run_single_file_repair("drive/MyDrive/llm_pre_project/dpa_outputs_v9/batch_v5.csv")

import pandas as pd

# è¯»å–CSVæ–‡ä»¶
file_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/fixed_batch/batch_v5_fixed.csv"
df = pd.read_csv(file_path)

# æŸ¥çœ‹æ•°æ®è¡Œæ•°ï¼ˆå»æ‰headerè¡Œï¼‰
print("æ•°æ®æ€»è¡Œæ•°ï¼ˆå«è¡¨å¤´ï¼‰:", len(df))
print("æ•°æ®æ€»è¡Œæ•°ï¼ˆå»æ‰è¡¨å¤´ï¼‰:", df.shape[0])

#v5 basleineæ‰“åˆ†
score_fixed_csv_file(
    input_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/fixed_batch/batch_v5_fixed.csv",
    output_dir="drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch"
)

#v5 basleine best response
process_scored_csv_and_extract_best(
    input_file="drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch/batch_v5_fixed_scored.csv",
    output_dir="drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch/best_response",
    best_response_filename="best_response_v5_baseline.csv"
)

merge_and_select_best_responses(
    input_dir="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v5_anglebased_top5/scored",
    output_file="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v5_anglebased_top5/fdiv_anglebased_v5_all_best_response.csv"
)

#åˆå¹¶fdivå’Œbaseline

import pandas as pd
import os

def merge_baseline_and_fdiv_results(baseline_path, fdiv_path, output_path):
    """
    åˆå¹¶ baseline å’Œ f-div çš„è¯„åˆ†ç»“æœï¼ŒæŒ‰ prompt_id å¯¹é½ï¼Œå¹¶è¾“å‡ºåˆå¹¶æ–‡ä»¶ä¾›è¯„ä¼°ä½¿ç”¨ã€‚

    Args:
        baseline_path (str): baseline CSV æ–‡ä»¶è·¯å¾„
        fdiv_path (str): f-div CSV æ–‡ä»¶è·¯å¾„
        output_path (str): åˆå¹¶ç»“æœè¾“å‡ºè·¯å¾„ï¼ˆåŒ…æ‹¬æ–‡ä»¶åï¼‰
    """
    # === åŠ è½½æ•°æ® ===
    df_baseline = pd.read_csv(baseline_path)
    df_fdiv = pd.read_csv(fdiv_path)

    # === åˆå¹¶æ•°æ®ï¼ˆæŒ‰ prompt_idï¼‰
    df_merged = pd.merge(
        df_baseline,
        df_fdiv,
        on="prompt_id",
        suffixes=("_baseline", "_fdiv")
    )

    # === å­—æ®µé‡å‘½å ===
    df_merged = df_merged.rename(columns={
        "prompt_baseline": "prompt",
        "best_response": "baseline_best_response",
        "score_total": "f_score_total",
        "total_score": "baseline_score_total"
    })

    # === ä¿ç•™å­—æ®µ
    cols_to_show = [
        "prompt_id", "prompt",
        "baseline_best_response", "f_best_response",
        "baseline_score_total", "f_score_total",
        "response_id",
        "v1_p", "v2_p", "valid_angle",
        "main_v1", "main_v2"
    ]
    df_clean = df_merged[cols_to_show]

    # === ä¿å­˜ç»“æœ ===
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    df_clean.to_csv(output_path, index=False)
    print(f"âœ… åˆå¹¶å®Œæˆï¼Œæ–‡ä»¶ä¿å­˜è‡³ï¼š{output_path}")

#åˆå¹¶v5 baselineå’Œfdiv
merge_baseline_and_fdiv_results(
    baseline_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch/best_response/best_response/best_response_v5_baseline.csv",
    fdiv_path="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v5_anglebased_top5/fdiv_anglebased_v5_all_best_response.csv",
    output_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/angle_output/v5_anglebased_final_merged_for_judge.csv"
)

import pandas as pd

# è¯»å–CSVæ–‡ä»¶
file_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/angle_output/v10_anglebased_final_merged_for_judge.csv"
df = pd.read_csv(file_path)

# æŸ¥çœ‹æ•°æ®è¡Œæ•°ï¼ˆå»æ‰headerè¡Œï¼‰
print("æ•°æ®æ€»è¡Œæ•°ï¼ˆå«è¡¨å¤´ï¼‰:", len(df))
print("æ•°æ®æ€»è¡Œæ•°ï¼ˆå»æ‰è¡¨å¤´ï¼‰:", df.shape[0])

!pip install openai==0.28

#gpt4æ‰“åˆ†

import pandas as pd
import json
import os
import random

def generate_pairwise_jsonl_for_gpt_judging(input_path, output_path, seed=42):
    """
    æ„é€ ç”¨äº GPT-4o åˆ¤æ–­çš„ A/B Pairwise JSONL æ ¼å¼æ•°æ®ï¼ˆå¸¦éšæœºé¡ºåºå’Œ auto-tieï¼‰

    Args:
        input_path (str): è¾“å…¥ CSV æ–‡ä»¶è·¯å¾„ï¼ˆéœ€åŒ…å« prompt, baseline_best_response, f_best_response ç­‰å­—æ®µï¼‰
        output_path (str): è¾“å‡º JSONL æ–‡ä»¶è·¯å¾„
        seed (int): éšæœºç§å­ï¼Œç¡®ä¿å¯å¤ç°çš„ A/B é¡ºåº
    """
    # === ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨ ===
    os.makedirs(os.path.dirname(output_path), exist_ok=True)

    # === è¯»å–åˆå¹¶æ•°æ® ===
    df = pd.read_csv(input_path)

    # === è®¾ç½®éšæœºç§å­ ===
    random.seed(seed)

    output = []
    tie_count = 0

    for idx, row in df.iterrows():
        prompt = row["prompt"]
        baseline = str(row["baseline_best_response"]) if pd.notna(row["baseline_best_response"]) else ""
        fdiv = str(row["f_best_response"]) if pd.notna(row["f_best_response"]) else ""
        pid = row["prompt_id"]
        baseline_response_id = row.get("response_id", None)

        if baseline.strip() == fdiv.strip():
            output.append({
                "pair_id": idx,
                "prompt_id": pid,
                "auto_result": "Tie",
                "baseline_id": baseline_response_id,
                "a_origin": "baseline",
                "b_origin": "f-div",
                "formatted_prompt": "[HH-RLHF]: (Responses identical, auto tie)"
            })
            tie_count += 1
            continue

        # éšæœº A/B
        if random.random() < 0.5:
            response_a, response_b = baseline, fdiv
            a_origin, b_origin = "baseline", "f-div"
        else:
            response_a, response_b = fdiv, baseline
            a_origin, b_origin = "f-div", "baseline"

        formatted_prompt = f"""[HH-RLHF]: For the following query to a chatbot, which response is more helpful?

Query: {prompt}

Response A: {response_a}

Response B: {response_b}

FIRST provide a one-sentence comparison of the two responses and explain which you feel is more helpful.
SECOND, on a new line, state only 'A' or 'B' to indicate which response is more helpful.

Format:
Comparison: ...
More helpful: A/B"""

        output.append({
            "pair_id": idx,
            "prompt_id": pid,
            "auto_result": None,
            "baseline_id": baseline_response_id,
            "a_origin": a_origin,
            "b_origin": b_origin,
            "formatted_prompt": formatted_prompt
        })

    # === ä¿å­˜ JSONL ===
    with open(output_path, "w", encoding="utf-8") as f:
        for item in output:
            f.write(json.dumps(item, ensure_ascii=False) + "\n")

    print(f"âœ… å·²æ„é€  JSONL æ–‡ä»¶ï¼Œå…± {len(output)} æ¡ï¼Œè‡ªåŠ¨ Tie: {tie_count}")
    print(f"ğŸ“ ä¿å­˜è‡³ï¼š{output_path}")

generate_pairwise_jsonl_for_gpt_judging(
    input_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/angle_output/v5_anglebased_final_merged_for_judge.csv",
    output_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/gpt4_judge_inputs_anglebased/v5_pairwise_randomized_angle.jsonl"
)

import pandas as pd

# è¯»å–CSVæ–‡ä»¶
file_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/angle_output/v5_anglebased_final_merged_for_judge.csv"
df = pd.read_csv(file_path)

# æŸ¥çœ‹æ•°æ®è¡Œæ•°ï¼ˆå»æ‰headerè¡Œï¼‰
print("æ•°æ®æ€»è¡Œæ•°ï¼ˆå«è¡¨å¤´ï¼‰:", len(df))
print("æ•°æ®æ€»è¡Œæ•°ï¼ˆå»æ‰è¡¨å¤´ï¼‰:", df.shape[0])

import openai
import osrps

# è®¾ç½® API Key å’Œä»£ç†åœ°å€ï¼ˆå¦‚æœä½¿ç”¨ä»£ç†ï¼‰
os.environ["OPENAI_API_KEY"] = "sk-XGGe5y0ZvLcQVFp6XnRizs7q47gsVnAbZx0Xr2mfcVlbr99f"
openai.api_key = os.environ["OPENAI_API_KEY"]
openai.api_base = "https://api2.aigcbest.top/v1"  # å¦‚æœç”¨ä»£ç†å°±æ”¹è¿™é‡Œ

print("âœ… OpenAI è®¾ç½®å®Œæˆ")

import openai
import json
import time
import os
from tqdm import tqdm

def run_gpt_judging(input_path, model="gpt-4o", sleep_time=1.0, max_retries=3):
    """
    ä½¿ç”¨ OpenAI GPT æ¨¡å‹å¯¹ jsonl æ ¼å¼çš„ A/B Pairwise æ•°æ®è¿›è¡Œè‡ªåŠ¨è¯„ä¼°ã€‚

    Args:
        input_path (str): è¾“å…¥ JSONL æ–‡ä»¶è·¯å¾„ï¼ˆåŒ…å« formatted_prompt å­—æ®µï¼‰
        model (str): æ‰€ç”¨ OpenAI æ¨¡å‹åï¼ˆå¦‚ "gpt-4o"ï¼‰
        sleep_time (float): æ¯è½®ä¹‹é—´æš‚åœæ—¶é—´ï¼ˆç§’ï¼‰
        max_retries (int): æ¯æ¡æ•°æ®æœ€å¤§é‡è¯•æ¬¡æ•°
    """
    output_path = input_path.replace(".jsonl", "_results.jsonl")
    os.makedirs(os.path.dirname(output_path), exist_ok=True)

    # === åŠ è½½è¾“å…¥æ•°æ® ===
    with open(input_path, "r", encoding="utf-8") as f:
        all_prompts = [json.loads(line) for line in f]

    # === æ–­ç‚¹ç»­è·‘ ===
    completed_ids = set()
    results = []
    if os.path.exists(output_path):
        with open(output_path, "r", encoding="utf-8") as f:
            for line in f:
                item = json.loads(line)
                results.append(item)
                completed_ids.add(item["pair_id"])
        print(f"ğŸ” å·²åŠ è½½ {len(completed_ids)} æ¡å†å²ç»“æœï¼Œè·³è¿‡")

    start_time = time.time()

    # === éå†æ•°æ®è¯„ä¼° ===
    for item in tqdm(all_prompts, desc=f"ğŸ§  {model} è¯„ä¼°ä¸­"):
        pid = item["pair_id"]
        if pid in completed_ids:
            continue

        if item.get("auto_result") == "Tie":
            item["gpt_judgment"] = "Tie"
            print(f"ğŸ¤ pair_id={pid} â†’ Auto-Tie")
            results.append(item)
            continue

        prompt = item["formatted_prompt"]
        for attempt in range(max_retries):
            try:
                response = openai.ChatCompletion.create(
                    model=model,
                    messages=[{"role": "user", "content": prompt}],
                    temperature=0.0,
                )
                reply = response.choices[0].message["content"].strip()
                item["gpt_raw_response"] = reply

                last_line = reply.strip().splitlines()[-1].strip().upper()
                if "MORE HELPFUL: A" in last_line or last_line == "A":
                    item["gpt_judgment"] = "A"
                elif "MORE HELPFUL: B" in last_line or last_line == "B":
                    item["gpt_judgment"] = "B"
                else:
                    item["gpt_judgment"] = "Unclear"

                print(f"âœ… pair_id={pid} â†’ {item['gpt_judgment']}")
                break
            except Exception as e:
                item["error"] = str(e)
                print(f"âŒ pair_id={pid} â†’ Error: {str(e)}")
                time.sleep(sleep_time)
        else:
            item["gpt_judgment"] = "Error"
            print(f"âŒ pair_id={pid} â†’ Failed after {max_retries} attempts")

        results.append(item)
        time.sleep(sleep_time)

        # å®æ—¶å†™å…¥æ–‡ä»¶ï¼ˆå¯é€‰ï¼Œé¿å…ä¸­æ–­ï¼‰
        with open(output_path, "w", encoding="utf-8") as f:
            for r in results:
                f.write(json.dumps(r, ensure_ascii=False) + "\n")

    end_time = time.time()
    print(f"\nâœ… è¯„ä¼°å®Œæˆï¼Œå…±è®¡ {len(results)} æ¡")
    print(f"ğŸ•’ è€—æ—¶ï¼š{end_time - start_time:.1f} ç§’")
    print(f"ğŸ“ è¾“å‡ºç»“æœè·¯å¾„ï¼š{output_path}")

#v5æ‰“åˆ†
run_gpt_judging(
    input_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/gpt4_judge_inputs_anglebased/v5_pairwise_randomized_angle.jsonl",
    model="gpt-4o-mini"
)

#ç»Ÿè®¡

import json
from collections import Counter
import pandas as pd
import matplotlib.pyplot as plt

def analyze_gpt_judgment_results(results_path):
    """
    åˆ†æ GPT è¯„å®¡ç»“æœï¼Œå¹¶å¯è§†åŒ–ä¸åŒé€‰é¡¹ï¼ˆbaseline / f-div / Tie / Errorï¼‰çš„å æ¯”ã€‚

    Args:
        results_path (str): JSONL è¯„å®¡ç»“æœæ–‡ä»¶è·¯å¾„ï¼Œéœ€åŒ…å«å­—æ®µ gpt_judgment, a_origin, b_origin
    """

    # === åŠ è½½æ•°æ® ===
    with open(results_path, "r", encoding="utf-8") as f:
        data = [json.loads(line) for line in f]

    # === åˆ¤å®šè·èƒœæ–¹å‡½æ•° ===
    def judge_winner(item):
        judgment = item.get("gpt_judgment", "")
        a_origin = item.get("a_origin")
        b_origin = item.get("b_origin")

        if judgment == "Tie":
            return "Tie"
        elif judgment == "A":
            return a_origin
        elif judgment == "B":
            return b_origin
        elif judgment == "Unclear":
            return "Unclear"
        elif judgment == "Error":
            return "Error"
        else:
            return "Invalid"

    # === ç»Ÿè®¡å„ç±»ç»“æœ ===
    outcomes = [judge_winner(item) for item in data]
    counter = Counter(outcomes)

    total = sum(counter.values())
    win_fdiv = counter.get("f-div", 0)
    win_baseline = counter.get("baseline", 0)
    tie = counter.get("Tie", 0)
    unclear = counter.get("Unclear", 0)
    error = counter.get("Error", 0)

    summary = pd.DataFrame([{
        "Total": total,
        "f-div wins": win_fdiv,
        "baseline wins": win_baseline,
        "Ties": tie,
        "Unclear": unclear,
        "Error": error,
        "f-div win rate (%)": round(win_fdiv / total * 100, 2),
        "baseline win rate (%)": round(win_baseline / total * 100, 2),
        "Tie rate (%)": round(tie / total * 100, 2),
        "Unclear/Error rate (%)": round((unclear + error) / total * 100, 2)
    }])

    print(summary)

    # === å¯è§†åŒ–ç»Ÿè®¡å›¾ ===
    summary_plot = {
        "f-div": win_fdiv,
        "baseline": win_baseline,
        "Tie": tie,
        "Unclear": unclear,
        "Error": error
    }

    plt.figure(figsize=(8, 4))
    bars = plt.bar(summary_plot.keys(), summary_plot.values(), color=["#2ca02c", "#1f77b4", "#ff7f0e", "#999999", "#d62728"])
    plt.title("GPT-4o Judged Outcome (Randomized A/B)", fontsize=14)
    plt.ylabel("Count")
    plt.xticks(fontsize=10)
    plt.grid(axis="y", linestyle="--", alpha=0.5)

    for bar, label in zip(bars, summary_plot.keys()):
        count = bar.get_height()
        pct = count / total * 100 if total else 0
        plt.text(bar.get_x() + bar.get_width() / 2, count + 2, f"{pct:.1f}%", ha="center", va="bottom", fontsize=10)

    plt.tight_layout()
    plt.show()

    return summary

analyze_gpt_judgment_results("drive/MyDrive/llm_pre_project/dpa_outputs_v9/gpt4_judge_inputs_anglebased/v5_pairwise_randomized_angle_results.jsonl")

#v6

run_dpa_generation(
    result_dir="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v6_anglebased_top5",
    valid_vs=[
        np.array([0.906308, 0.422618]),  # 25Â°
        np.array([0.939693, 0.342020]),  # 20Â°
        np.array([0.866025, 0.5     ]),  # 30Â°
        np.array([0.965926, 0.258819]),  # 15Â°
        np.array([0.819152, 0.573576])   # 35Â°
    ],
    valid_angles=[25, 20, 30, 15, 35],
    main_v=np.array([0.9063, 0.4226]),  # v6
    prompt_limit=2000  # å¯æŒ‰éœ€è°ƒæ•´
)

#v6ä¿®å¤
run_response_repair("/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v6_anglebased_top5")

#v6æ‰“åˆ†
run_reward_scoring(
    input_dir="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v6_anglebased_top5/fixed",
    output_dir="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v6_anglebased_top5/scored"
)

#v6 fdivé€‰å‡ºbest response

merge_and_select_best_responses(
    input_dir="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v6_anglebased_top5/scored",
    output_file="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v6_anglebased_top5/fdiv_anglebased_v6_all_best_response.csv"
)

#v6 baslienè¡¥å……

from google.colab import drive
import os
import numpy as np
import pandas as pd
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM
from tqdm.auto import tqdm
import torch
import time
import random

# âœ… è®¾ç½® v_index ä¸º6ï¼Œå¯¹åº” v6 = (0.9063, 0.4226)
v_indices = [6]
v_mapping = {6: (0.9063, 0.4226)}

# âœ… æŒ‚è½½ Google Drive
drive.mount('/content/drive')

# âœ… ç¯å¢ƒè®¾ç½®
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
seed = 42
np.random.seed(seed)
random.seed(seed)
torch.manual_seed(seed)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(seed)

# âœ… è¾“å‡ºç›®å½•
result_dir = "/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9"
os.makedirs(result_dir, exist_ok=True)

# âœ… è½½å…¥æ•°æ®é›†
print("ğŸ“¦ Loading prompts from UltraFeedback...")
ds = load_dataset("HuggingFaceH4/ultrafeedback_binarized", split="test_prefs")
prompts = ds["prompt"][:2000]
all_prompt_ids = list(range(len(prompts)))

# âœ… åŠ è½½æ¨¡å‹
print("ğŸ¤– Loading DPA-v1 model...")
model = AutoModelForCausalLM.from_pretrained(
    "Haoxiang-Wang/DPA-v1-Mistral-7B",
    torch_dtype=torch.float16
).to(device)

tokenizer = AutoTokenizer.from_pretrained("Haoxiang-Wang/RewardModel-Mistral-7B-for-DPA-v1")
tokenizer.padding_side = "left"
if tokenizer.pad_token_id is None:
    tokenizer.pad_token = tokenizer.eos_token

# âœ… æ„é€  Prompt
def build_input(prompt, v1, v2):
    h = int(np.round(v1 * 100))
    v = int(np.round(v2 * 100))
    sys_instruction = f"You are a helpful assistant. Your response should maximize weighted rating = helpfulness*{h} + verbosity*{v}."
    return [{"role": "user", "content": f"{sys_instruction}\n\n{prompt}"}]

# âœ… æ‰¹é‡ç”Ÿæˆå‡½æ•°
def generate_response_batch(prompts_batch, prompt_ids_batch, v1, v2):
    input_ids_list = []
    for prompt in prompts_batch:
        messages = build_input(prompt, v1, v2)
        input_ids = tokenizer.apply_chat_template(
            messages, add_generation_prompt=True, return_tensors="pt"
        )[0]
        input_ids_list.append(input_ids.to(device))

    input_ids_padded = torch.nn.utils.rnn.pad_sequence(
        input_ids_list, batch_first=True, padding_value=tokenizer.pad_token_id
    ).to(device)

    attention_mask = (input_ids_padded != tokenizer.pad_token_id).to(device)
    max_input_len = input_ids_padded.shape[1]
    dynamic_max_new_tokens = min(2048, 4096 - max_input_len)

    with torch.no_grad():
        outputs = model.generate(
            input_ids=input_ids_padded,
            attention_mask=attention_mask,
            max_new_tokens=dynamic_max_new_tokens,
            temperature=0.7,
            do_sample=True,
            num_return_sequences=3,
            pad_token_id=tokenizer.eos_token_id
        )

    responses = []
    for i in range(len(prompts_batch)):
        prompt_responses = []
        for j in range(3):
            idx = i * 3 + j
            generated_tokens = outputs[idx][input_ids_padded.shape[1]:]
            decoded = tokenizer.decode(generated_tokens, skip_special_tokens=True)
            prompt_responses.append(decoded)
        responses.append({
            "prompt_id": prompt_ids_batch[i],
            "prompt": prompts_batch[i],
            "responses": prompt_responses
        })
    return responses

# âœ… ä¸»é€»è¾‘ï¼šæ–­ç‚¹ç»­è·‘ + å®Œæ•´å•æ¡ç¼ºè¡¥é€»è¾‘
batch_size = 8

for v_index in v_indices:
    v1, v2 = v_mapping[v_index]
    output_file = os.path.join(result_dir, f"batch_v{v_index}.csv")
    print(f"\nğŸ§­ Running v{v_index}: v = ({v1:.4f}, {v2:.4f})")

    if os.path.exists(output_file):
        existing_df = pd.read_csv(output_file)
        done_prompt_ids = set(existing_df["prompt_id"].unique())
        print(f"ğŸ” Loaded {len(done_prompt_ids)} prompts from previous run.")
    else:
        done_prompt_ids = set()

    remaining_prompt_ids = [pid for pid in all_prompt_ids if pid not in done_prompt_ids]
    print(f"ğŸ” Remaining prompts to process: {len(remaining_prompt_ids)}")

    start_time = time.time()

    for i in tqdm(range(0, len(remaining_prompt_ids), batch_size), desc=f"Generating v{v_index}"):
        batch_prompt_ids = remaining_prompt_ids[i:i+batch_size]
        prompt_batch = [prompts[pid] for pid in batch_prompt_ids]

        try:
            batch_results = generate_response_batch(prompt_batch, batch_prompt_ids, v1, v2)
            new_rows = []
            for item in batch_results:
                for k, resp in enumerate(item["responses"]):
                    row = {
                        "prompt_id": item["prompt_id"],
                        "v1": v1,
                        "v2": v2,
                        "prompt": item["prompt"],
                        "response_id": k + 1,
                        "response": resp
                    }
                    new_rows.append(row)
            pd.DataFrame(new_rows).to_csv(output_file, mode='a', header=not os.path.exists(output_file), index=False)
        except Exception as e:
            print(f"âš ï¸ Error at batch {i}-{i+batch_size}: {e}")

    print(f"âœ… Completed v{v_index}. Total time: {time.time() - start_time:.1f}s")

#ä¿®å¤v6 basleine
run_single_file_repair("drive/MyDrive/llm_pre_project/dpa_outputs_v9/batch_v6.csv")

#baseline æ‰“åˆ†

import os
import time
import pandas as pd
import numpy as np
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
from tqdm.auto import tqdm
from google.colab import drive

def score_fixed_csv_file(input_path, output_dir, model_name="Haoxiang-Wang/RewardModel-Mistral-7B-for-DPA-v1"):
    """
    å¯¹å›ºå®šåçš„ CSV æ–‡ä»¶è¿›è¡Œæ‰“åˆ†å¹¶ä¿å­˜ç»“æœï¼ˆæ”¯æŒæ–­ç‚¹ç»­è·‘ï¼‰ã€‚
    """
    drive.mount('/content/drive', force_remount=False)
    os.makedirs(output_dir, exist_ok=True)

    print(f"ğŸ“¦ Loading file: {input_path}")
    df = pd.read_csv(input_path)

    # ç¡®ä¿ helpfulness å’Œ verbosity åˆ—å­˜åœ¨
    if "helpfulness" not in df.columns:
        df["helpfulness"] = np.nan
    if "verbosity" not in df.columns:
        df["verbosity"] = np.nan

    print("ğŸ¤– Loading Reward Model...")
    device = "cuda" if torch.cuda.is_available() else "cpu"
    rm = AutoModelForSequenceClassification.from_pretrained(
        model_name, trust_remote_code=True
    ).to(device)
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    def score_response(prompt, response):
        template = "[INST] You must read the following conversation carefully and rate the assistant's response from score 0-100 in these aspects: helpfulness, correctness, coherence, honesty, complexity, verbosity\n\nUser: {prompt}\n\nAssistant: {response} [/INST]"
        inputs = tokenizer(template.format(prompt=prompt, response=response), return_tensors="pt").to(device)
        with torch.no_grad():
            logits = rm(**inputs).logits.squeeze().cpu().numpy()
        return logits[9], logits[4]

    print("ğŸ§  Scoring responses...")
    start_time = time.time()

    total_rows = len(df)
    remaining = df[df["helpfulness"].isna()].shape[0]
    print(f"Total rows: {total_rows}, Remaining to score: {remaining}")

    for i, row in tqdm(df.iterrows(), total=total_rows, desc="ğŸ” Scoring"):
        if pd.notnull(row["helpfulness"]) and pd.notnull(row["verbosity"]):
            continue  # å·²æ‰“è¿‡åˆ†ï¼Œè·³è¿‡
        try:
            h, v = score_response(row["prompt"], row["response"])
            df.loc[i, "helpfulness"] = h
            df.loc[i, "verbosity"] = v
        except Exception as e:
            print(f"âš ï¸ Error on row {i}: {e}")

    output_file = os.path.join(output_dir, os.path.basename(input_path).replace(".csv", "_scored.csv"))
    df.to_csv(output_file, index=False)

    end_time = time.time()
    print(f"\nâœ… Scoring complete! Total scored: {len(df)}")
    print(f"ğŸ•’ Time elapsed: {end_time - start_time:.1f} seconds")
    print(f"ğŸ“ Saved to: {output_file}")

#v6 baselineæ‰“åˆ†

score_fixed_csv_file(
    input_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/fixed_batch/batch_v6_fixed.csv",
    output_dir="drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch"
)

import pandas as pd

# è¯»å–CSVæ–‡ä»¶
file_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/fixed_batch/batch_v6_fixed.csv"
df = pd.read_csv(file_path)

# æŸ¥çœ‹æ•°æ®è¡Œæ•°ï¼ˆå»æ‰headerè¡Œï¼‰
print("æ•°æ®æ€»è¡Œæ•°ï¼ˆå«è¡¨å¤´ï¼‰:", len(df))
print("æ•°æ®æ€»è¡Œæ•°ï¼ˆå»æ‰è¡¨å¤´ï¼‰:", df.shape[0])

#é€‰å‡ºbaseline best response

import pandas as pd
import os

def process_scored_csv_and_extract_best(
    input_file,
    output_dir,
    best_response_filename="best_response_v1_baseline.csv"
):
    """
    å¯¹æ‰“åˆ†åçš„CSVæ–‡ä»¶æ·»åŠ total_scoreï¼Œå¹¶æå–æ¯ä¸ªprompt_idçš„æœ€ä½³responseã€‚

    Args:
        input_file (str): è¾“å…¥CSVæ–‡ä»¶è·¯å¾„ï¼ˆéœ€åŒ…å« columns: v1, v2, helpfulness, verbosityï¼‰
        output_dir (str): è¾“å‡ºç›®å½•ï¼Œå°†ä¿å­˜åŒ…å«total_scoreçš„æ–‡ä»¶å’Œbest_responseæ–‡ä»¶
        best_response_filename (str): æœ€ä½³responseè¾“å‡ºæ–‡ä»¶å
    """
    # åˆ›å»ºå¿…è¦è·¯å¾„
    os.makedirs(output_dir, exist_ok=True)
    best_response_dir = os.path.join(output_dir, "best_response")
    os.makedirs(best_response_dir, exist_ok=True)

    # è¾“å‡ºè·¯å¾„
    output_file_with_total = os.path.join(output_dir, os.path.basename(input_file).replace(".csv", "_with_total.csv"))
    output_best_file = os.path.join(best_response_dir, best_response_filename)

    # åŠ è½½æ•°æ®
    df = pd.read_csv(input_file)

    # è®¡ç®—æ€»åˆ†
    df["total_score"] = df["v1"] * df["helpfulness"] + df["v2"] * df["verbosity"]

    # ä¿å­˜å¸¦æ€»åˆ†æ–‡ä»¶
    df.to_csv(output_file_with_total, index=False)
    print(f"âœ… Saved file with total_score: {output_file_with_total}")

    # æå–æ¯ä¸ª prompt_id çš„æœ€ä½³å“åº”
    df_best = df.loc[df.groupby("prompt_id")["total_score"].idxmax()].copy()
    df_best = df_best.rename(columns={"response": "best_response"})

    # ä¿å­˜æœ€ä½³å“åº”æ–‡ä»¶
    df_best.to_csv(output_best_file, index=False)
    print(f"ğŸ† Best responses per prompt saved to: {output_best_file}")

#v6 basleine best response
process_scored_csv_and_extract_best(
    input_file="drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch/batch_v6_fixed_scored.csv",
    output_dir="drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch/best_response",
    best_response_filename="best_response_v6_baseline.csv"
)

#åˆå¹¶v6 baselineå’Œfdiv
merge_baseline_and_fdiv_results(
    baseline_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch/best_response/best_response/best_response_v6_baseline.csv",
    fdiv_path="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v6_anglebased_top5/fdiv_anglebased_v6_all_best_response.csv",
    output_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/angle_output/v6_anglebased_final_merged_for_judge.csv"
)

#gpt4 judge

generate_pairwise_jsonl_for_gpt_judging(
    input_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/angle_output/v6_anglebased_final_merged_for_judge.csv",
    output_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/gpt4_judge_inputs_anglebased/v6_pairwise_randomized_angle.jsonl"
)

#v6æ‰“åˆ†
run_gpt_judging(
    input_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/gpt4_judge_inputs_anglebased/v6_pairwise_randomized_angle.jsonl",
    model="gpt-4o-mini"
)

analyze_gpt_judgment_results("drive/MyDrive/llm_pre_project/dpa_outputs_v9/gpt4_judge_inputs_anglebased/v6_pairwise_randomized_angle_results.jsonl")

#v7
run_dpa_generation(
    result_dir="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v7_anglebased_top5",
    valid_vs=[
        np.array([0.866025, 0.5     ]),  # 30Â°
        np.array([0.819152, 0.573576]),  # 35Â°
        np.array([0.906308, 0.422618]),  # 25Â°
        np.array([0.766044, 0.642788]),  # 40Â°
        np.array([0.939693, 0.342020])   # 20Â°
    ],
    valid_angles=[30, 35, 25, 40, 20],
    main_v=np.array([0.8660, 0.5000]),  # v7
    prompt_limit=2000  # å¯æŒ‰éœ€è°ƒæ•´
)

#v7ä¿®å¤
run_response_repair("/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v7_anglebased_top5")

#v7æ‰“åˆ†
run_reward_scoring(
    input_dir="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v7_anglebased_top5/fixed",
    output_dir="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v7_anglebased_top5/scored"
)

#v7 basleineæ‰“åˆ†
score_fixed_csv_file(
    input_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/fixed_batch/batch_v7_fixed.csv",
    output_dir="drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch"
)

#v7 basleine best response
process_scored_csv_and_extract_best(
    input_file="drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch/batch_v7_fixed_scored.csv",
    output_dir="drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch/best_response",
    best_response_filename="best_response_v7_baseline.csv"
)

#v7 fdivé€‰å‡ºbest response

merge_and_select_best_responses(
    input_dir="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v7_anglebased_top5/scored",
    output_file="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v7_anglebased_top5/fdiv_anglebased_v7_all_best_response.csv"
)

#åˆå¹¶v7 baselineå’Œfdiv
merge_baseline_and_fdiv_results(
    baseline_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch/best_response/best_response/best_response_v7_baseline.csv",
    fdiv_path="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v7_anglebased_top5/fdiv_anglebased_v7_all_best_response.csv",
    output_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/angle_output/v7_anglebased_final_merged_for_judge.csv"
)

import pandas as pd

# è¯»å–CSVæ–‡ä»¶
file_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/angle_output/v7_anglebased_final_merged_for_judge.csv"
df = pd.read_csv(file_path)

# æŸ¥çœ‹æ•°æ®è¡Œæ•°ï¼ˆå»æ‰headerè¡Œï¼‰
print("æ•°æ®æ€»è¡Œæ•°ï¼ˆå«è¡¨å¤´ï¼‰:", len(df))
print("æ•°æ®æ€»è¡Œæ•°ï¼ˆå»æ‰è¡¨å¤´ï¼‰:", df.shape[0])

#gpt4æ‰“åˆ†

#v7æ‰“åˆ†
generate_pairwise_jsonl_for_gpt_judging(
    input_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/angle_output/v7_anglebased_final_merged_for_judge.csv",
    output_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/gpt4_judge_inputs_anglebased/v7_pairwise_randomized_angle.jsonl"
)

#v7æ‰“åˆ†
run_gpt_judging(
    input_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/gpt4_judge_inputs_anglebased/v7_pairwise_randomized_angle.jsonl",
    model="gpt-4o-mini"
)

#v7
analyze_gpt_judgment_results("drive/MyDrive/llm_pre_project/dpa_outputs_v9/gpt4_judge_inputs_anglebased/v7_pairwise_randomized_angle_results.jsonl")



#v8
run_dpa_generation(
    result_dir="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v8_anglebased_top5",
    valid_vs=[
        np.array([0.819152, 0.573576]),  # 35Â°
        np.array([0.866025, 0.5     ]),  # 30Â°
        np.array([0.766044, 0.642788]),  # 40Â°
        np.array([0.906308, 0.422618]),  # 25Â°
        np.array([0.939693, 0.342020])   # 20Â°
    ],
    valid_angles=[35, 30, 40, 25, 20],
    main_v=np.array([0.8192, 0.5736]),  # v8
    prompt_limit=2000  # å¯æŒ‰éœ€è°ƒæ•´
)

#v8ä¿®å¤
run_response_repair("/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v8_anglebased_top5")

#v8æ‰“åˆ†
run_reward_scoring(
    input_dir="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v8_anglebased_top5/fixed",
    output_dir="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v8_anglebased_top5/scored"
)

#v9
run_dpa_generation(
    result_dir="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v9_anglebased_top5",
    valid_vs=[
        np.array([0.766044, 0.642788]),  # 40Â°
        np.array([0.819152, 0.573576]),  # 35Â°
        np.array([0.866025, 0.5     ]),  # 30Â°
        np.array([0.906308, 0.422618]),  # 25Â°
        np.array([0.939693, 0.342020])   # 20Â°
    ],
    valid_angles=[40, 35, 30, 25, 20],
    main_v=np.array([0.7660, 0.6428]),  # v9
    prompt_limit=2000  # å¯æŒ‰éœ€è°ƒæ•´
)

#v9ä¿®å¤
run_response_repair("/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v9_anglebased_top5")

#ä¿®å¤baseline

def run_single_file_repair(input_file_path, save_subdir="fixed_batch"):
    import os
    import pandas as pd
    import numpy as np
    import torch
    from tqdm import tqdm
    from transformers import AutoTokenizer, AutoModelForCausalLM
    from accelerate import Accelerator

    # === æ¨¡å‹åˆå§‹åŒ– ===
    device = "cuda" if torch.cuda.is_available() else "cpu"
    accelerator = Accelerator()

    model = AutoModelForCausalLM.from_pretrained(
        "Haoxiang-Wang/DPA-v1-Mistral-7B",
        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
        device_map="auto" if torch.cuda.is_available() else None
    )
    model = accelerator.prepare(model)

    tokenizer = AutoTokenizer.from_pretrained("Haoxiang-Wang/DPA-v1-Mistral-7B")
    tokenizer.padding_side = "left"
    if tokenizer.pad_token_id is None:
        tokenizer.pad_token = tokenizer.eos_token

    # === åˆ¤æ–­ response æ˜¯å¦éœ€è¦é‡ç”Ÿæˆ ===
    def is_invalid_response(resp):
        if pd.isna(resp): return True
        resp = str(resp).strip()
        if not resp: return True
        invalid_keywords = [
            "[A:]", "[/SOLUTION]", "[ASS]", "[A]:", "[/CONV]", "[/CODE]", "[/USER]",
            "[OUTPUT]", "[CC]", "[OUT]", "[/DATA]", "[/ASST]", "[/ME]", "[/REST]",
            "[RESP]", "<[user]>", "<|user|>", "[Response]", "[Assistant]", "[Answer]",
            "[End of Response]", "<human>", "<|endoftext|>", "<<", "[INST]", "[/INST]",
            "[]", "[USER]"
        ]
        if resp in invalid_keywords: return True
        tokens = resp.split()
        if all(token.strip() in invalid_keywords for token in tokens): return True
        tag_like_count = sum(1 for t in tokens if (t.startswith("[") and t.endswith("]")) or (t.startswith("<") and t.endswith(">")))
        return len(tokens) > 0 and tag_like_count / len(tokens) > 0.8

    # === Prompt æ„å»ºå‡½æ•° ===
    def build_input(prompt, v1, v2):
        h = int(np.round(v1 * 100))
        v = int(np.round(v2 * 100))
        sys_instruction = f"You are a helpful assistant. Your response should maximize weighted rating = helpfulness*{h} + verbosity*{v}."
        return [{"role": "system", "content": sys_instruction}, {"role": "user", "content": prompt}]

    # === é‡ç”Ÿæˆå‡½æ•° ===
    def regenerate_response(prompt, v1, v2):
        try:
            messages = build_input(prompt, v1, v2)
            input_ids = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors="pt").to(device)
            if input_ids.ndim != 2 or input_ids.shape[1] == 0:
                print(f"âš ï¸ Invalid input_ids shape {input_ids.shape}, skipping: {prompt[:60]}")
                return ""
            outputs = model.generate(
                input_ids=input_ids,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
            generated = outputs[0][input_ids.shape[1]:]
            return tokenizer.decode(generated, skip_special_tokens=True).strip()
        except Exception as e:
            print(f"âŒ Error: {e} \nPrompt: {prompt[:60]}")
            return ""

    # === ä¿®å¤é€»è¾‘ ===
    if not os.path.exists(input_file_path):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {input_file_path}")
        return

    df = pd.read_csv(input_file_path)
    bad_mask = df["response"].apply(is_invalid_response)
    num_bad = bad_mask.sum()

    if num_bad == 0:
        print("âœ… æ— éœ€ä¿®å¤ï¼Œè·³è¿‡ã€‚")
        return

    print(f"\nğŸ“‚ æ£€æŸ¥ï¼š{input_file_path}")
    print(f"ğŸ”§ æ£€æµ‹åˆ°æ— æ•ˆ response æ•°é‡ï¼š{num_bad}")
    for i in tqdm(bad_mask[bad_mask].index, total=num_bad, desc=os.path.basename(input_file_path), dynamic_ncols=True):
        row = df.loc[i]
        new_response = regenerate_response(row["prompt"], row["v1"], row["v2"])
        df.at[i, "response"] = new_response

    base_dir = os.path.dirname(input_file_path)
    save_dir = os.path.join(base_dir, save_subdir)
    os.makedirs(save_dir, exist_ok=True)
    file_name = os.path.basename(input_file_path).replace(".csv", "_fixed.csv")
    save_path = os.path.join(save_dir, file_name)
    df.to_csv(save_path, index=False)
    print(f"âœ… ä¿®å¤å®Œæˆ â†’ ä¿å­˜åˆ°: {save_path}")

#ä¿®å¤v9 basleine
run_single_file_repair("drive/MyDrive/llm_pre_project/dpa_outputs_v9/batch_v9.csv")

#v9 fdiv æ‰“åˆ†
run_reward_scoring(
    input_dir="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v9_anglebased_top5/fixed",
    output_dir="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v9_anglebased_top5/scored"
)

#v9 basleineæ‰“åˆ†
score_fixed_csv_file(
    input_path="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fixed_batch/batch_v9_fixed.csv",
    output_dir="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch"
)

#v9 fdivé€‰å‡ºbest response

merge_and_select_best_responses(
    input_dir="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v9_anglebased_top5/scored",
    output_file="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v9_anglebased_top5/fdiv_anglebased_v9_all_best_response.csv"
)

#v9 basleine best response
process_scored_csv_and_extract_best(
    input_file="drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch/batch_v9_fixed_scored.csv",
    output_dir="drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch/best_response",
    best_response_filename="best_response_v9_baseline.csv"
)

#åˆå¹¶v9 baselineå’Œfdiv
merge_baseline_and_fdiv_results(
    baseline_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch/best_response/best_response/best_response_v9_baseline.csv",
    fdiv_path="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v9_anglebased_top5/fdiv_anglebased_v9_all_best_response.csv",
    output_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/angle_output/v9_anglebased_final_merged_for_judge.csv"
)

#v9æ‰“åˆ†
generate_pairwise_jsonl_for_gpt_judging(
    input_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/angle_output/v9_anglebased_final_merged_for_judge.csv",
    output_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/gpt4_judge_inputs_anglebased/v9_pairwise_randomized_angle.jsonl"
)

#v9æ‰“åˆ†
run_gpt_judging(
    input_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/gpt4_judge_inputs_anglebased/v9_pairwise_randomized_angle.jsonl",
    model="gpt-4o-mini"
)

#v9
analyze_gpt_judgment_results("drive/MyDrive/llm_pre_project/dpa_outputs_v9/gpt4_judge_inputs_anglebased/v9_pairwise_randomized_angle_results.jsonl")





#v10
run_dpa_generation(
    result_dir="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v10_anglebased_top5",
    valid_vs=[
        np.array([0.766044, 0.642788]),  # 40Â°
        np.array([0.819152, 0.573576]),  # 35Â°
        np.array([0.866025, 0.5     ]),  # 30Â°
        np.array([0.906308, 0.422618]),  # 25Â°
        np.array([0.939693, 0.342020])   # 20Â°
    ],
    valid_angles=[40, 35, 30, 25, 20],
    main_v=np.array([0.7071, 0.7071]),  # v10
    prompt_limit=2000  # å¯æŒ‰éœ€è°ƒæ•´
)

#v10ä¿®å¤
run_response_repair("/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v10_anglebased_top5")

#v10æ‰“åˆ†
run_reward_scoring(
    input_dir="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v10_anglebased_top5/fixed",
    output_dir="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v10_anglebased_top5/scored"
)

#v10 fdivæ‰“åˆ†
run_reward_scoring(
    input_dir="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v10_anglebased_top5/fixed",
    output_dir="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v10_anglebased_top5/scored"
)

#v10 baselineè¡¥å……

from google.colab import drive
import os
import numpy as np
import pandas as pd
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM
from tqdm.auto import tqdm
import torch
import time
import random

# âœ… è®¾ç½® v_index ä¸º10ï¼Œå¯¹åº” v10 = (0.7071, 0.7071)
v_indices = [10]
v_mapping = {10: (0.7071, 0.7071)}

# âœ… æŒ‚è½½ Google Drive
drive.mount('/content/drive')

# âœ… ç¯å¢ƒè®¾ç½®
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
seed = 42
np.random.seed(seed)
random.seed(seed)
torch.manual_seed(seed)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(seed)

# âœ… è¾“å‡ºç›®å½•
result_dir = "/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9"
os.makedirs(result_dir, exist_ok=True)

# âœ… è½½å…¥æ•°æ®é›†
print("ğŸ“¦ Loading prompts from UltraFeedback...")
ds = load_dataset("HuggingFaceH4/ultrafeedback_binarized", split="test_prefs")
prompts = ds["prompt"][:2000]
all_prompt_ids = list(range(len(prompts)))

# âœ… åŠ è½½æ¨¡å‹
print("ğŸ¤– Loading DPA-v1 model...")
model = AutoModelForCausalLM.from_pretrained(
    "Haoxiang-Wang/DPA-v1-Mistral-7B",
    torch_dtype=torch.float16
).to(device)

tokenizer = AutoTokenizer.from_pretrained("Haoxiang-Wang/RewardModel-Mistral-7B-for-DPA-v1")
tokenizer.padding_side = "left"
if tokenizer.pad_token_id is None:
    tokenizer.pad_token = tokenizer.eos_token

# âœ… æ„é€  Prompt
def build_input(prompt, v1, v2):
    h = int(np.round(v1 * 100))
    v = int(np.round(v2 * 100))
    sys_instruction = f"You are a helpful assistant. Your response should maximize weighted rating = helpfulness*{h} + verbosity*{v}."
    return [{"role": "user", "content": f"{sys_instruction}\n\n{prompt}"}]

# âœ… æ‰¹é‡ç”Ÿæˆå‡½æ•°
def generate_response_batch(prompts_batch, prompt_ids_batch, v1, v2):
    input_ids_list = []
    for prompt in prompts_batch:
        messages = build_input(prompt, v1, v2)
        input_ids = tokenizer.apply_chat_template(
            messages, add_generation_prompt=True, return_tensors="pt"
        )[0]
        input_ids_list.append(input_ids.to(device))

    input_ids_padded = torch.nn.utils.rnn.pad_sequence(
        input_ids_list, batch_first=True, padding_value=tokenizer.pad_token_id
    ).to(device)

    attention_mask = (input_ids_padded != tokenizer.pad_token_id).to(device)
    max_input_len = input_ids_padded.shape[1]
    dynamic_max_new_tokens = min(2048, 4096 - max_input_len)

    with torch.no_grad():
        outputs = model.generate(
            input_ids=input_ids_padded,
            attention_mask=attention_mask,
            max_new_tokens=dynamic_max_new_tokens,
            temperature=0.7,
            do_sample=True,
            num_return_sequences=3,
            pad_token_id=tokenizer.eos_token_id
        )

    responses = []
    for i in range(len(prompts_batch)):
        prompt_responses = []
        for j in range(3):
            idx = i * 3 + j
            generated_tokens = outputs[idx][input_ids_padded.shape[1]:]
            decoded = tokenizer.decode(generated_tokens, skip_special_tokens=True)
            prompt_responses.append(decoded)
        responses.append({
            "prompt_id": prompt_ids_batch[i],
            "prompt": prompts_batch[i],
            "responses": prompt_responses
        })
    return responses

# âœ… ä¸»é€»è¾‘ï¼šæ–­ç‚¹ç»­è·‘ + å®Œæ•´å•æ¡ç¼ºè¡¥é€»è¾‘
batch_size = 8

for v_index in v_indices:
    v1, v2 = v_mapping[v_index]
    output_file = os.path.join(result_dir, f"batch_v{v_index}.csv")
    print(f"\nğŸ§­ Running v{v_index}: v = ({v1:.4f}, {v2:.4f})")

    if os.path.exists(output_file):
        existing_df = pd.read_csv(output_file)
        done_prompt_ids = set(existing_df["prompt_id"].unique())
        print(f"ğŸ” Loaded {len(done_prompt_ids)} prompts from previous run.")
    else:
        done_prompt_ids = set()

    remaining_prompt_ids = [pid for pid in all_prompt_ids if pid not in done_prompt_ids]
    print(f"ğŸ” Remaining prompts to process: {len(remaining_prompt_ids)}")

    start_time = time.time()

    for i in tqdm(range(0, len(remaining_prompt_ids), batch_size), desc=f"Generating v{v_index}"):
        batch_prompt_ids = remaining_prompt_ids[i:i+batch_size]
        prompt_batch = [prompts[pid] for pid in batch_prompt_ids]

        try:
            batch_results = generate_response_batch(prompt_batch, batch_prompt_ids, v1, v2)
            new_rows = []
            for item in batch_results:
                for k, resp in enumerate(item["responses"]):
                    row = {
                        "prompt_id": item["prompt_id"],
                        "v1": v1,
                        "v2": v2,
                        "prompt": item["prompt"],
                        "response_id": k + 1,
                        "response": resp
                    }
                    new_rows.append(row)
            pd.DataFrame(new_rows).to_csv(output_file, mode='a', header=not os.path.exists(output_file), index=False)
        except Exception as e:
            print(f"âš ï¸ Error at batch {i}-{i+batch_size}: {e}")

    print(f"âœ… Completed v{v_index}. Total time: {time.time() - start_time:.1f}s")

#ä¿®å¤v10 basleine
run_single_file_repair("drive/MyDrive/llm_pre_project/dpa_outputs_v9/batch_v10.csv")

#v10 basleineæ‰“åˆ†
score_fixed_csv_file(
    input_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/fixed_batch/batch_v10_fixed.csv",
    output_dir="drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch"
)

#v10 fdivé€‰å‡ºbest response

merge_and_select_best_responses(
    input_dir="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v10_anglebased_top5/scored",
    output_file="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v10_anglebased_top5/fdiv_anglebased_v10_all_best_response.csv"
)

#v10 basleine best response
process_scored_csv_and_extract_best(
    input_file="drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch/batch_v10_fixed_scored.csv",
    output_dir="drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch/best_response",
    best_response_filename="best_response_v10_baseline.csv"
)

#åˆå¹¶v10 baselineå’Œfdiv
merge_baseline_and_fdiv_results(
    baseline_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch/best_response/best_response/best_response_v10_baseline.csv",
    fdiv_path="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v10_anglebased_top5/fdiv_anglebased_v10_all_best_response.csv",
    output_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/angle_output/v10_anglebased_final_merged_for_judge.csv"
)

generate_pairwise_jsonl_for_gpt_judging(
    input_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/angle_output/v10_anglebased_final_merged_for_judge.csv",
    output_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/gpt4_judge_inputs_anglebased/v10_pairwise_randomized_angle.jsonl"
)

#v10æ‰“åˆ†
run_gpt_judging(
    input_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/gpt4_judge_inputs_anglebased/v10_pairwise_randomized_angle.jsonl",
    model="gpt-4o-mini"
)

#v10
analyze_gpt_judgment_results("drive/MyDrive/llm_pre_project/dpa_outputs_v9/gpt4_judge_inputs_anglebased/v10_pairwise_randomized_angle_results.jsonl")

#è¡¥è·‘v8

!pip install -U datasets

from google.colab import drive
import os
import numpy as np
import pandas as pd
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM
from tqdm.auto import tqdm
import torch
import time
import random

# âœ… è®¾ç½® v_index ä¸º8ï¼Œå¯¹åº” v8 = (0.8192, 0.5736)
v_indices = [8]
v_mapping = {8: (0.8192, 0.5736)}

# âœ… æŒ‚è½½ Google Drive
drive.mount('/content/drive')

# âœ… ç¯å¢ƒè®¾ç½®
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
seed = 42
np.random.seed(seed)
random.seed(seed)
torch.manual_seed(seed)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(seed)

# âœ… è¾“å‡ºç›®å½•
result_dir = "/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9"
os.makedirs(result_dir, exist_ok=True)

# âœ… è½½å…¥æ•°æ®é›†
print("ğŸ“¦ Loading prompts from UltraFeedback...")
ds = load_dataset("HuggingFaceH4/ultrafeedback_binarized", split="test_prefs")
prompts = ds["prompt"][:2000]
all_prompt_ids = list(range(len(prompts)))

# âœ… åŠ è½½æ¨¡å‹
print("ğŸ¤– Loading DPA-v1 model...")
model = AutoModelForCausalLM.from_pretrained(
    "Haoxiang-Wang/DPA-v1-Mistral-7B",
    torch_dtype=torch.float16
).to(device)

tokenizer = AutoTokenizer.from_pretrained("Haoxiang-Wang/RewardModel-Mistral-7B-for-DPA-v1")
tokenizer.padding_side = "left"
if tokenizer.pad_token_id is None:
    tokenizer.pad_token = tokenizer.eos_token

# âœ… æ„é€  Prompt
def build_input(prompt, v1, v2):
    h = int(np.round(v1 * 100))
    v = int(np.round(v2 * 100))
    sys_instruction = f"You are a helpful assistant. Your response should maximize weighted rating = helpfulness*{h} + verbosity*{v}."
    return [{"role": "user", "content": f"{sys_instruction}\n\n{prompt}"}]

# âœ… æ‰¹é‡ç”Ÿæˆå‡½æ•°
def generate_response_batch(prompts_batch, prompt_ids_batch, v1, v2):
    input_ids_list = []
    for prompt in prompts_batch:
        messages = build_input(prompt, v1, v2)
        input_ids = tokenizer.apply_chat_template(
            messages, add_generation_prompt=True, return_tensors="pt"
        )[0]
        input_ids_list.append(input_ids.to(device))

    input_ids_padded = torch.nn.utils.rnn.pad_sequence(
        input_ids_list, batch_first=True, padding_value=tokenizer.pad_token_id
    ).to(device)

    attention_mask = (input_ids_padded != tokenizer.pad_token_id).to(device)
    max_input_len = input_ids_padded.shape[1]
    dynamic_max_new_tokens = min(2048, 4096 - max_input_len)

    with torch.no_grad():
        outputs = model.generate(
            input_ids=input_ids_padded,
            attention_mask=attention_mask,
            max_new_tokens=dynamic_max_new_tokens,
            temperature=0.7,
            do_sample=True,
            num_return_sequences=3,
            pad_token_id=tokenizer.eos_token_id
        )

    responses = []
    for i in range(len(prompts_batch)):
        prompt_responses = []
        for j in range(3):
            idx = i * 3 + j
            generated_tokens = outputs[idx][input_ids_padded.shape[1]:]
            decoded = tokenizer.decode(generated_tokens, skip_special_tokens=True)
            prompt_responses.append(decoded)
        responses.append({
            "prompt_id": prompt_ids_batch[i],
            "prompt": prompts_batch[i],
            "responses": prompt_responses
        })
    return responses

# âœ… ä¸»é€»è¾‘ï¼šæ–­ç‚¹ç»­è·‘ + å®Œæ•´å•æ¡ç¼ºè¡¥é€»è¾‘
batch_size = 8

for v_index in v_indices:
    v1, v2 = v_mapping[v_index]
    output_file = os.path.join(result_dir, f"batch_v{v_index}.csv")
    print(f"\nğŸ§­ Running v{v_index}: v = ({v1:.4f}, {v2:.4f})")

    if os.path.exists(output_file):
        existing_df = pd.read_csv(output_file)
        done_prompt_ids = set(existing_df["prompt_id"].unique())
        print(f"ğŸ” Loaded {len(done_prompt_ids)} prompts from previous run.")
    else:
        done_prompt_ids = set()

    remaining_prompt_ids = [pid for pid in all_prompt_ids if pid not in done_prompt_ids]
    print(f"ğŸ” Remaining prompts to process: {len(remaining_prompt_ids)}")

    start_time = time.time()

    for i in tqdm(range(0, len(remaining_prompt_ids), batch_size), desc=f"Generating v{v_index}"):
        batch_prompt_ids = remaining_prompt_ids[i:i+batch_size]
        prompt_batch = [prompts[pid] for pid in batch_prompt_ids]

        try:
            batch_results = generate_response_batch(prompt_batch, batch_prompt_ids, v1, v2)
            new_rows = []
            for item in batch_results:
                for k, resp in enumerate(item["responses"]):
                    row = {
                        "prompt_id": item["prompt_id"],
                        "v1": v1,
                        "v2": v2,
                        "prompt": item["prompt"],
                        "response_id": k + 1,
                        "response": resp
                    }
                    new_rows.append(row)
            pd.DataFrame(new_rows).to_csv(output_file, mode='a', header=not os.path.exists(output_file), index=False)
        except Exception as e:
            print(f"âš ï¸ Error at batch {i}-{i+batch_size}: {e}")

    print(f"âœ… Completed v{v_index}. Total time: {time.time() - start_time:.1f}s")

#ä¿®å¤v8 basleine
run_single_file_repair("drive/MyDrive/llm_pre_project/dpa_outputs_v9/batch_v8.csv")

import pandas as pd

# è¯»å–CSVæ–‡ä»¶
file_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/angle_output/v8_anglebased_final_merged_for_judge.csv"
df = pd.read_csv(file_path)

# æŸ¥çœ‹æ•°æ®è¡Œæ•°ï¼ˆå»æ‰headerè¡Œï¼‰
print("æ•°æ®æ€»è¡Œæ•°ï¼ˆå«è¡¨å¤´ï¼‰:", len(df))
print("æ•°æ®æ€»è¡Œæ•°ï¼ˆå»æ‰è¡¨å¤´ï¼‰:", df.shape[0])

#v8 basleineæ‰“åˆ†
score_fixed_csv_file(
    input_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/fixed_batch/batch_v8_fixed.csv",
    output_dir="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch"
)

#v8 basleine best response
process_scored_csv_and_extract_best(
    input_file="drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch/batch_v8_fixed_scored.csv",
    output_dir="drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch/best_response",
    best_response_filename="best_response_v8_baseline.csv"
)

#v8 fdivé€‰å‡ºbest response

merge_and_select_best_responses(
    input_dir="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v8_anglebased_top5/scored",
    output_file="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v8_anglebased_top5/fdiv_anglebased_v8_all_best_response.csv"
)


#åˆå¹¶v8 baselineå’Œfdiv
merge_baseline_and_fdiv_results(
    baseline_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch/best_response/best_response/best_response_v8_baseline.csv",
    fdiv_path="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v8_anglebased_top5/fdiv_anglebased_v8_all_best_response.csv",
    output_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/angle_output/v8_anglebased_final_merged_for_judge.csv"
)

generate_pairwise_jsonl_for_gpt_judging(
    input_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/angle_output/v8_anglebased_final_merged_for_judge.csv",
    output_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/gpt4_judge_inputs_anglebased/v8_pairwise_randomized_angle.jsonl"
)

#v8æ‰“åˆ†
run_gpt_judging(
    input_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/gpt4_judge_inputs_anglebased/v8_pairwise_randomized_angle.jsonl",
    model="gpt-4o-mini"
)

#v8ç»Ÿè®¡
analyze_gpt_judgment_results("drive/MyDrive/llm_pre_project/dpa_outputs_v9/gpt4_judge_inputs_anglebased/v8_pairwise_randomized_angle_results.jsonl")

# v4
run_dpa_generation(
    result_dir="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v4_anglebased_top5",
    valid_vs=[
        np.array([0.965926, 0.258819]),  # 15Â°
        np.array([0.984808, 0.173648]),  # 10Â°
        np.array([0.939693, 0.342020]),  # 20Â°
        np.array([0.996195, 0.087156]),  # 5Â°
        np.array([0.906308, 0.422618])   # 25Â°
    ],
    valid_angles=[15, 10, 20, 5, 25],
    main_v=np.array([0.965926, 0.258819]),  # v4 ä¸»æ–¹å‘ä¸º 15Â°
    prompt_limit=2000  # å¯æŒ‰éœ€è°ƒæ•´
)

!pip install -U datasets

#v4ä¿®å¤
run_response_repair("/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v4_anglebased_top5")

#v4 fdivæ‰“åˆ†
run_reward_scoring(
    input_dir="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v4_anglebased_top5/fixed",
    output_dir="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v4_anglebased_top5/scored"
)

import pandas as pd

# è¯»å–CSVæ–‡ä»¶
file_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/angle_output/v4_anglebased_final_merged_for_judge.csv"
df = pd.read_csv(file_path)

# æŸ¥çœ‹æ•°æ®è¡Œæ•°ï¼ˆå»æ‰headerè¡Œï¼‰
print("æ•°æ®æ€»è¡Œæ•°ï¼ˆå«è¡¨å¤´ï¼‰:", len(df))
print("æ•°æ®æ€»è¡Œæ•°ï¼ˆå»æ‰è¡¨å¤´ï¼‰:", df.shape[0])

#ä¿®å¤v4 basleine
run_single_file_repair("drive/MyDrive/llm_pre_project/dpa_outputs_v9/batch_v4.csv")

#v4 basleineæ‰“åˆ†
score_fixed_csv_file(
    input_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/fixed_batch/batch_v4_fixed.csv",
    output_dir="drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch"
)

#v4 basleine best response
process_scored_csv_and_extract_best(
    input_file="drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch/batch_v4_fixed_scored.csv",
    output_dir="drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch/best_response",
    best_response_filename="best_response_v4_baseline.csv"
)

#v4 fdivé€‰å‡ºbest response

merge_and_select_best_responses(
    input_dir="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v4_anglebased_top5/scored",
    output_file="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v4_anglebased_top5/fdiv_anglebased_v4_all_best_response.csv"
)


#åˆå¹¶v4 baselineå’Œfdiv
merge_baseline_and_fdiv_results(
    baseline_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch/best_response/best_response/best_response_v4_baseline.csv",
    fdiv_path="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v4_anglebased_top5/fdiv_anglebased_v4_all_best_response.csv",
    output_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/angle_output/v4_anglebased_final_merged_for_judge.csv"
)

generate_pairwise_jsonl_for_gpt_judging(
    input_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/angle_output/v4_anglebased_final_merged_for_judge.csv",
    output_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/gpt4_judge_inputs_anglebased/v4_pairwise_randomized_angle.jsonl"
)

#v4æ‰“åˆ†
run_gpt_judging(
    input_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/gpt4_judge_inputs_anglebased/v4_pairwise_randomized_angle.jsonl",
    model="gpt-4o-mini"
)

#v4ç»Ÿè®¡
analyze_gpt_judgment_results("drive/MyDrive/llm_pre_project/dpa_outputs_v9/gpt4_judge_inputs_anglebased/v4_pairwise_randomized_angle_results.jsonl")

# v3
run_dpa_generation(
    result_dir="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v3_anglebased_top5",
    valid_vs=[
        np.array([0.984808, 0.173648]),  # 10Â°
        np.array([0.996195, 0.087156]),  # 5Â°
        np.array([0.965926, 0.258819]),  # 15Â°
        np.array([1.000000, 0.000000]),  # 0Â°
        np.array([0.939693, 0.342020])   # 20Â°
    ],
    valid_angles=[10, 5, 15, 0, 20],
    main_v=np.array([0.984808, 0.173648]),  # v3 ä¸»æ–¹å‘ä¸º 10Â°
    prompt_limit=2000  # å¯æŒ‰éœ€è°ƒæ•´
)

#v3ä¿®å¤
run_response_repair("/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v3_anglebased_top5")

#v3 fdivæ‰“åˆ†
run_reward_scoring(
    input_dir="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v3_anglebased_top5/fixed",
    output_dir="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v3_anglebased_top5/scored"
)

#v3 basleineæ‰“åˆ†
score_fixed_csv_file(
    input_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/fixed_batch/batch_v3_fixed.csv",
    output_dir="drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch"
)

#v3 basleine best response
process_scored_csv_and_extract_best(
    input_file="drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch/batch_v3_fixed_scored.csv",
    output_dir="drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch/best_response",
    best_response_filename="best_response_v3_baseline.csv"
)

#3 fdivé€‰å‡ºbest response

merge_and_select_best_responses(
    input_dir="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v3_anglebased_top5/scored",
    output_file="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v3_anglebased_top5/fdiv_anglebased_v3_all_best_response.csv"
)


#åˆå¹¶v3 baselineå’Œfdiv
merge_baseline_and_fdiv_results(
    baseline_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch/best_response/best_response/best_response_v3_baseline.csv",
    fdiv_path="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v3_anglebased_top5/fdiv_anglebased_v3_all_best_response.csv",
    output_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/angle_output/v3_anglebased_final_merged_for_judge.csv"
)

generate_pairwise_jsonl_for_gpt_judging(
    input_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/angle_output/v3_anglebased_final_merged_for_judge.csv",
    output_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/gpt4_judge_inputs_anglebased/v3_pairwise_randomized_angle.jsonl"
)

#v3æ‰“åˆ†
run_gpt_judging(
    input_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/gpt4_judge_inputs_anglebased/v3_pairwise_randomized_angle.jsonl",
    model="gpt-4o-mini"
)

#v3ç»Ÿè®¡
analyze_gpt_judgment_results("drive/MyDrive/llm_pre_project/dpa_outputs_v9/gpt4_judge_inputs_anglebased/v3_pairwise_randomized_angle_results.jsonl")









