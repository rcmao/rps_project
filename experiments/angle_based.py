# -*- coding: utf-8 -*-
"""angle_based.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PsBiAui4lvvGc8YJvieVk6XCqjdopeF6
"""

vectors = {
    "v1":  np.array([1.0000, 0.0000]),
    "v2":  np.array([0.9962, 0.0872]),
    "v3":  np.array([0.9848, 0.1736]),
    "v4":  np.array([0.9659, 0.2588]),
    "v5":  np.array([0.9397, 0.3420]),
    "v6":  np.array([0.9063, 0.4226]),
    "v7":  np.array([0.8660, 0.5000]),
    "v8":  np.array([0.8192, 0.5736]),
    "v9":  np.array([0.7660, 0.6428]),
    "v10": np.array([0.7071, 0.7071])
}

!pip install -U datasets

# 🚀 Step 0: 防止 Colab 空闲断线
from IPython.display import Javascript
Javascript('''
function ClickConnect(){
  console.log("🔄 保持连接中 - ClickConnect 被调用了");
  document.querySelector("colab-connect-button").click();
}
setInterval(ClickConnect, 60000);
''')

from google.colab import drive
drive.mount('/content/drive')

# ✅ Step 2: 安装依赖
!pip install -q transformers datasets accelerate

import numpy as np

def get_top_k_angle_perturbations(v_main,
                                   angle_range=(-40, 40),
                                   step=5,
                                   theta_max=30,
                                   top_k=5,
                                   verbose=True):
    """
    返回与主方向夹角最小的前 top_k 个合法扰动方向（Angle-Based Perturbation）

    Args:
        v_main (np.ndarray): 主方向单位向量，例如 np.array([0.7071, 0.7071])
        angle_range (tuple): 扰动角度范围 (min_angle, max_angle)，单位为度
        step (int): 每隔多少度采样一个方向
        theta_max (float): 最大允许的夹角（单位为度）
        top_k (int): 保留夹角最小的前 k 个方向
        verbose (bool): 是否打印筛选结果

    Returns:
        valid_vs (List[np.ndarray]): 有效扰动方向向量
        valid_angles (List[float]): 有效扰动角度（相对角度）
    """

    def angle_between(v1, v2):
        cos_angle = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))
        return np.degrees(np.arccos(np.clip(cos_angle, -1.0, 1.0)))

    # 构造单位扰动向量
    angle_offsets = np.arange(angle_range[0], angle_range[1] + 1, step)
    perturbed_vs = []
    perturbed_angles = []
    angle_diffs = []

    for offset in angle_offsets:
        angle_rad = np.radians(offset)
        v = np.array([np.cos(angle_rad), np.sin(angle_rad)])
        angle_diff = angle_between(v, v_main)
        if angle_diff <= theta_max:
            perturbed_vs.append(v)
            perturbed_angles.append(offset)
            angle_diffs.append(angle_diff)

    # 根据夹角排序并选出 top_k 个
    sorted_indices = np.argsort(angle_diffs)
    top_indices = sorted_indices[:top_k]
    valid_vs = [perturbed_vs[i] for i in top_indices]
    valid_angles = [perturbed_angles[i] for i in top_indices]

    if verbose:
        print("✅ 最小夹角 Top-K 扰动方向:")
        for v, a in zip(valid_vs, valid_angles):
            print(f"→ angle = {a}°, v = {np.round(v, 6)}")

    return valid_vs, valid_angles

#v4
valid_vs, valid_angles = get_top_k_angle_perturbations(
    v_main=vectors["v4"],   # 使用你定义的主方向 v5 = (0.9397, 0.3420)
    angle_range=(-40, 40),  # 在主方向 ±40° 范围内构造扰动
    step=5,                 # 每 5° 采样一次
    theta_max=30,           # 最大允许的夹角为 30°
    top_k=5,                # 选择夹角最小的前 5 个方向
    verbose=True            # 打印有效向量和角度
)

#v5
valid_vs, valid_angles = get_top_k_angle_perturbations(
    v_main=vectors["v5"],   # 使用你定义的主方向 v5 = (0.9397, 0.3420)
    angle_range=(-40, 40),  # 在主方向 ±40° 范围内构造扰动
    step=5,                 # 每 5° 采样一次
    theta_max=30,           # 最大允许的夹角为 30°
    top_k=5,                # 选择夹角最小的前 5 个方向
    verbose=True            # 打印有效向量和角度
)

#v6
valid_vs, valid_angles = get_top_k_angle_perturbations(
    v_main=vectors["v6"],   # 使用你定义的主方向 v5 = (0.9397, 0.3420)
    angle_range=(-40, 40),  # 在主方向 ±40° 范围内构造扰动
    step=5,                 # 每 5° 采样一次
    theta_max=30,           # 最大允许的夹角为 30°
    top_k=5,                # 选择夹角最小的前 5 个方向
    verbose=True            # 打印有效向量和角度
)

#v7
valid_vs, valid_angles = get_top_k_angle_perturbations(
    v_main=vectors["v7"],   # 使用你定义的主方向 v5 = (0.9397, 0.3420)
    angle_range=(-40, 40),  # 在主方向 ±40° 范围内构造扰动
    step=5,                 # 每 5° 采样一次
    theta_max=30,           # 最大允许的夹角为 30°
    top_k=5,                # 选择夹角最小的前 5 个方向
    verbose=True            # 打印有效向量和角度
)

#v8
valid_vs, valid_angles = get_top_k_angle_perturbations(
    v_main=vectors["v8"],   # 使用你定义的主方向 v5 = (0.9397, 0.3420)
    angle_range=(-40, 40),  # 在主方向 ±40° 范围内构造扰动
    step=5,                 # 每 5° 采样一次
    theta_max=30,           # 最大允许的夹角为 30°
    top_k=5,                # 选择夹角最小的前 5 个方向
    verbose=True            # 打印有效向量和角度
)

#v9
valid_vs, valid_angles = get_top_k_angle_perturbations(
    v_main=vectors["v9"],   # 使用你定义的主方向 v5 = (0.9397, 0.3420)
    angle_range=(-40, 40),  # 在主方向 ±40° 范围内构造扰动
    step=5,                 # 每 5° 采样一次
    theta_max=30,           # 最大允许的夹角为 30°
    top_k=5,                # 选择夹角最小的前 5 个方向
    verbose=True            # 打印有效向量和角度
)

#v10
valid_vs, valid_angles = get_top_k_angle_perturbations(
    v_main=vectors["v10"],   # 使用你定义的主方向 v5 = (0.9397, 0.3420)
    angle_range=(-40, 40),  # 在主方向 ±40° 范围内构造扰动
    step=5,                 # 每 5° 采样一次
    theta_max=30,           # 最大允许的夹角为 30°
    top_k=5,                # 选择夹角最小的前 5 个方向
    verbose=True            # 打印有效向量和角度
)

#先测试下v5

def run_dpa_generation(
    result_dir,
    valid_vs,
    valid_angles,
    main_v,
    prompt_limit=500,
    batch_size=8,
    model_name="Haoxiang-Wang/DPA-v1-Mistral-7B",
    tokenizer_name="Haoxiang-Wang/RewardModel-Mistral-7B-for-DPA-v1"
):
    from datasets import load_dataset
    from transformers import AutoTokenizer, AutoModelForCausalLM
    from tqdm.auto import tqdm
    import torch
    import os
    import random
    import numpy as np
    import pandas as pd

    # === 设置设备与种子 ===
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    seed = 42
    np.random.seed(seed)
    random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

    os.makedirs(result_dir, exist_ok=True)

    # === 加载数据集 ===
    ds = load_dataset("HuggingFaceH4/ultrafeedback_binarized", split="test_prefs")
    prompts = ds["prompt"][:prompt_limit]
    prompt_ids = list(range(prompt_limit))

    # === 加载模型与 tokenizer ===
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
        device_map="auto"
    ).to(device)

    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
    tokenizer.padding_side = "left"
    if tokenizer.pad_token_id is None:
        tokenizer.pad_token = tokenizer.eos_token

    def build_input(prompt, v1, v2):
        h = int(np.round(v1 * 100))
        v = int(np.round(v2 * 100))
        sys_instruction = f"You are a helpful assistant. Your response should maximize weighted rating = helpfulness*{h} + verbosity*{v}."
        return [{"role": "user", "content": f"{sys_instruction}\n\n{prompt}"}]

    def generate_response_batch(prompts_batch, prompt_ids_batch, v1, v2):
        input_ids_list = []
        for prompt in prompts_batch:
            messages = build_input(prompt, v1, v2)
            input_ids = tokenizer.apply_chat_template(
                messages, add_generation_prompt=True, return_tensors="pt"
            )[0]
            input_ids_list.append(input_ids.to(device))

        input_ids_padded = torch.nn.utils.rnn.pad_sequence(
            input_ids_list, batch_first=True, padding_value=tokenizer.pad_token_id
        ).to(device)

        attention_mask = (input_ids_padded != tokenizer.pad_token_id).to(device)
        max_input_len = input_ids_padded.shape[1]
        max_new_tokens = min(2048, 4096 - max_input_len)

        with torch.no_grad():
            outputs = model.generate(
                input_ids=input_ids_padded,
                attention_mask=attention_mask,
                max_new_tokens=max_new_tokens,
                temperature=0.7,
                do_sample=True,
                num_return_sequences=1,
                pad_token_id=tokenizer.eos_token_id
            )

        responses = []
        for i, input_ids in enumerate(input_ids_list):
            generated_tokens = outputs[i][input_ids.shape[0]:]
            decoded = tokenizer.decode(generated_tokens, skip_special_tokens=True)
            responses.append({
                "prompt_id": prompt_ids_batch[i],
                "prompt": prompts_batch[i],
                "response": decoded
            })
        return responses

    # === 主循环 ===
    for i, (v_vec, angle_deg) in enumerate(zip(valid_vs, valid_angles)):
        v1, v2 = v_vec[0], v_vec[1]
        output_file = os.path.join(result_dir, f"fdiv_v1_angle{int(angle_deg)}.csv")
        print(f"\n🚀 Generating for direction {i}: angle ≈ {angle_deg}°, v = ({v1:.4f}, {v2:.4f})")

        if os.path.exists(output_file):
            existing_df = pd.read_csv(output_file)
            done_prompt_ids = set(existing_df["prompt_id"].unique())
            results = existing_df.to_dict("records")
            print(f"🔁 Resuming from previous run: {len(done_prompt_ids)} prompts already completed.")
        else:
            done_prompt_ids = set()
            results = []

        for start in tqdm(range(0, len(prompts), batch_size), desc=f"Generating angle {angle_deg}"):
            end = min(start + batch_size, len(prompts))
            batch_prompts_all = prompts[start:end]
            batch_ids_all = prompt_ids[start:end]

            unprocessed_indices = [j for j, pid in enumerate(batch_ids_all) if pid not in done_prompt_ids]
            if not unprocessed_indices:
                continue

            batch_prompts = [batch_prompts_all[j] for j in unprocessed_indices]
            batch_ids = [batch_ids_all[j] for j in unprocessed_indices]

            try:
                batch_outputs = generate_response_batch(batch_prompts, batch_ids, v1, v2)
                for item in batch_outputs:
                    item.update({
                        "v1_p": round(v1, 4),
                        "v2_p": round(v2, 4),
                        "direction_index": i,
                        "valid_angle": round(angle_deg, 1),
                        "main_v1": round(main_v[0], 4),
                        "main_v2": round(main_v[1], 4)
                    })
                    results.append(item)

                pd.DataFrame(batch_outputs).to_csv(
                    output_file, mode='a', index=False,
                    header=not os.path.exists(output_file)
                )

            except Exception as e:
                print(f"⚠️ Error at batch {start}-{end}: {e}")

        print(f"✅ Final saved {len(results)} responses to {output_file}")

#v5

run_dpa_generation(
    result_dir="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v5_anglebased_top5",
    valid_vs=[
        np.array([0.939693, 0.342020]),  # 20°
        np.array([0.965926, 0.258819]),  # 15°
        np.array([0.906308, 0.422618]),  # 25°
        np.array([0.984808, 0.173648]),  # 10°
        np.array([0.866025, 0.5     ])   # 30°
    ],
    valid_angles=[20, 15, 25, 10, 30],
    main_v=np.array([0.9397, 0.3420]),  # v5
    prompt_limit=2000
)

#修复

import os
import pandas as pd
import numpy as np
import torch
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForCausalLM
from accelerate import Accelerator

def run_response_repair(input_dir, output_subdir="fixed"):
    """
    修复指定目录下所有 CSV 文件中无效的 LLM response，并保存到子目录中。

    Args:
        input_dir (str): 包含待修复 CSV 文件的目录路径。
        output_subdir (str): 修复后文件保存的子目录名称，默认 "fixed"。
    """

    # 初始化模型与 tokenizer
    device = "cuda" if torch.cuda.is_available() else "cpu"
    accelerator = Accelerator()

    model = AutoModelForCausalLM.from_pretrained(
        "Haoxiang-Wang/DPA-v1-Mistral-7B",
        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
        device_map="auto" if torch.cuda.is_available() else None
    )
    model = accelerator.prepare(model)

    tokenizer = AutoTokenizer.from_pretrained("Haoxiang-Wang/DPA-v1-Mistral-7B")
    tokenizer.padding_side = "left"
    if tokenizer.pad_token_id is None:
        tokenizer.pad_token = tokenizer.eos_token

    def is_invalid_response(resp):
        if pd.isna(resp):
            return True
        resp = str(resp).strip()
        if not resp:
            return True
        invalid_keywords = [
            "[A:]", "[/SOLUTION]", "[ASS]", "[A]:", "[/CONV]", "[/CODE]", "[/USER]",
            "[OUTPUT]", "[CC]", "[OUT]", "[/DATA]", "[/ASST]", "[/ME]", "[/REST]",
            "[RESP]", "<[user]>", "<|user|>", "[Response]", "[Assistant]", "[Answer]",
            "[End of Response]", "<human>", "<|endoftext|>", "<<", "[INST]", "[/INST]",
            "[]", "[USER]"
        ]
        tokens = resp.split()
        if all(token.strip() in invalid_keywords for token in tokens):
            return True
        tag_like_count = sum(1 for t in tokens if (t.startswith("[") and t.endswith("]")) or (t.startswith("<") and t.endswith(">")))
        return len(tokens) > 0 and tag_like_count / len(tokens) > 0.8

    def build_input(prompt, v1, v2):
        h = int(np.round(v1 * 100))
        v = int(np.round(v2 * 100))
        sys_instruction = f"You are a helpful assistant. Your response should maximize weighted rating = helpfulness*{h} + verbosity*{v}."
        return [{"role": "system", "content": sys_instruction}, {"role": "user", "content": prompt}]

    def regenerate_response(prompt, v1, v2):
        try:
            messages = build_input(prompt, v1, v2)
            input_ids = tokenizer.apply_chat_template(
                messages, add_generation_prompt=True, return_tensors="pt"
            ).to(device)

            if input_ids.ndim != 2 or input_ids.shape[1] == 0:
                print(f"⚠️ Invalid input_ids shape {input_ids.shape}, skipping: {prompt[:60]}")
                return ""

            outputs = model.generate(
                input_ids=input_ids,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
            generated = outputs[0][input_ids.shape[1]:]
            return tokenizer.decode(generated, skip_special_tokens=True).strip()

        except Exception as e:
            print(f"❌ Error: {e} \nPrompt: {prompt[:60]}")
            return ""

    # 创建保存目录
    output_dir = os.path.join(input_dir, output_subdir)
    os.makedirs(output_dir, exist_ok=True)

    # 遍历并修复所有未修复文件
    for file in os.listdir(input_dir):
        if file.endswith(".csv") and not file.endswith("_fixed.csv"):
            file_path = os.path.join(input_dir, file)
            print(f"\n📂 正在检查: {file_path}")

            df = pd.read_csv(file_path)
            if "v1_p" not in df.columns or "v2_p" not in df.columns:
                print(f"❌ 缺少 v1_p/v2_p 列，跳过: {file}")
                continue

            bad_mask = df["response"].apply(is_invalid_response)
            num_bad = bad_mask.sum()

            if num_bad == 0:
                print("✅ 无需修复，跳过。")
                continue

            print(f"🔧 发现 {num_bad} 条无效 response，开始重生成...")
            for i in tqdm(bad_mask[bad_mask].index, total=num_bad, desc=file, dynamic_ncols=True):
                row = df.loc[i]
                new_resp = regenerate_response(row["prompt"], row["v1_p"], row["v2_p"])
                df.at[i, "response"] = new_resp

            save_name = os.path.basename(file).replace(".csv", "_fixed.csv")
            save_path = os.path.join(output_dir, save_name)
            df.to_csv(save_path, index=False)
            print(f"✅ 修复完成 → 已保存: {save_path}")

#v5
run_response_repair("/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v5_anglebased_top5")



#打分

import os
import pandas as pd
import numpy as np
import torch
from tqdm.auto import tqdm
from transformers import AutoTokenizer, AutoModelForSequenceClassification

def run_reward_scoring(input_dir, output_dir):
    """
    对 input_dir 中所有 `_fixed.csv` 文件进行打分（helpfulness, verbosity），支持断点续跑，只打缺的，保存至 output_dir。

    Args:
        input_dir (str): 包含待打分文件的路径
        output_dir (str): 打分结果保存路径
    """
    os.makedirs(output_dir, exist_ok=True)

    print("🤖 Loading Reward Model...")
    device = "cuda" if torch.cuda.is_available() else "cpu"
    rm = AutoModelForSequenceClassification.from_pretrained(
        "Haoxiang-Wang/RewardModel-Mistral-7B-for-DPA-v1", trust_remote_code=True
    ).to(device)
    tokenizer = AutoTokenizer.from_pretrained("Haoxiang-Wang/RewardModel-Mistral-7B-for-DPA-v1")

    def score_response(prompt, response):
        template = "[INST] You must read the following conversation carefully and rate the assistant's response from score 0-100 in these aspects: helpfulness, correctness, coherence, honesty, complexity, verbosity\n\nUser: {prompt}\n\nAssistant: {response} [/INST]"
        inputs = tokenizer(template.format(prompt=prompt, response=response), return_tensors="pt").to(device)
        with torch.no_grad():
            logits = rm(**inputs).logits.squeeze().cpu().numpy()
        return logits[9], logits[4]  # helpfulness, verbosity

    def score_file(file_path):
        print(f"\n📂 Scoring file: {file_path}")
        df = pd.read_csv(file_path)

        # 确保两列存在
        if "helpfulness" not in df.columns:
            df["helpfulness"] = np.nan
        if "verbosity" not in df.columns:
            df["verbosity"] = np.nan

        total = len(df)
        remaining = df["helpfulness"].isna().sum()
        print(f"🔎 Total: {total} rows, Remaining to score: {remaining}")

        for i, row in tqdm(df.iterrows(), total=total, desc=os.path.basename(file_path), dynamic_ncols=True):
            if pd.notnull(row["helpfulness"]) and pd.notnull(row["verbosity"]):
                continue  # 已经打过分，跳过
            try:
                h, v = score_response(row["prompt"], row["response"])
            except Exception as e:
                print(f"⚠️ Error on row {i}: {e}")
                h, v = None, None
            df.loc[i, "helpfulness"] = h
            df.loc[i, "verbosity"] = v

            # 每打完一个可以即时保存（可选，如果你担心断电）：
            # df.to_csv(save_path, index=False)

        # 保存最终结果
        file_name = os.path.basename(file_path).replace(".csv", "_scored.csv")
        save_path = os.path.join(output_dir, file_name)
        df.to_csv(save_path, index=False)
        print(f"✅ Scoring done → Saved to: {save_path}")
        return df

    for file in os.listdir(input_dir):
        if file.endswith("_fixed.csv"):
            score_file(os.path.join(input_dir, file))

#v5打分
run_reward_scoring(
    input_dir="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v5_anglebased_top5/fixed",
    output_dir="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v5_anglebased_top5/scored"
)

#div选出best response

import os
import pandas as pd

def merge_and_select_best_responses(input_dir, output_file, main_v1=0.7071, main_v2=0.7071):
    """
    合并指定目录中的 `_scored.csv` 文件，计算主方向加权得分，并为每个 prompt 选出得分最高的响应。

    Args:
        input_dir (str): 包含 `_scored.csv` 文件的目录路径
        output_file (str): 最终合并后的输出文件路径
        main_v1 (float): 主方向在 helpfulness 维度上的权重
        main_v2 (float): 主方向在 verbosity 维度上的权重
    """
    dfs = []
    for file in os.listdir(input_dir):
        if file.endswith("_scored.csv"):
            df = pd.read_csv(os.path.join(input_dir, file))
            df["main_v1"] = main_v1
            df["main_v2"] = main_v2
            df["score_total"] = df["main_v1"] * df["helpfulness"] + df["main_v2"] * df["verbosity"]
            dfs.append(df)

    df_all = pd.concat(dfs, ignore_index=True)
    df_best = df_all.loc[df_all.groupby("prompt_id")["score_total"].idxmax()]
    df_best = df_best.rename(columns={"response": "f_best_response"})

    cols_to_keep = [
        "prompt_id", "prompt", "f_best_response",
        "v1_p", "v2_p", "valid_angle",
        "main_v1", "main_v2",
        "helpfulness", "verbosity", "score_total"
    ]
    df_best_clean = df_best[cols_to_keep]

    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    df_best_clean.to_csv(output_file, index=False)
    print(f"✅ 已保存到: {output_file}")

#v5 basleien补充

from google.colab import drive
import os
import numpy as np
import pandas as pd
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM
from tqdm.auto import tqdm
import torch
import time
import random

# ✅ 设置 v_index 为5，对应 v5 = (0.9397, 0.3420)
v_indices = [5]
v_mapping = {5: (0.9397, 0.3420)}

# ✅ 挂载 Google Drive
drive.mount('/content/drive')

# ✅ 环境设置
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
seed = 42
np.random.seed(seed)
random.seed(seed)
torch.manual_seed(seed)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(seed)

# ✅ 输出目录
result_dir = "/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9"
os.makedirs(result_dir, exist_ok=True)

# ✅ 载入数据集
print("📦 Loading prompts from UltraFeedback...")
ds = load_dataset("HuggingFaceH4/ultrafeedback_binarized", split="test_prefs")
prompts = ds["prompt"][:2000]
all_prompt_ids = list(range(len(prompts)))

# ✅ 加载模型
print("🤖 Loading DPA-v1 model...")
model = AutoModelForCausalLM.from_pretrained(
    "Haoxiang-Wang/DPA-v1-Mistral-7B",
    torch_dtype=torch.float16
).to(device)

tokenizer = AutoTokenizer.from_pretrained("Haoxiang-Wang/RewardModel-Mistral-7B-for-DPA-v1")
tokenizer.padding_side = "left"
if tokenizer.pad_token_id is None:
    tokenizer.pad_token = tokenizer.eos_token

# ✅ 构造 Prompt
def build_input(prompt, v1, v2):
    h = int(np.round(v1 * 100))
    v = int(np.round(v2 * 100))
    sys_instruction = f"You are a helpful assistant. Your response should maximize weighted rating = helpfulness*{h} + verbosity*{v}."
    return [{"role": "user", "content": f"{sys_instruction}\n\n{prompt}"}]

# ✅ 批量生成函数
def generate_response_batch(prompts_batch, prompt_ids_batch, v1, v2):
    input_ids_list = []
    for prompt in prompts_batch:
        messages = build_input(prompt, v1, v2)
        input_ids = tokenizer.apply_chat_template(
            messages, add_generation_prompt=True, return_tensors="pt"
        )[0]
        input_ids_list.append(input_ids.to(device))

    input_ids_padded = torch.nn.utils.rnn.pad_sequence(
        input_ids_list, batch_first=True, padding_value=tokenizer.pad_token_id
    ).to(device)

    attention_mask = (input_ids_padded != tokenizer.pad_token_id).to(device)
    max_input_len = input_ids_padded.shape[1]
    dynamic_max_new_tokens = min(2048, 4096 - max_input_len)

    with torch.no_grad():
        outputs = model.generate(
            input_ids=input_ids_padded,
            attention_mask=attention_mask,
            max_new_tokens=dynamic_max_new_tokens,
            temperature=0.7,
            do_sample=True,
            num_return_sequences=3,
            pad_token_id=tokenizer.eos_token_id
        )

    responses = []
    for i in range(len(prompts_batch)):
        prompt_responses = []
        for j in range(3):
            idx = i * 3 + j
            generated_tokens = outputs[idx][input_ids_padded.shape[1]:]
            decoded = tokenizer.decode(generated_tokens, skip_special_tokens=True)
            prompt_responses.append(decoded)
        responses.append({
            "prompt_id": prompt_ids_batch[i],
            "prompt": prompts_batch[i],
            "responses": prompt_responses
        })
    return responses

# ✅ 主逻辑：断点续跑 + 完整单条缺补逻辑
batch_size = 8

for v_index in v_indices:
    v1, v2 = v_mapping[v_index]
    output_file = os.path.join(result_dir, f"batch_v{v_index}.csv")
    print(f"\n🧭 Running v{v_index}: v = ({v1:.4f}, {v2:.4f})")

    if os.path.exists(output_file):
        existing_df = pd.read_csv(output_file)
        done_prompt_ids = set(existing_df["prompt_id"].unique())
        print(f"🔁 Loaded {len(done_prompt_ids)} prompts from previous run.")
    else:
        done_prompt_ids = set()

    remaining_prompt_ids = [pid for pid in all_prompt_ids if pid not in done_prompt_ids]
    print(f"🔍 Remaining prompts to process: {len(remaining_prompt_ids)}")

    start_time = time.time()

    for i in tqdm(range(0, len(remaining_prompt_ids), batch_size), desc=f"Generating v{v_index}"):
        batch_prompt_ids = remaining_prompt_ids[i:i+batch_size]
        prompt_batch = [prompts[pid] for pid in batch_prompt_ids]

        try:
            batch_results = generate_response_batch(prompt_batch, batch_prompt_ids, v1, v2)
            new_rows = []
            for item in batch_results:
                for k, resp in enumerate(item["responses"]):
                    row = {
                        "prompt_id": item["prompt_id"],
                        "v1": v1,
                        "v2": v2,
                        "prompt": item["prompt"],
                        "response_id": k + 1,
                        "response": resp
                    }
                    new_rows.append(row)
            pd.DataFrame(new_rows).to_csv(output_file, mode='a', header=not os.path.exists(output_file), index=False)
        except Exception as e:
            print(f"⚠️ Error at batch {i}-{i+batch_size}: {e}")

    print(f"✅ Completed v{v_index}. Total time: {time.time() - start_time:.1f}s")

#修复v5 basleine
run_single_file_repair("drive/MyDrive/llm_pre_project/dpa_outputs_v9/batch_v5.csv")

import pandas as pd

# 读取CSV文件
file_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/fixed_batch/batch_v5_fixed.csv"
df = pd.read_csv(file_path)

# 查看数据行数（去掉header行）
print("数据总行数（含表头）:", len(df))
print("数据总行数（去掉表头）:", df.shape[0])

#v5 basleine打分
score_fixed_csv_file(
    input_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/fixed_batch/batch_v5_fixed.csv",
    output_dir="drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch"
)

#v5 basleine best response
process_scored_csv_and_extract_best(
    input_file="drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch/batch_v5_fixed_scored.csv",
    output_dir="drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch/best_response",
    best_response_filename="best_response_v5_baseline.csv"
)

merge_and_select_best_responses(
    input_dir="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v5_anglebased_top5/scored",
    output_file="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v5_anglebased_top5/fdiv_anglebased_v5_all_best_response.csv"
)

#合并fdiv和baseline

import pandas as pd
import os

def merge_baseline_and_fdiv_results(baseline_path, fdiv_path, output_path):
    """
    合并 baseline 和 f-div 的评分结果，按 prompt_id 对齐，并输出合并文件供评估使用。

    Args:
        baseline_path (str): baseline CSV 文件路径
        fdiv_path (str): f-div CSV 文件路径
        output_path (str): 合并结果输出路径（包括文件名）
    """
    # === 加载数据 ===
    df_baseline = pd.read_csv(baseline_path)
    df_fdiv = pd.read_csv(fdiv_path)

    # === 合并数据（按 prompt_id）
    df_merged = pd.merge(
        df_baseline,
        df_fdiv,
        on="prompt_id",
        suffixes=("_baseline", "_fdiv")
    )

    # === 字段重命名 ===
    df_merged = df_merged.rename(columns={
        "prompt_baseline": "prompt",
        "best_response": "baseline_best_response",
        "score_total": "f_score_total",
        "total_score": "baseline_score_total"
    })

    # === 保留字段
    cols_to_show = [
        "prompt_id", "prompt",
        "baseline_best_response", "f_best_response",
        "baseline_score_total", "f_score_total",
        "response_id",
        "v1_p", "v2_p", "valid_angle",
        "main_v1", "main_v2"
    ]
    df_clean = df_merged[cols_to_show]

    # === 保存结果 ===
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    df_clean.to_csv(output_path, index=False)
    print(f"✅ 合并完成，文件保存至：{output_path}")

#合并v5 baseline和fdiv
merge_baseline_and_fdiv_results(
    baseline_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch/best_response/best_response/best_response_v5_baseline.csv",
    fdiv_path="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v5_anglebased_top5/fdiv_anglebased_v5_all_best_response.csv",
    output_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/angle_output/v5_anglebased_final_merged_for_judge.csv"
)

import pandas as pd

# 读取CSV文件
file_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/angle_output/v10_anglebased_final_merged_for_judge.csv"
df = pd.read_csv(file_path)

# 查看数据行数（去掉header行）
print("数据总行数（含表头）:", len(df))
print("数据总行数（去掉表头）:", df.shape[0])

!pip install openai==0.28

#gpt4打分

import pandas as pd
import json
import os
import random

def generate_pairwise_jsonl_for_gpt_judging(input_path, output_path, seed=42):
    """
    构造用于 GPT-4o 判断的 A/B Pairwise JSONL 格式数据（带随机顺序和 auto-tie）

    Args:
        input_path (str): 输入 CSV 文件路径（需包含 prompt, baseline_best_response, f_best_response 等字段）
        output_path (str): 输出 JSONL 文件路径
        seed (int): 随机种子，确保可复现的 A/B 顺序
    """
    # === 确保输出目录存在 ===
    os.makedirs(os.path.dirname(output_path), exist_ok=True)

    # === 读取合并数据 ===
    df = pd.read_csv(input_path)

    # === 设置随机种子 ===
    random.seed(seed)

    output = []
    tie_count = 0

    for idx, row in df.iterrows():
        prompt = row["prompt"]
        baseline = str(row["baseline_best_response"]) if pd.notna(row["baseline_best_response"]) else ""
        fdiv = str(row["f_best_response"]) if pd.notna(row["f_best_response"]) else ""
        pid = row["prompt_id"]
        baseline_response_id = row.get("response_id", None)

        if baseline.strip() == fdiv.strip():
            output.append({
                "pair_id": idx,
                "prompt_id": pid,
                "auto_result": "Tie",
                "baseline_id": baseline_response_id,
                "a_origin": "baseline",
                "b_origin": "f-div",
                "formatted_prompt": "[HH-RLHF]: (Responses identical, auto tie)"
            })
            tie_count += 1
            continue

        # 随机 A/B
        if random.random() < 0.5:
            response_a, response_b = baseline, fdiv
            a_origin, b_origin = "baseline", "f-div"
        else:
            response_a, response_b = fdiv, baseline
            a_origin, b_origin = "f-div", "baseline"

        formatted_prompt = f"""[HH-RLHF]: For the following query to a chatbot, which response is more helpful?

Query: {prompt}

Response A: {response_a}

Response B: {response_b}

FIRST provide a one-sentence comparison of the two responses and explain which you feel is more helpful.
SECOND, on a new line, state only 'A' or 'B' to indicate which response is more helpful.

Format:
Comparison: ...
More helpful: A/B"""

        output.append({
            "pair_id": idx,
            "prompt_id": pid,
            "auto_result": None,
            "baseline_id": baseline_response_id,
            "a_origin": a_origin,
            "b_origin": b_origin,
            "formatted_prompt": formatted_prompt
        })

    # === 保存 JSONL ===
    with open(output_path, "w", encoding="utf-8") as f:
        for item in output:
            f.write(json.dumps(item, ensure_ascii=False) + "\n")

    print(f"✅ 已构造 JSONL 文件，共 {len(output)} 条，自动 Tie: {tie_count}")
    print(f"📁 保存至：{output_path}")

generate_pairwise_jsonl_for_gpt_judging(
    input_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/angle_output/v5_anglebased_final_merged_for_judge.csv",
    output_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/gpt4_judge_inputs_anglebased/v5_pairwise_randomized_angle.jsonl"
)

import pandas as pd

# 读取CSV文件
file_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/angle_output/v5_anglebased_final_merged_for_judge.csv"
df = pd.read_csv(file_path)

# 查看数据行数（去掉header行）
print("数据总行数（含表头）:", len(df))
print("数据总行数（去掉表头）:", df.shape[0])

import openai
import osrps

# 设置 API Key 和代理地址（如果使用代理）
os.environ["OPENAI_API_KEY"] = "sk-XGGe5y0ZvLcQVFp6XnRizs7q47gsVnAbZx0Xr2mfcVlbr99f"
openai.api_key = os.environ["OPENAI_API_KEY"]
openai.api_base = "https://api2.aigcbest.top/v1"  # 如果用代理就改这里

print("✅ OpenAI 设置完成")

import openai
import json
import time
import os
from tqdm import tqdm

def run_gpt_judging(input_path, model="gpt-4o", sleep_time=1.0, max_retries=3):
    """
    使用 OpenAI GPT 模型对 jsonl 格式的 A/B Pairwise 数据进行自动评估。

    Args:
        input_path (str): 输入 JSONL 文件路径（包含 formatted_prompt 字段）
        model (str): 所用 OpenAI 模型名（如 "gpt-4o"）
        sleep_time (float): 每轮之间暂停时间（秒）
        max_retries (int): 每条数据最大重试次数
    """
    output_path = input_path.replace(".jsonl", "_results.jsonl")
    os.makedirs(os.path.dirname(output_path), exist_ok=True)

    # === 加载输入数据 ===
    with open(input_path, "r", encoding="utf-8") as f:
        all_prompts = [json.loads(line) for line in f]

    # === 断点续跑 ===
    completed_ids = set()
    results = []
    if os.path.exists(output_path):
        with open(output_path, "r", encoding="utf-8") as f:
            for line in f:
                item = json.loads(line)
                results.append(item)
                completed_ids.add(item["pair_id"])
        print(f"🔁 已加载 {len(completed_ids)} 条历史结果，跳过")

    start_time = time.time()

    # === 遍历数据评估 ===
    for item in tqdm(all_prompts, desc=f"🧠 {model} 评估中"):
        pid = item["pair_id"]
        if pid in completed_ids:
            continue

        if item.get("auto_result") == "Tie":
            item["gpt_judgment"] = "Tie"
            print(f"🤝 pair_id={pid} → Auto-Tie")
            results.append(item)
            continue

        prompt = item["formatted_prompt"]
        for attempt in range(max_retries):
            try:
                response = openai.ChatCompletion.create(
                    model=model,
                    messages=[{"role": "user", "content": prompt}],
                    temperature=0.0,
                )
                reply = response.choices[0].message["content"].strip()
                item["gpt_raw_response"] = reply

                last_line = reply.strip().splitlines()[-1].strip().upper()
                if "MORE HELPFUL: A" in last_line or last_line == "A":
                    item["gpt_judgment"] = "A"
                elif "MORE HELPFUL: B" in last_line or last_line == "B":
                    item["gpt_judgment"] = "B"
                else:
                    item["gpt_judgment"] = "Unclear"

                print(f"✅ pair_id={pid} → {item['gpt_judgment']}")
                break
            except Exception as e:
                item["error"] = str(e)
                print(f"❌ pair_id={pid} → Error: {str(e)}")
                time.sleep(sleep_time)
        else:
            item["gpt_judgment"] = "Error"
            print(f"❌ pair_id={pid} → Failed after {max_retries} attempts")

        results.append(item)
        time.sleep(sleep_time)

        # 实时写入文件（可选，避免中断）
        with open(output_path, "w", encoding="utf-8") as f:
            for r in results:
                f.write(json.dumps(r, ensure_ascii=False) + "\n")

    end_time = time.time()
    print(f"\n✅ 评估完成，共计 {len(results)} 条")
    print(f"🕒 耗时：{end_time - start_time:.1f} 秒")
    print(f"📁 输出结果路径：{output_path}")

#v5打分
run_gpt_judging(
    input_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/gpt4_judge_inputs_anglebased/v5_pairwise_randomized_angle.jsonl",
    model="gpt-4o-mini"
)

#统计

import json
from collections import Counter
import pandas as pd
import matplotlib.pyplot as plt

def analyze_gpt_judgment_results(results_path):
    """
    分析 GPT 评审结果，并可视化不同选项（baseline / f-div / Tie / Error）的占比。

    Args:
        results_path (str): JSONL 评审结果文件路径，需包含字段 gpt_judgment, a_origin, b_origin
    """

    # === 加载数据 ===
    with open(results_path, "r", encoding="utf-8") as f:
        data = [json.loads(line) for line in f]

    # === 判定获胜方函数 ===
    def judge_winner(item):
        judgment = item.get("gpt_judgment", "")
        a_origin = item.get("a_origin")
        b_origin = item.get("b_origin")

        if judgment == "Tie":
            return "Tie"
        elif judgment == "A":
            return a_origin
        elif judgment == "B":
            return b_origin
        elif judgment == "Unclear":
            return "Unclear"
        elif judgment == "Error":
            return "Error"
        else:
            return "Invalid"

    # === 统计各类结果 ===
    outcomes = [judge_winner(item) for item in data]
    counter = Counter(outcomes)

    total = sum(counter.values())
    win_fdiv = counter.get("f-div", 0)
    win_baseline = counter.get("baseline", 0)
    tie = counter.get("Tie", 0)
    unclear = counter.get("Unclear", 0)
    error = counter.get("Error", 0)

    summary = pd.DataFrame([{
        "Total": total,
        "f-div wins": win_fdiv,
        "baseline wins": win_baseline,
        "Ties": tie,
        "Unclear": unclear,
        "Error": error,
        "f-div win rate (%)": round(win_fdiv / total * 100, 2),
        "baseline win rate (%)": round(win_baseline / total * 100, 2),
        "Tie rate (%)": round(tie / total * 100, 2),
        "Unclear/Error rate (%)": round((unclear + error) / total * 100, 2)
    }])

    print(summary)

    # === 可视化统计图 ===
    summary_plot = {
        "f-div": win_fdiv,
        "baseline": win_baseline,
        "Tie": tie,
        "Unclear": unclear,
        "Error": error
    }

    plt.figure(figsize=(8, 4))
    bars = plt.bar(summary_plot.keys(), summary_plot.values(), color=["#2ca02c", "#1f77b4", "#ff7f0e", "#999999", "#d62728"])
    plt.title("GPT-4o Judged Outcome (Randomized A/B)", fontsize=14)
    plt.ylabel("Count")
    plt.xticks(fontsize=10)
    plt.grid(axis="y", linestyle="--", alpha=0.5)

    for bar, label in zip(bars, summary_plot.keys()):
        count = bar.get_height()
        pct = count / total * 100 if total else 0
        plt.text(bar.get_x() + bar.get_width() / 2, count + 2, f"{pct:.1f}%", ha="center", va="bottom", fontsize=10)

    plt.tight_layout()
    plt.show()

    return summary

analyze_gpt_judgment_results("drive/MyDrive/llm_pre_project/dpa_outputs_v9/gpt4_judge_inputs_anglebased/v5_pairwise_randomized_angle_results.jsonl")

#v6

run_dpa_generation(
    result_dir="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v6_anglebased_top5",
    valid_vs=[
        np.array([0.906308, 0.422618]),  # 25°
        np.array([0.939693, 0.342020]),  # 20°
        np.array([0.866025, 0.5     ]),  # 30°
        np.array([0.965926, 0.258819]),  # 15°
        np.array([0.819152, 0.573576])   # 35°
    ],
    valid_angles=[25, 20, 30, 15, 35],
    main_v=np.array([0.9063, 0.4226]),  # v6
    prompt_limit=2000  # 可按需调整
)

#v6修复
run_response_repair("/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v6_anglebased_top5")

#v6打分
run_reward_scoring(
    input_dir="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v6_anglebased_top5/fixed",
    output_dir="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v6_anglebased_top5/scored"
)

#v6 fdiv选出best response

merge_and_select_best_responses(
    input_dir="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v6_anglebased_top5/scored",
    output_file="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v6_anglebased_top5/fdiv_anglebased_v6_all_best_response.csv"
)

#v6 baslien补充

from google.colab import drive
import os
import numpy as np
import pandas as pd
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM
from tqdm.auto import tqdm
import torch
import time
import random

# ✅ 设置 v_index 为6，对应 v6 = (0.9063, 0.4226)
v_indices = [6]
v_mapping = {6: (0.9063, 0.4226)}

# ✅ 挂载 Google Drive
drive.mount('/content/drive')

# ✅ 环境设置
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
seed = 42
np.random.seed(seed)
random.seed(seed)
torch.manual_seed(seed)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(seed)

# ✅ 输出目录
result_dir = "/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9"
os.makedirs(result_dir, exist_ok=True)

# ✅ 载入数据集
print("📦 Loading prompts from UltraFeedback...")
ds = load_dataset("HuggingFaceH4/ultrafeedback_binarized", split="test_prefs")
prompts = ds["prompt"][:2000]
all_prompt_ids = list(range(len(prompts)))

# ✅ 加载模型
print("🤖 Loading DPA-v1 model...")
model = AutoModelForCausalLM.from_pretrained(
    "Haoxiang-Wang/DPA-v1-Mistral-7B",
    torch_dtype=torch.float16
).to(device)

tokenizer = AutoTokenizer.from_pretrained("Haoxiang-Wang/RewardModel-Mistral-7B-for-DPA-v1")
tokenizer.padding_side = "left"
if tokenizer.pad_token_id is None:
    tokenizer.pad_token = tokenizer.eos_token

# ✅ 构造 Prompt
def build_input(prompt, v1, v2):
    h = int(np.round(v1 * 100))
    v = int(np.round(v2 * 100))
    sys_instruction = f"You are a helpful assistant. Your response should maximize weighted rating = helpfulness*{h} + verbosity*{v}."
    return [{"role": "user", "content": f"{sys_instruction}\n\n{prompt}"}]

# ✅ 批量生成函数
def generate_response_batch(prompts_batch, prompt_ids_batch, v1, v2):
    input_ids_list = []
    for prompt in prompts_batch:
        messages = build_input(prompt, v1, v2)
        input_ids = tokenizer.apply_chat_template(
            messages, add_generation_prompt=True, return_tensors="pt"
        )[0]
        input_ids_list.append(input_ids.to(device))

    input_ids_padded = torch.nn.utils.rnn.pad_sequence(
        input_ids_list, batch_first=True, padding_value=tokenizer.pad_token_id
    ).to(device)

    attention_mask = (input_ids_padded != tokenizer.pad_token_id).to(device)
    max_input_len = input_ids_padded.shape[1]
    dynamic_max_new_tokens = min(2048, 4096 - max_input_len)

    with torch.no_grad():
        outputs = model.generate(
            input_ids=input_ids_padded,
            attention_mask=attention_mask,
            max_new_tokens=dynamic_max_new_tokens,
            temperature=0.7,
            do_sample=True,
            num_return_sequences=3,
            pad_token_id=tokenizer.eos_token_id
        )

    responses = []
    for i in range(len(prompts_batch)):
        prompt_responses = []
        for j in range(3):
            idx = i * 3 + j
            generated_tokens = outputs[idx][input_ids_padded.shape[1]:]
            decoded = tokenizer.decode(generated_tokens, skip_special_tokens=True)
            prompt_responses.append(decoded)
        responses.append({
            "prompt_id": prompt_ids_batch[i],
            "prompt": prompts_batch[i],
            "responses": prompt_responses
        })
    return responses

# ✅ 主逻辑：断点续跑 + 完整单条缺补逻辑
batch_size = 8

for v_index in v_indices:
    v1, v2 = v_mapping[v_index]
    output_file = os.path.join(result_dir, f"batch_v{v_index}.csv")
    print(f"\n🧭 Running v{v_index}: v = ({v1:.4f}, {v2:.4f})")

    if os.path.exists(output_file):
        existing_df = pd.read_csv(output_file)
        done_prompt_ids = set(existing_df["prompt_id"].unique())
        print(f"🔁 Loaded {len(done_prompt_ids)} prompts from previous run.")
    else:
        done_prompt_ids = set()

    remaining_prompt_ids = [pid for pid in all_prompt_ids if pid not in done_prompt_ids]
    print(f"🔍 Remaining prompts to process: {len(remaining_prompt_ids)}")

    start_time = time.time()

    for i in tqdm(range(0, len(remaining_prompt_ids), batch_size), desc=f"Generating v{v_index}"):
        batch_prompt_ids = remaining_prompt_ids[i:i+batch_size]
        prompt_batch = [prompts[pid] for pid in batch_prompt_ids]

        try:
            batch_results = generate_response_batch(prompt_batch, batch_prompt_ids, v1, v2)
            new_rows = []
            for item in batch_results:
                for k, resp in enumerate(item["responses"]):
                    row = {
                        "prompt_id": item["prompt_id"],
                        "v1": v1,
                        "v2": v2,
                        "prompt": item["prompt"],
                        "response_id": k + 1,
                        "response": resp
                    }
                    new_rows.append(row)
            pd.DataFrame(new_rows).to_csv(output_file, mode='a', header=not os.path.exists(output_file), index=False)
        except Exception as e:
            print(f"⚠️ Error at batch {i}-{i+batch_size}: {e}")

    print(f"✅ Completed v{v_index}. Total time: {time.time() - start_time:.1f}s")

#修复v6 basleine
run_single_file_repair("drive/MyDrive/llm_pre_project/dpa_outputs_v9/batch_v6.csv")

#baseline 打分

import os
import time
import pandas as pd
import numpy as np
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
from tqdm.auto import tqdm
from google.colab import drive

def score_fixed_csv_file(input_path, output_dir, model_name="Haoxiang-Wang/RewardModel-Mistral-7B-for-DPA-v1"):
    """
    对固定后的 CSV 文件进行打分并保存结果（支持断点续跑）。
    """
    drive.mount('/content/drive', force_remount=False)
    os.makedirs(output_dir, exist_ok=True)

    print(f"📦 Loading file: {input_path}")
    df = pd.read_csv(input_path)

    # 确保 helpfulness 和 verbosity 列存在
    if "helpfulness" not in df.columns:
        df["helpfulness"] = np.nan
    if "verbosity" not in df.columns:
        df["verbosity"] = np.nan

    print("🤖 Loading Reward Model...")
    device = "cuda" if torch.cuda.is_available() else "cpu"
    rm = AutoModelForSequenceClassification.from_pretrained(
        model_name, trust_remote_code=True
    ).to(device)
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    def score_response(prompt, response):
        template = "[INST] You must read the following conversation carefully and rate the assistant's response from score 0-100 in these aspects: helpfulness, correctness, coherence, honesty, complexity, verbosity\n\nUser: {prompt}\n\nAssistant: {response} [/INST]"
        inputs = tokenizer(template.format(prompt=prompt, response=response), return_tensors="pt").to(device)
        with torch.no_grad():
            logits = rm(**inputs).logits.squeeze().cpu().numpy()
        return logits[9], logits[4]

    print("🧠 Scoring responses...")
    start_time = time.time()

    total_rows = len(df)
    remaining = df[df["helpfulness"].isna()].shape[0]
    print(f"Total rows: {total_rows}, Remaining to score: {remaining}")

    for i, row in tqdm(df.iterrows(), total=total_rows, desc="🔍 Scoring"):
        if pd.notnull(row["helpfulness"]) and pd.notnull(row["verbosity"]):
            continue  # 已打过分，跳过
        try:
            h, v = score_response(row["prompt"], row["response"])
            df.loc[i, "helpfulness"] = h
            df.loc[i, "verbosity"] = v
        except Exception as e:
            print(f"⚠️ Error on row {i}: {e}")

    output_file = os.path.join(output_dir, os.path.basename(input_path).replace(".csv", "_scored.csv"))
    df.to_csv(output_file, index=False)

    end_time = time.time()
    print(f"\n✅ Scoring complete! Total scored: {len(df)}")
    print(f"🕒 Time elapsed: {end_time - start_time:.1f} seconds")
    print(f"📁 Saved to: {output_file}")

#v6 baseline打分

score_fixed_csv_file(
    input_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/fixed_batch/batch_v6_fixed.csv",
    output_dir="drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch"
)

import pandas as pd

# 读取CSV文件
file_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/fixed_batch/batch_v6_fixed.csv"
df = pd.read_csv(file_path)

# 查看数据行数（去掉header行）
print("数据总行数（含表头）:", len(df))
print("数据总行数（去掉表头）:", df.shape[0])

#选出baseline best response

import pandas as pd
import os

def process_scored_csv_and_extract_best(
    input_file,
    output_dir,
    best_response_filename="best_response_v1_baseline.csv"
):
    """
    对打分后的CSV文件添加total_score，并提取每个prompt_id的最佳response。

    Args:
        input_file (str): 输入CSV文件路径（需包含 columns: v1, v2, helpfulness, verbosity）
        output_dir (str): 输出目录，将保存包含total_score的文件和best_response文件
        best_response_filename (str): 最佳response输出文件名
    """
    # 创建必要路径
    os.makedirs(output_dir, exist_ok=True)
    best_response_dir = os.path.join(output_dir, "best_response")
    os.makedirs(best_response_dir, exist_ok=True)

    # 输出路径
    output_file_with_total = os.path.join(output_dir, os.path.basename(input_file).replace(".csv", "_with_total.csv"))
    output_best_file = os.path.join(best_response_dir, best_response_filename)

    # 加载数据
    df = pd.read_csv(input_file)

    # 计算总分
    df["total_score"] = df["v1"] * df["helpfulness"] + df["v2"] * df["verbosity"]

    # 保存带总分文件
    df.to_csv(output_file_with_total, index=False)
    print(f"✅ Saved file with total_score: {output_file_with_total}")

    # 提取每个 prompt_id 的最佳响应
    df_best = df.loc[df.groupby("prompt_id")["total_score"].idxmax()].copy()
    df_best = df_best.rename(columns={"response": "best_response"})

    # 保存最佳响应文件
    df_best.to_csv(output_best_file, index=False)
    print(f"🏆 Best responses per prompt saved to: {output_best_file}")

#v6 basleine best response
process_scored_csv_and_extract_best(
    input_file="drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch/batch_v6_fixed_scored.csv",
    output_dir="drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch/best_response",
    best_response_filename="best_response_v6_baseline.csv"
)

#合并v6 baseline和fdiv
merge_baseline_and_fdiv_results(
    baseline_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch/best_response/best_response/best_response_v6_baseline.csv",
    fdiv_path="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v6_anglebased_top5/fdiv_anglebased_v6_all_best_response.csv",
    output_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/angle_output/v6_anglebased_final_merged_for_judge.csv"
)

#gpt4 judge

generate_pairwise_jsonl_for_gpt_judging(
    input_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/angle_output/v6_anglebased_final_merged_for_judge.csv",
    output_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/gpt4_judge_inputs_anglebased/v6_pairwise_randomized_angle.jsonl"
)

#v6打分
run_gpt_judging(
    input_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/gpt4_judge_inputs_anglebased/v6_pairwise_randomized_angle.jsonl",
    model="gpt-4o-mini"
)

analyze_gpt_judgment_results("drive/MyDrive/llm_pre_project/dpa_outputs_v9/gpt4_judge_inputs_anglebased/v6_pairwise_randomized_angle_results.jsonl")

#v7
run_dpa_generation(
    result_dir="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v7_anglebased_top5",
    valid_vs=[
        np.array([0.866025, 0.5     ]),  # 30°
        np.array([0.819152, 0.573576]),  # 35°
        np.array([0.906308, 0.422618]),  # 25°
        np.array([0.766044, 0.642788]),  # 40°
        np.array([0.939693, 0.342020])   # 20°
    ],
    valid_angles=[30, 35, 25, 40, 20],
    main_v=np.array([0.8660, 0.5000]),  # v7
    prompt_limit=2000  # 可按需调整
)

#v7修复
run_response_repair("/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v7_anglebased_top5")

#v7打分
run_reward_scoring(
    input_dir="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v7_anglebased_top5/fixed",
    output_dir="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v7_anglebased_top5/scored"
)

#v7 basleine打分
score_fixed_csv_file(
    input_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/fixed_batch/batch_v7_fixed.csv",
    output_dir="drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch"
)

#v7 basleine best response
process_scored_csv_and_extract_best(
    input_file="drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch/batch_v7_fixed_scored.csv",
    output_dir="drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch/best_response",
    best_response_filename="best_response_v7_baseline.csv"
)

#v7 fdiv选出best response

merge_and_select_best_responses(
    input_dir="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v7_anglebased_top5/scored",
    output_file="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v7_anglebased_top5/fdiv_anglebased_v7_all_best_response.csv"
)

#合并v7 baseline和fdiv
merge_baseline_and_fdiv_results(
    baseline_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch/best_response/best_response/best_response_v7_baseline.csv",
    fdiv_path="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v7_anglebased_top5/fdiv_anglebased_v7_all_best_response.csv",
    output_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/angle_output/v7_anglebased_final_merged_for_judge.csv"
)

import pandas as pd

# 读取CSV文件
file_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/angle_output/v7_anglebased_final_merged_for_judge.csv"
df = pd.read_csv(file_path)

# 查看数据行数（去掉header行）
print("数据总行数（含表头）:", len(df))
print("数据总行数（去掉表头）:", df.shape[0])

#gpt4打分

#v7打分
generate_pairwise_jsonl_for_gpt_judging(
    input_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/angle_output/v7_anglebased_final_merged_for_judge.csv",
    output_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/gpt4_judge_inputs_anglebased/v7_pairwise_randomized_angle.jsonl"
)

#v7打分
run_gpt_judging(
    input_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/gpt4_judge_inputs_anglebased/v7_pairwise_randomized_angle.jsonl",
    model="gpt-4o-mini"
)

#v7
analyze_gpt_judgment_results("drive/MyDrive/llm_pre_project/dpa_outputs_v9/gpt4_judge_inputs_anglebased/v7_pairwise_randomized_angle_results.jsonl")



#v8
run_dpa_generation(
    result_dir="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v8_anglebased_top5",
    valid_vs=[
        np.array([0.819152, 0.573576]),  # 35°
        np.array([0.866025, 0.5     ]),  # 30°
        np.array([0.766044, 0.642788]),  # 40°
        np.array([0.906308, 0.422618]),  # 25°
        np.array([0.939693, 0.342020])   # 20°
    ],
    valid_angles=[35, 30, 40, 25, 20],
    main_v=np.array([0.8192, 0.5736]),  # v8
    prompt_limit=2000  # 可按需调整
)

#v8修复
run_response_repair("/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v8_anglebased_top5")

#v8打分
run_reward_scoring(
    input_dir="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v8_anglebased_top5/fixed",
    output_dir="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v8_anglebased_top5/scored"
)

#v9
run_dpa_generation(
    result_dir="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v9_anglebased_top5",
    valid_vs=[
        np.array([0.766044, 0.642788]),  # 40°
        np.array([0.819152, 0.573576]),  # 35°
        np.array([0.866025, 0.5     ]),  # 30°
        np.array([0.906308, 0.422618]),  # 25°
        np.array([0.939693, 0.342020])   # 20°
    ],
    valid_angles=[40, 35, 30, 25, 20],
    main_v=np.array([0.7660, 0.6428]),  # v9
    prompt_limit=2000  # 可按需调整
)

#v9修复
run_response_repair("/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v9_anglebased_top5")

#修复baseline

def run_single_file_repair(input_file_path, save_subdir="fixed_batch"):
    import os
    import pandas as pd
    import numpy as np
    import torch
    from tqdm import tqdm
    from transformers import AutoTokenizer, AutoModelForCausalLM
    from accelerate import Accelerator

    # === 模型初始化 ===
    device = "cuda" if torch.cuda.is_available() else "cpu"
    accelerator = Accelerator()

    model = AutoModelForCausalLM.from_pretrained(
        "Haoxiang-Wang/DPA-v1-Mistral-7B",
        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
        device_map="auto" if torch.cuda.is_available() else None
    )
    model = accelerator.prepare(model)

    tokenizer = AutoTokenizer.from_pretrained("Haoxiang-Wang/DPA-v1-Mistral-7B")
    tokenizer.padding_side = "left"
    if tokenizer.pad_token_id is None:
        tokenizer.pad_token = tokenizer.eos_token

    # === 判断 response 是否需要重生成 ===
    def is_invalid_response(resp):
        if pd.isna(resp): return True
        resp = str(resp).strip()
        if not resp: return True
        invalid_keywords = [
            "[A:]", "[/SOLUTION]", "[ASS]", "[A]:", "[/CONV]", "[/CODE]", "[/USER]",
            "[OUTPUT]", "[CC]", "[OUT]", "[/DATA]", "[/ASST]", "[/ME]", "[/REST]",
            "[RESP]", "<[user]>", "<|user|>", "[Response]", "[Assistant]", "[Answer]",
            "[End of Response]", "<human>", "<|endoftext|>", "<<", "[INST]", "[/INST]",
            "[]", "[USER]"
        ]
        if resp in invalid_keywords: return True
        tokens = resp.split()
        if all(token.strip() in invalid_keywords for token in tokens): return True
        tag_like_count = sum(1 for t in tokens if (t.startswith("[") and t.endswith("]")) or (t.startswith("<") and t.endswith(">")))
        return len(tokens) > 0 and tag_like_count / len(tokens) > 0.8

    # === Prompt 构建函数 ===
    def build_input(prompt, v1, v2):
        h = int(np.round(v1 * 100))
        v = int(np.round(v2 * 100))
        sys_instruction = f"You are a helpful assistant. Your response should maximize weighted rating = helpfulness*{h} + verbosity*{v}."
        return [{"role": "system", "content": sys_instruction}, {"role": "user", "content": prompt}]

    # === 重生成函数 ===
    def regenerate_response(prompt, v1, v2):
        try:
            messages = build_input(prompt, v1, v2)
            input_ids = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors="pt").to(device)
            if input_ids.ndim != 2 or input_ids.shape[1] == 0:
                print(f"⚠️ Invalid input_ids shape {input_ids.shape}, skipping: {prompt[:60]}")
                return ""
            outputs = model.generate(
                input_ids=input_ids,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
            generated = outputs[0][input_ids.shape[1]:]
            return tokenizer.decode(generated, skip_special_tokens=True).strip()
        except Exception as e:
            print(f"❌ Error: {e} \nPrompt: {prompt[:60]}")
            return ""

    # === 修复逻辑 ===
    if not os.path.exists(input_file_path):
        print(f"❌ 文件不存在: {input_file_path}")
        return

    df = pd.read_csv(input_file_path)
    bad_mask = df["response"].apply(is_invalid_response)
    num_bad = bad_mask.sum()

    if num_bad == 0:
        print("✅ 无需修复，跳过。")
        return

    print(f"\n📂 检查：{input_file_path}")
    print(f"🔧 检测到无效 response 数量：{num_bad}")
    for i in tqdm(bad_mask[bad_mask].index, total=num_bad, desc=os.path.basename(input_file_path), dynamic_ncols=True):
        row = df.loc[i]
        new_response = regenerate_response(row["prompt"], row["v1"], row["v2"])
        df.at[i, "response"] = new_response

    base_dir = os.path.dirname(input_file_path)
    save_dir = os.path.join(base_dir, save_subdir)
    os.makedirs(save_dir, exist_ok=True)
    file_name = os.path.basename(input_file_path).replace(".csv", "_fixed.csv")
    save_path = os.path.join(save_dir, file_name)
    df.to_csv(save_path, index=False)
    print(f"✅ 修复完成 → 保存到: {save_path}")

#修复v9 basleine
run_single_file_repair("drive/MyDrive/llm_pre_project/dpa_outputs_v9/batch_v9.csv")

#v9 fdiv 打分
run_reward_scoring(
    input_dir="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v9_anglebased_top5/fixed",
    output_dir="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v9_anglebased_top5/scored"
)

#v9 basleine打分
score_fixed_csv_file(
    input_path="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fixed_batch/batch_v9_fixed.csv",
    output_dir="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch"
)

#v9 fdiv选出best response

merge_and_select_best_responses(
    input_dir="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v9_anglebased_top5/scored",
    output_file="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v9_anglebased_top5/fdiv_anglebased_v9_all_best_response.csv"
)

#v9 basleine best response
process_scored_csv_and_extract_best(
    input_file="drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch/batch_v9_fixed_scored.csv",
    output_dir="drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch/best_response",
    best_response_filename="best_response_v9_baseline.csv"
)

#合并v9 baseline和fdiv
merge_baseline_and_fdiv_results(
    baseline_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch/best_response/best_response/best_response_v9_baseline.csv",
    fdiv_path="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v9_anglebased_top5/fdiv_anglebased_v9_all_best_response.csv",
    output_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/angle_output/v9_anglebased_final_merged_for_judge.csv"
)

#v9打分
generate_pairwise_jsonl_for_gpt_judging(
    input_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/angle_output/v9_anglebased_final_merged_for_judge.csv",
    output_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/gpt4_judge_inputs_anglebased/v9_pairwise_randomized_angle.jsonl"
)

#v9打分
run_gpt_judging(
    input_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/gpt4_judge_inputs_anglebased/v9_pairwise_randomized_angle.jsonl",
    model="gpt-4o-mini"
)

#v9
analyze_gpt_judgment_results("drive/MyDrive/llm_pre_project/dpa_outputs_v9/gpt4_judge_inputs_anglebased/v9_pairwise_randomized_angle_results.jsonl")





#v10
run_dpa_generation(
    result_dir="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v10_anglebased_top5",
    valid_vs=[
        np.array([0.766044, 0.642788]),  # 40°
        np.array([0.819152, 0.573576]),  # 35°
        np.array([0.866025, 0.5     ]),  # 30°
        np.array([0.906308, 0.422618]),  # 25°
        np.array([0.939693, 0.342020])   # 20°
    ],
    valid_angles=[40, 35, 30, 25, 20],
    main_v=np.array([0.7071, 0.7071]),  # v10
    prompt_limit=2000  # 可按需调整
)

#v10修复
run_response_repair("/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v10_anglebased_top5")

#v10打分
run_reward_scoring(
    input_dir="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v10_anglebased_top5/fixed",
    output_dir="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v10_anglebased_top5/scored"
)

#v10 fdiv打分
run_reward_scoring(
    input_dir="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v10_anglebased_top5/fixed",
    output_dir="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v10_anglebased_top5/scored"
)

#v10 baseline补充

from google.colab import drive
import os
import numpy as np
import pandas as pd
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM
from tqdm.auto import tqdm
import torch
import time
import random

# ✅ 设置 v_index 为10，对应 v10 = (0.7071, 0.7071)
v_indices = [10]
v_mapping = {10: (0.7071, 0.7071)}

# ✅ 挂载 Google Drive
drive.mount('/content/drive')

# ✅ 环境设置
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
seed = 42
np.random.seed(seed)
random.seed(seed)
torch.manual_seed(seed)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(seed)

# ✅ 输出目录
result_dir = "/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9"
os.makedirs(result_dir, exist_ok=True)

# ✅ 载入数据集
print("📦 Loading prompts from UltraFeedback...")
ds = load_dataset("HuggingFaceH4/ultrafeedback_binarized", split="test_prefs")
prompts = ds["prompt"][:2000]
all_prompt_ids = list(range(len(prompts)))

# ✅ 加载模型
print("🤖 Loading DPA-v1 model...")
model = AutoModelForCausalLM.from_pretrained(
    "Haoxiang-Wang/DPA-v1-Mistral-7B",
    torch_dtype=torch.float16
).to(device)

tokenizer = AutoTokenizer.from_pretrained("Haoxiang-Wang/RewardModel-Mistral-7B-for-DPA-v1")
tokenizer.padding_side = "left"
if tokenizer.pad_token_id is None:
    tokenizer.pad_token = tokenizer.eos_token

# ✅ 构造 Prompt
def build_input(prompt, v1, v2):
    h = int(np.round(v1 * 100))
    v = int(np.round(v2 * 100))
    sys_instruction = f"You are a helpful assistant. Your response should maximize weighted rating = helpfulness*{h} + verbosity*{v}."
    return [{"role": "user", "content": f"{sys_instruction}\n\n{prompt}"}]

# ✅ 批量生成函数
def generate_response_batch(prompts_batch, prompt_ids_batch, v1, v2):
    input_ids_list = []
    for prompt in prompts_batch:
        messages = build_input(prompt, v1, v2)
        input_ids = tokenizer.apply_chat_template(
            messages, add_generation_prompt=True, return_tensors="pt"
        )[0]
        input_ids_list.append(input_ids.to(device))

    input_ids_padded = torch.nn.utils.rnn.pad_sequence(
        input_ids_list, batch_first=True, padding_value=tokenizer.pad_token_id
    ).to(device)

    attention_mask = (input_ids_padded != tokenizer.pad_token_id).to(device)
    max_input_len = input_ids_padded.shape[1]
    dynamic_max_new_tokens = min(2048, 4096 - max_input_len)

    with torch.no_grad():
        outputs = model.generate(
            input_ids=input_ids_padded,
            attention_mask=attention_mask,
            max_new_tokens=dynamic_max_new_tokens,
            temperature=0.7,
            do_sample=True,
            num_return_sequences=3,
            pad_token_id=tokenizer.eos_token_id
        )

    responses = []
    for i in range(len(prompts_batch)):
        prompt_responses = []
        for j in range(3):
            idx = i * 3 + j
            generated_tokens = outputs[idx][input_ids_padded.shape[1]:]
            decoded = tokenizer.decode(generated_tokens, skip_special_tokens=True)
            prompt_responses.append(decoded)
        responses.append({
            "prompt_id": prompt_ids_batch[i],
            "prompt": prompts_batch[i],
            "responses": prompt_responses
        })
    return responses

# ✅ 主逻辑：断点续跑 + 完整单条缺补逻辑
batch_size = 8

for v_index in v_indices:
    v1, v2 = v_mapping[v_index]
    output_file = os.path.join(result_dir, f"batch_v{v_index}.csv")
    print(f"\n🧭 Running v{v_index}: v = ({v1:.4f}, {v2:.4f})")

    if os.path.exists(output_file):
        existing_df = pd.read_csv(output_file)
        done_prompt_ids = set(existing_df["prompt_id"].unique())
        print(f"🔁 Loaded {len(done_prompt_ids)} prompts from previous run.")
    else:
        done_prompt_ids = set()

    remaining_prompt_ids = [pid for pid in all_prompt_ids if pid not in done_prompt_ids]
    print(f"🔍 Remaining prompts to process: {len(remaining_prompt_ids)}")

    start_time = time.time()

    for i in tqdm(range(0, len(remaining_prompt_ids), batch_size), desc=f"Generating v{v_index}"):
        batch_prompt_ids = remaining_prompt_ids[i:i+batch_size]
        prompt_batch = [prompts[pid] for pid in batch_prompt_ids]

        try:
            batch_results = generate_response_batch(prompt_batch, batch_prompt_ids, v1, v2)
            new_rows = []
            for item in batch_results:
                for k, resp in enumerate(item["responses"]):
                    row = {
                        "prompt_id": item["prompt_id"],
                        "v1": v1,
                        "v2": v2,
                        "prompt": item["prompt"],
                        "response_id": k + 1,
                        "response": resp
                    }
                    new_rows.append(row)
            pd.DataFrame(new_rows).to_csv(output_file, mode='a', header=not os.path.exists(output_file), index=False)
        except Exception as e:
            print(f"⚠️ Error at batch {i}-{i+batch_size}: {e}")

    print(f"✅ Completed v{v_index}. Total time: {time.time() - start_time:.1f}s")

#修复v10 basleine
run_single_file_repair("drive/MyDrive/llm_pre_project/dpa_outputs_v9/batch_v10.csv")

#v10 basleine打分
score_fixed_csv_file(
    input_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/fixed_batch/batch_v10_fixed.csv",
    output_dir="drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch"
)

#v10 fdiv选出best response

merge_and_select_best_responses(
    input_dir="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v10_anglebased_top5/scored",
    output_file="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v10_anglebased_top5/fdiv_anglebased_v10_all_best_response.csv"
)

#v10 basleine best response
process_scored_csv_and_extract_best(
    input_file="drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch/batch_v10_fixed_scored.csv",
    output_dir="drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch/best_response",
    best_response_filename="best_response_v10_baseline.csv"
)

#合并v10 baseline和fdiv
merge_baseline_and_fdiv_results(
    baseline_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch/best_response/best_response/best_response_v10_baseline.csv",
    fdiv_path="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v10_anglebased_top5/fdiv_anglebased_v10_all_best_response.csv",
    output_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/angle_output/v10_anglebased_final_merged_for_judge.csv"
)

generate_pairwise_jsonl_for_gpt_judging(
    input_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/angle_output/v10_anglebased_final_merged_for_judge.csv",
    output_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/gpt4_judge_inputs_anglebased/v10_pairwise_randomized_angle.jsonl"
)

#v10打分
run_gpt_judging(
    input_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/gpt4_judge_inputs_anglebased/v10_pairwise_randomized_angle.jsonl",
    model="gpt-4o-mini"
)

#v10
analyze_gpt_judgment_results("drive/MyDrive/llm_pre_project/dpa_outputs_v9/gpt4_judge_inputs_anglebased/v10_pairwise_randomized_angle_results.jsonl")

#补跑v8

!pip install -U datasets

from google.colab import drive
import os
import numpy as np
import pandas as pd
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM
from tqdm.auto import tqdm
import torch
import time
import random

# ✅ 设置 v_index 为8，对应 v8 = (0.8192, 0.5736)
v_indices = [8]
v_mapping = {8: (0.8192, 0.5736)}

# ✅ 挂载 Google Drive
drive.mount('/content/drive')

# ✅ 环境设置
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
seed = 42
np.random.seed(seed)
random.seed(seed)
torch.manual_seed(seed)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(seed)

# ✅ 输出目录
result_dir = "/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9"
os.makedirs(result_dir, exist_ok=True)

# ✅ 载入数据集
print("📦 Loading prompts from UltraFeedback...")
ds = load_dataset("HuggingFaceH4/ultrafeedback_binarized", split="test_prefs")
prompts = ds["prompt"][:2000]
all_prompt_ids = list(range(len(prompts)))

# ✅ 加载模型
print("🤖 Loading DPA-v1 model...")
model = AutoModelForCausalLM.from_pretrained(
    "Haoxiang-Wang/DPA-v1-Mistral-7B",
    torch_dtype=torch.float16
).to(device)

tokenizer = AutoTokenizer.from_pretrained("Haoxiang-Wang/RewardModel-Mistral-7B-for-DPA-v1")
tokenizer.padding_side = "left"
if tokenizer.pad_token_id is None:
    tokenizer.pad_token = tokenizer.eos_token

# ✅ 构造 Prompt
def build_input(prompt, v1, v2):
    h = int(np.round(v1 * 100))
    v = int(np.round(v2 * 100))
    sys_instruction = f"You are a helpful assistant. Your response should maximize weighted rating = helpfulness*{h} + verbosity*{v}."
    return [{"role": "user", "content": f"{sys_instruction}\n\n{prompt}"}]

# ✅ 批量生成函数
def generate_response_batch(prompts_batch, prompt_ids_batch, v1, v2):
    input_ids_list = []
    for prompt in prompts_batch:
        messages = build_input(prompt, v1, v2)
        input_ids = tokenizer.apply_chat_template(
            messages, add_generation_prompt=True, return_tensors="pt"
        )[0]
        input_ids_list.append(input_ids.to(device))

    input_ids_padded = torch.nn.utils.rnn.pad_sequence(
        input_ids_list, batch_first=True, padding_value=tokenizer.pad_token_id
    ).to(device)

    attention_mask = (input_ids_padded != tokenizer.pad_token_id).to(device)
    max_input_len = input_ids_padded.shape[1]
    dynamic_max_new_tokens = min(2048, 4096 - max_input_len)

    with torch.no_grad():
        outputs = model.generate(
            input_ids=input_ids_padded,
            attention_mask=attention_mask,
            max_new_tokens=dynamic_max_new_tokens,
            temperature=0.7,
            do_sample=True,
            num_return_sequences=3,
            pad_token_id=tokenizer.eos_token_id
        )

    responses = []
    for i in range(len(prompts_batch)):
        prompt_responses = []
        for j in range(3):
            idx = i * 3 + j
            generated_tokens = outputs[idx][input_ids_padded.shape[1]:]
            decoded = tokenizer.decode(generated_tokens, skip_special_tokens=True)
            prompt_responses.append(decoded)
        responses.append({
            "prompt_id": prompt_ids_batch[i],
            "prompt": prompts_batch[i],
            "responses": prompt_responses
        })
    return responses

# ✅ 主逻辑：断点续跑 + 完整单条缺补逻辑
batch_size = 8

for v_index in v_indices:
    v1, v2 = v_mapping[v_index]
    output_file = os.path.join(result_dir, f"batch_v{v_index}.csv")
    print(f"\n🧭 Running v{v_index}: v = ({v1:.4f}, {v2:.4f})")

    if os.path.exists(output_file):
        existing_df = pd.read_csv(output_file)
        done_prompt_ids = set(existing_df["prompt_id"].unique())
        print(f"🔁 Loaded {len(done_prompt_ids)} prompts from previous run.")
    else:
        done_prompt_ids = set()

    remaining_prompt_ids = [pid for pid in all_prompt_ids if pid not in done_prompt_ids]
    print(f"🔍 Remaining prompts to process: {len(remaining_prompt_ids)}")

    start_time = time.time()

    for i in tqdm(range(0, len(remaining_prompt_ids), batch_size), desc=f"Generating v{v_index}"):
        batch_prompt_ids = remaining_prompt_ids[i:i+batch_size]
        prompt_batch = [prompts[pid] for pid in batch_prompt_ids]

        try:
            batch_results = generate_response_batch(prompt_batch, batch_prompt_ids, v1, v2)
            new_rows = []
            for item in batch_results:
                for k, resp in enumerate(item["responses"]):
                    row = {
                        "prompt_id": item["prompt_id"],
                        "v1": v1,
                        "v2": v2,
                        "prompt": item["prompt"],
                        "response_id": k + 1,
                        "response": resp
                    }
                    new_rows.append(row)
            pd.DataFrame(new_rows).to_csv(output_file, mode='a', header=not os.path.exists(output_file), index=False)
        except Exception as e:
            print(f"⚠️ Error at batch {i}-{i+batch_size}: {e}")

    print(f"✅ Completed v{v_index}. Total time: {time.time() - start_time:.1f}s")

#修复v8 basleine
run_single_file_repair("drive/MyDrive/llm_pre_project/dpa_outputs_v9/batch_v8.csv")

import pandas as pd

# 读取CSV文件
file_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/angle_output/v8_anglebased_final_merged_for_judge.csv"
df = pd.read_csv(file_path)

# 查看数据行数（去掉header行）
print("数据总行数（含表头）:", len(df))
print("数据总行数（去掉表头）:", df.shape[0])

#v8 basleine打分
score_fixed_csv_file(
    input_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/fixed_batch/batch_v8_fixed.csv",
    output_dir="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch"
)

#v8 basleine best response
process_scored_csv_and_extract_best(
    input_file="drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch/batch_v8_fixed_scored.csv",
    output_dir="drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch/best_response",
    best_response_filename="best_response_v8_baseline.csv"
)

#v8 fdiv选出best response

merge_and_select_best_responses(
    input_dir="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v8_anglebased_top5/scored",
    output_file="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v8_anglebased_top5/fdiv_anglebased_v8_all_best_response.csv"
)


#合并v8 baseline和fdiv
merge_baseline_and_fdiv_results(
    baseline_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch/best_response/best_response/best_response_v8_baseline.csv",
    fdiv_path="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v8_anglebased_top5/fdiv_anglebased_v8_all_best_response.csv",
    output_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/angle_output/v8_anglebased_final_merged_for_judge.csv"
)

generate_pairwise_jsonl_for_gpt_judging(
    input_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/angle_output/v8_anglebased_final_merged_for_judge.csv",
    output_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/gpt4_judge_inputs_anglebased/v8_pairwise_randomized_angle.jsonl"
)

#v8打分
run_gpt_judging(
    input_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/gpt4_judge_inputs_anglebased/v8_pairwise_randomized_angle.jsonl",
    model="gpt-4o-mini"
)

#v8统计
analyze_gpt_judgment_results("drive/MyDrive/llm_pre_project/dpa_outputs_v9/gpt4_judge_inputs_anglebased/v8_pairwise_randomized_angle_results.jsonl")

# v4
run_dpa_generation(
    result_dir="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v4_anglebased_top5",
    valid_vs=[
        np.array([0.965926, 0.258819]),  # 15°
        np.array([0.984808, 0.173648]),  # 10°
        np.array([0.939693, 0.342020]),  # 20°
        np.array([0.996195, 0.087156]),  # 5°
        np.array([0.906308, 0.422618])   # 25°
    ],
    valid_angles=[15, 10, 20, 5, 25],
    main_v=np.array([0.965926, 0.258819]),  # v4 主方向为 15°
    prompt_limit=2000  # 可按需调整
)

!pip install -U datasets

#v4修复
run_response_repair("/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v4_anglebased_top5")

#v4 fdiv打分
run_reward_scoring(
    input_dir="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v4_anglebased_top5/fixed",
    output_dir="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v4_anglebased_top5/scored"
)

import pandas as pd

# 读取CSV文件
file_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/angle_output/v4_anglebased_final_merged_for_judge.csv"
df = pd.read_csv(file_path)

# 查看数据行数（去掉header行）
print("数据总行数（含表头）:", len(df))
print("数据总行数（去掉表头）:", df.shape[0])

#修复v4 basleine
run_single_file_repair("drive/MyDrive/llm_pre_project/dpa_outputs_v9/batch_v4.csv")

#v4 basleine打分
score_fixed_csv_file(
    input_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/fixed_batch/batch_v4_fixed.csv",
    output_dir="drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch"
)

#v4 basleine best response
process_scored_csv_and_extract_best(
    input_file="drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch/batch_v4_fixed_scored.csv",
    output_dir="drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch/best_response",
    best_response_filename="best_response_v4_baseline.csv"
)

#v4 fdiv选出best response

merge_and_select_best_responses(
    input_dir="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v4_anglebased_top5/scored",
    output_file="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v4_anglebased_top5/fdiv_anglebased_v4_all_best_response.csv"
)


#合并v4 baseline和fdiv
merge_baseline_and_fdiv_results(
    baseline_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch/best_response/best_response/best_response_v4_baseline.csv",
    fdiv_path="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v4_anglebased_top5/fdiv_anglebased_v4_all_best_response.csv",
    output_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/angle_output/v4_anglebased_final_merged_for_judge.csv"
)

generate_pairwise_jsonl_for_gpt_judging(
    input_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/angle_output/v4_anglebased_final_merged_for_judge.csv",
    output_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/gpt4_judge_inputs_anglebased/v4_pairwise_randomized_angle.jsonl"
)

#v4打分
run_gpt_judging(
    input_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/gpt4_judge_inputs_anglebased/v4_pairwise_randomized_angle.jsonl",
    model="gpt-4o-mini"
)

#v4统计
analyze_gpt_judgment_results("drive/MyDrive/llm_pre_project/dpa_outputs_v9/gpt4_judge_inputs_anglebased/v4_pairwise_randomized_angle_results.jsonl")

# v3
run_dpa_generation(
    result_dir="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v3_anglebased_top5",
    valid_vs=[
        np.array([0.984808, 0.173648]),  # 10°
        np.array([0.996195, 0.087156]),  # 5°
        np.array([0.965926, 0.258819]),  # 15°
        np.array([1.000000, 0.000000]),  # 0°
        np.array([0.939693, 0.342020])   # 20°
    ],
    valid_angles=[10, 5, 15, 0, 20],
    main_v=np.array([0.984808, 0.173648]),  # v3 主方向为 10°
    prompt_limit=2000  # 可按需调整
)

#v3修复
run_response_repair("/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v3_anglebased_top5")

#v3 fdiv打分
run_reward_scoring(
    input_dir="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v3_anglebased_top5/fixed",
    output_dir="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v3_anglebased_top5/scored"
)

#v3 basleine打分
score_fixed_csv_file(
    input_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/fixed_batch/batch_v3_fixed.csv",
    output_dir="drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch"
)

#v3 basleine best response
process_scored_csv_and_extract_best(
    input_file="drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch/batch_v3_fixed_scored.csv",
    output_dir="drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch/best_response",
    best_response_filename="best_response_v3_baseline.csv"
)

#3 fdiv选出best response

merge_and_select_best_responses(
    input_dir="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v3_anglebased_top5/scored",
    output_file="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v3_anglebased_top5/fdiv_anglebased_v3_all_best_response.csv"
)


#合并v3 baseline和fdiv
merge_baseline_and_fdiv_results(
    baseline_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch/best_response/best_response/best_response_v3_baseline.csv",
    fdiv_path="/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v3_anglebased_top5/fdiv_anglebased_v3_all_best_response.csv",
    output_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/angle_output/v3_anglebased_final_merged_for_judge.csv"
)

generate_pairwise_jsonl_for_gpt_judging(
    input_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/angle_output/v3_anglebased_final_merged_for_judge.csv",
    output_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/gpt4_judge_inputs_anglebased/v3_pairwise_randomized_angle.jsonl"
)

#v3打分
run_gpt_judging(
    input_path="drive/MyDrive/llm_pre_project/dpa_outputs_v9/gpt4_judge_inputs_anglebased/v3_pairwise_randomized_angle.jsonl",
    model="gpt-4o-mini"
)

#v3统计
analyze_gpt_judgment_results("drive/MyDrive/llm_pre_project/dpa_outputs_v9/gpt4_judge_inputs_anglebased/v3_pairwise_randomized_angle_results.jsonl")









