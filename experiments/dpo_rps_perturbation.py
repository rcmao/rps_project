# dpo_rps_perturbation_final.py
# æœ€ç»ˆç‰ˆæœ¬ï¼šåŸºäºDPOæ¨¡å‹ä½¿ç”¨RPSæ–¹æ³•ï¼Œåœ¨UltraFeedbackå‰2000ä¸ªpromptä¸Šæµ‹è¯•

import os
import numpy as np
import pandas as pd
import json
import random
import time
import openai
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification
from tqdm.auto import tqdm
import torch
from collections import Counter

# ğŸ‡¨ğŸ‡³ è®¾ç½®å›½å†…é•œåƒ
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
os.environ['HUGGINGFACE_HUB_CACHE'] = '/root/.cache/huggingface'

# ğŸ¤– è®¾ç½® OpenAI API
os.environ["OPENAI_API_KEY"] = "sk-XGGe5y0ZvLcQVFp6XnRizs7q47gsVnAbZx0Xr2mfcVlbr99f"
openai.api_key = os.environ["OPENAI_API_KEY"]
openai.api_base = "https://api2.aigcbest.top/v1"

print("âœ… OpenAI è®¾ç½®å®Œæˆ")

# å®šä¹‰DPOçš„ä¸»æ–¹å‘å‘é‡
DPO_PREFERENCE_DIRECTIONS = {
    "v3": {"vector": (0.9848, 0.1736), "angle": 10},
    "v4": {"vector": (0.9659, 0.2588), "angle": 15}, 
    "v5": {"vector": (0.9397, 0.3420), "angle": 20},
    "v6": {"vector": (0.9063, 0.4226), "angle": 25},
    "v7": {"vector": (0.8660, 0.5000), "angle": 30},
    "v8": {"vector": (0.8192, 0.5736), "angle": 35},
    "v9": {"vector": (0.7660, 0.6428), "angle": 40},
    "v10": {"vector": (0.7071, 0.7071), "angle": 45},
}

def setup_environment():
    """è®¾ç½®ç¯å¢ƒå’Œéšæœºç§å­"""
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ”§ Using device: {device}")
    
    seed = 42
    np.random.seed(seed)
    random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    
    return device

def get_dpo_angle_perturbations(v_main, angle_range=(-30, 30), step=5, theta_max=25, top_k=5):
    """ä¸ºDPOæ–¹å‘ç”Ÿæˆè§’åº¦æ‰°åŠ¨ï¼ˆå‚è€ƒangle_based.pyï¼‰"""
    def angle_between(v1, v2):
        cos_angle = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))
        return np.degrees(np.arccos(np.clip(cos_angle, -1.0, 1.0)))

    main_angle = np.degrees(np.arctan2(v_main[1], v_main[0]))
    
    angle_offsets = np.arange(angle_range[0], angle_range[1] + 1, step)
    perturbed_vs = []
    perturbed_angles = []
    angle_diffs = []

    for offset in angle_offsets:
        new_angle = main_angle + offset
        angle_rad = np.radians(new_angle)
        v = np.array([np.cos(angle_rad), np.sin(angle_rad)])
        
        angle_diff = angle_between(v, v_main)
        if angle_diff <= theta_max:
            perturbed_vs.append(v)
            perturbed_angles.append(new_angle)
            angle_diffs.append(angle_diff)

    sorted_indices = np.argsort(angle_diffs)
    top_indices = sorted_indices[:top_k]
    
    valid_vs = [perturbed_vs[i] for i in top_indices]
    valid_angles = [perturbed_angles[i] for i in top_indices]
    
    print(f"âœ… ç”Ÿæˆäº† {len(valid_vs)} ä¸ªæœ‰æ•ˆæ‰°åŠ¨æ–¹å‘")
    for i, (v, a) in enumerate(zip(valid_vs, valid_angles)):
        print(f"  æ‰°åŠ¨{i+1}: angle={a:.1f}Â°, v=({v[0]:.4f}, {v[1]:.4f})")
    
    return valid_vs, valid_angles

def load_dpo_models(device):
    """åŠ è½½DPOæ¨¡å‹å’ŒRewardæ¨¡å‹"""
    print("ğŸ¤– Loading DPO model...")
    try:
        dpo_model = AutoModelForCausalLM.from_pretrained(
            "HuggingFaceH4/zephyr-7b-beta",  # ğŸ”„ æ›¿æ¢ä¸ºçœŸæ­£çš„DPOæ¨¡å‹
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto" if torch.cuda.is_available() else None,
            trust_remote_code=True
        ).to(device)
        
        dpo_tokenizer = AutoTokenizer.from_pretrained(
            "HuggingFaceH4/zephyr-7b-beta",  # ğŸ”„ æ›¿æ¢ä¸ºå¯¹åº”çš„tokenizer
            trust_remote_code=True
        )
        dpo_tokenizer.padding_side = "left"
        if dpo_tokenizer.pad_token_id is None:
            dpo_tokenizer.pad_token = dpo_tokenizer.eos_token
        print("âœ… DPO model loaded!")
        
    except Exception as e:
        print(f"âŒ DPO model loading failed: {e}")
        return None, None, None, None
    
    print("ğŸ† Loading Reward model...")
    try:
        reward_model = AutoModelForSequenceClassification.from_pretrained(
            "Haoxiang-Wang/RewardModel-Mistral-7B-for-DPA-v1",
            trust_remote_code=True
        ).to(device)
        
        reward_tokenizer = AutoTokenizer.from_pretrained(
            "Haoxiang-Wang/RewardModel-Mistral-7B-for-DPA-v1",
            trust_remote_code=True
        )
        print("âœ… Reward model loaded!")
        
    except Exception as e:
        print(f"âŒ Reward model loading failed: {e}")
        return None, None, None, None
    
    return dpo_model, dpo_tokenizer, reward_model, reward_tokenizer

def build_dpo_input(prompt, v1, v2):
    """æ„é€ DPOè¾“å…¥ï¼ˆä½¿ç”¨DPAæ ¼å¼ä¿æŒä¸€è‡´ï¼‰"""
    h = int(np.round(v1 * 100))
    v = int(np.round(v2 * 100))
    sys_instruction = f"You are a helpful assistant. Your response should maximize weighted rating = helpfulness*{h} + verbosity*{v}."
    return [{"role": "user", "content": f"{sys_instruction}\n\n{prompt}"}]

def generate_dpo_responses_for_perturbation(prompt, prompt_id, v_vec, angle_deg, 
                                          dpo_model, dpo_tokenizer, device, num_responses=3):
    """ä¸ºå•ä¸ªæ‰°åŠ¨æ–¹å‘ç”ŸæˆDPOå“åº”"""
    try:
        v1, v2 = v_vec[0], v_vec[1]
        messages = build_dpo_input(prompt, v1, v2)
        input_ids = dpo_tokenizer.apply_chat_template(
            messages, add_generation_prompt=True, return_tensors="pt"
        ).to(device)
        
        max_input_len = input_ids.shape[1]
        max_new_tokens = min(2048, 4096 - max_input_len)
        
        with torch.no_grad():
            outputs = dpo_model.generate(
                input_ids=input_ids,
                max_new_tokens=max_new_tokens,
                temperature=0.7,
                do_sample=True,
                num_return_sequences=num_responses,
                pad_token_id=dpo_tokenizer.eos_token_id,
                top_p=0.9,  # ğŸ”„ æ·»åŠ top_på‚æ•°
                repetition_penalty=1.1  # ğŸ”„ æ·»åŠ repetition_penaltyå‚æ•°
            )
        
        responses = []
        for i in range(num_responses):
            generated_tokens = outputs[i][input_ids.shape[1]:]
            decoded = dpo_tokenizer.decode(generated_tokens, skip_special_tokens=True)
            responses.append({
                "prompt_id": prompt_id,
                "prompt": prompt,
                "perturbation_vector": f"({v1:.4f}, {v2:.4f})",
                "perturbation_angle": angle_deg,
                "response_id": i + 1,
                "response": decoded
            })
        
        return responses
    
    except Exception as e:
        print(f"âš ï¸ Error generating for prompt {prompt_id}, angle {angle_deg}: {e}")
        return []

def score_dpo_response(prompt, response, reward_model, reward_tokenizer, device, v1, v2):
    """ä½¿ç”¨Reward Modelä¸ºDPOå“åº”æ‰“åˆ†"""
    try:
        template = "[INST] You must read the following conversation carefully and rate the assistant's response from score 0-100 in these aspects: helpfulness, correctness, coherence, honesty, complexity, verbosity\n\nUser: {prompt}\n\nAssistant: {response} [/INST]"
        
        inputs = reward_tokenizer(
            template.format(prompt=prompt, response=response),
            return_tensors="pt"
        ).to(device)
        
        with torch.no_grad():
            logits = reward_model(**inputs).logits.squeeze().cpu().numpy()
        
        helpfulness = logits[9]
        verbosity = logits[4]
        rps_score = v1 * helpfulness + v2 * verbosity
        
        return {
            "helpfulness": float(helpfulness),
            "verbosity": float(verbosity),
            "v1": float(v1),
            "v2": float(v2),
            "rps_score": float(rps_score)
        }
    
    except Exception as e:
        print(f"âš ï¸ Error scoring response: {e}")
        return {
            "helpfulness": 0.0,
            "verbosity": 0.0,
            "v1": float(v1),
            "v2": float(v2),
            "rps_score": 0.0
        }

def run_dpo_rps_for_direction(direction_name, direction_info, prompts, prompt_ids,
                             dpo_model, dpo_tokenizer, reward_model, reward_tokenizer,
                             device, output_dir, batch_size=8):  # ğŸ”„ ä»4æ”¹ä¸º8
    """ä¸ºå•ä¸ªä¸»æ–¹å‘è¿è¡ŒDPO RPSå®éªŒï¼Œæ”¯æŒæ–­ç‚¹ç»­è·‘"""
    
    print(f"\nğŸ¯ å¤„ç†æ–¹å‘ {direction_name}: {direction_info['vector']} ({direction_info['angle']}Â°)")
    
    # æ£€æŸ¥æ˜¯å¦å·²æœ‰æœ€ç»ˆç»“æœ
    output_file = os.path.join(output_dir, f"dpo_rps_{direction_name}_best_responses.csv")
    if os.path.exists(output_file):
        print(f"âœ… å‘ç°å·²æœ‰æœ€ç»ˆç»“æœï¼Œç›´æ¥åŠ è½½: {output_file}")
        df = pd.read_csv(output_file)
        return df.to_dict("records")
    
    # ç”Ÿæˆæ‰°åŠ¨æ–¹å‘
    main_v = np.array(direction_info["vector"])
    valid_vs, valid_angles = get_dpo_angle_perturbations(
        v_main=main_v,
        angle_range=(-30, 30),
        step=5,
        theta_max=25,
        top_k=5
    )
    
    # ä¸ºæ¯ä¸ªæ‰°åŠ¨æ–¹å‘ç”Ÿæˆå“åº”
    all_perturbation_results = []
    
    for i, (v_vec, angle_deg) in enumerate(zip(valid_vs, valid_angles)):
        print(f"\nğŸ“ æ‰°åŠ¨æ–¹å‘ {i+1}: angle={angle_deg:.1f}Â°, v=({v_vec[0]:.4f}, {v_vec[1]:.4f})")
        
        # æ£€æŸ¥å•ä¸ªæ‰°åŠ¨æ–¹å‘çš„æ–­ç‚¹ç»­è·‘
        perturbation_file = os.path.join(output_dir, f"temp_{direction_name}_perturbation_{i+1}.csv")
        if os.path.exists(perturbation_file):
            print(f"ğŸ” åŠ è½½å·²æœ‰æ‰°åŠ¨ç»“æœ: {perturbation_file}")
            df_temp = pd.read_csv(perturbation_file)
            all_perturbation_results.extend(df_temp.to_dict("records"))
            continue
        
        v1, v2 = v_vec[0], v_vec[1]
        perturbation_results = []
        
        # æ‰¹é‡å¤„ç†prompts
        for start in tqdm(range(0, len(prompts), batch_size), 
                         desc=f"DPO RPS {direction_name} æ‰°åŠ¨{i+1}"):
            end = min(start + batch_size, len(prompts))
            batch_prompts = prompts[start:end]
            batch_ids = prompt_ids[start:end]
            
            for j, (prompt, pid) in enumerate(zip(batch_prompts, batch_ids)):
                # ç”Ÿæˆå“åº”
                responses = generate_dpo_responses_for_perturbation(
                    prompt, pid, v_vec, angle_deg,
                    dpo_model, dpo_tokenizer, device, num_responses=3
                )
                
                if not responses:
                    continue
                
                # ä¸ºæ¯ä¸ªå“åº”æ‰“åˆ†
                scored_responses = []
                for resp_data in responses:
                    scores = score_dpo_response(
                        resp_data["prompt"], 
                        resp_data["response"],
                        reward_model, reward_tokenizer, device,
                        v1, v2
                    )
                    resp_data.update(scores)
                    scored_responses.append(resp_data)
                
                # é€‰æ‹©æœ€ä½³å“åº”
                if scored_responses:
                    best_response = max(scored_responses, key=lambda x: x["rps_score"])
                    best_response["is_best"] = True
                    best_response["all_scores"] = [r["rps_score"] for r in scored_responses]
                    perturbation_results.append(best_response)
        
        # ä¿å­˜å•ä¸ªæ‰°åŠ¨çš„ç»“æœ
        if perturbation_results:
            df_temp = pd.DataFrame(perturbation_results)
            df_temp.to_csv(perturbation_file, index=False)
            print(f"ğŸ’¾ ä¿å­˜æ‰°åŠ¨{i+1}ç»“æœ: {len(perturbation_results)} ä¸ªå“åº”")
        
        all_perturbation_results.extend(perturbation_results)
    
    # ä¸ºæ¯ä¸ªprompté€‰æ‹©æ‰€æœ‰æ‰°åŠ¨ä¸­çš„æœ€ä½³å“åº”
    prompt_best_results = {}
    for result in all_perturbation_results:
        pid = result["prompt_id"]
        if pid not in prompt_best_results or result["rps_score"] > prompt_best_results[pid]["rps_score"]:
            prompt_best_results[pid] = result
    
    final_results = list(prompt_best_results.values())
    
    # ä¿å­˜æœ€ç»ˆç»“æœ
    df = pd.DataFrame(final_results)
    df.to_csv(output_file, index=False)
    
    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    for i in range(len(valid_vs)):
        temp_file = os.path.join(output_dir, f"temp_{direction_name}_perturbation_{i+1}.csv")
        if os.path.exists(temp_file):
            os.remove(temp_file)
    
    print(f"âœ… {direction_name} å®Œæˆï¼Œæœ€ä½³å“åº”ä¿å­˜è‡³: {output_file}")
    print(f"ğŸ“Š å¤„ç†äº† {len(final_results)} ä¸ªpromptsï¼Œå¹³å‡RPSå¾—åˆ†: {df['rps_score'].mean():.2f}")
    
    return final_results

def load_dpo_baseline_results(dpo_outputs_dir):
    """åŠ è½½å·²æœ‰çš„DPO baselineç»“æœ"""
    baseline_results = {}
    
    for direction_name in DPO_PREFERENCE_DIRECTIONS.keys():
        file_path = os.path.join(dpo_outputs_dir, f"dpo_responses_{direction_name}.csv")
        if os.path.exists(file_path):
            df = pd.read_csv(file_path)
            baseline_results[direction_name] = df
            print(f"âœ… åŠ è½½ {direction_name} baseline: {len(df)} å“åº”")
        else:
            print(f"âš ï¸ æœªæ‰¾åˆ° {direction_name} baseline æ–‡ä»¶: {file_path}")
    
    return baseline_results

def run_gpt_judging(input_path, model="gpt-4o-mini", sleep_time=1.0, max_retries=3):
    """ä½¿ç”¨ OpenAI GPT æ¨¡å‹è¿›è¡Œè‡ªåŠ¨è¯„ä¼°ï¼Œæ”¯æŒæ–­ç‚¹ç»­è·‘"""
    output_path = input_path.replace("_judge_input.jsonl", "_results.jsonl")
    
    # æ–­ç‚¹ç»­è·‘æ£€æŸ¥
    completed_ids = set()
    results = []
    if os.path.exists(output_path):
        with open(output_path, "r", encoding="utf-8") as f:
            for line in f:
                item = json.loads(line)
                results.append(item)
                completed_ids.add(item["pair_id"])
        print(f"ğŸ” å·²åŠ è½½ {len(completed_ids)} æ¡å†å²ç»“æœ")

    # åŠ è½½è¾“å…¥æ•°æ®
    with open(input_path, "r", encoding="utf-8") as f:
        all_prompts = [json.loads(line) for line in f]

    start_time = time.time()

    # éå†æ•°æ®è¯„ä¼°
    for item in tqdm(all_prompts, desc=f"ğŸ§  {model} è¯„ä¼°ä¸­"):
        pid = item["pair_id"]
        if pid in completed_ids:
            continue

        if item.get("auto_result") == "Tie":
            item["gpt_judgment"] = "Tie"
            print(f"ğŸ¤ pair_id={pid} â†’ Auto-Tie")
            results.append(item)
            # å®æ—¶ä¿å­˜
            with open(output_path, "w", encoding="utf-8") as f:
                for r in results:
                    f.write(json.dumps(r, ensure_ascii=False) + "\n")
            continue

        prompt = item["formatted_prompt"]
        for attempt in range(max_retries):
            try:
                response = openai.ChatCompletion.create(
                    model=model,
                    messages=[{"role": "user", "content": prompt}],
                    temperature=0.0,
                )
                reply = response.choices[0].message["content"].strip()
                item["gpt_raw_response"] = reply

                # è§£æåˆ¤æ–­ç»“æœ
                last_line = reply.strip().splitlines()[-1].strip().upper()
                if "MORE HELPFUL: A" in last_line or last_line == "A":
                    item["gpt_judgment"] = "A"
                elif "MORE HELPFUL: B" in last_line or last_line == "B":
                    item["gpt_judgment"] = "B"
                else:
                    item["gpt_judgment"] = "Unclear"

                print(f"âœ… pair_id={pid} â†’ {item['gpt_judgment']}")
                break
            except Exception as e:
                item["error"] = str(e)
                print(f"âŒ pair_id={pid} â†’ Error: {str(e)}")
                time.sleep(sleep_time)
        else:
            item["gpt_judgment"] = "Error"
            print(f"âŒ pair_id={pid} â†’ Failed after {max_retries} attempts")

        results.append(item)
        time.sleep(sleep_time)

        # å®æ—¶ä¿å­˜ï¼ˆæ¯10ä¸ªä¿å­˜ä¸€æ¬¡ï¼‰
        if len(results) % 10 == 0:
            with open(output_path, "w", encoding="utf-8") as f:
                for r in results:
                    f.write(json.dumps(r, ensure_ascii=False) + "\n")

    # æœ€ç»ˆä¿å­˜
    with open(output_path, "w", encoding="utf-8") as f:
        for r in results:
            f.write(json.dumps(r, ensure_ascii=False) + "\n")

    end_time = time.time()
    print(f"\nâœ… è¯„ä¼°å®Œæˆï¼Œå…±è®¡ {len(results)} æ¡")
    print(f"ğŸ•’ è€—æ—¶ï¼š{end_time - start_time:.1f} ç§’")
    print(f"ğŸ“ è¾“å‡ºç»“æœè·¯å¾„ï¼š{output_path}")
    
    return output_path

def analyze_gpt_judgment_results(results_path):
    """åˆ†æ GPT è¯„å®¡ç»“æœï¼Œè¾“å‡ºç±»ä¼¼compare_resultçš„ç»Ÿè®¡"""
    with open(results_path, "r", encoding="utf-8") as f:
        data = [json.loads(line) for line in f]

    def judge_winner(item):
        judgment = item.get("gpt_judgment", "")
        a_origin = item.get("a_origin")
        b_origin = item.get("b_origin")

        if judgment == "Tie":
            return "Tie"
        elif judgment == "A":
            return a_origin
        elif judgment == "B":
            return b_origin
        elif judgment == "Unclear":
            return "Unclear"
        elif judgment == "Error":
            return "Error"
        else:
            return "Invalid"

    outcomes = [judge_winner(item) for item in data]
    counter = Counter(outcomes)

    total = sum(counter.values())
    win_rps = counter.get("RPS", 0)
    win_baseline = counter.get("Baseline", 0)
    tie = counter.get("Tie", 0)
    unclear = counter.get("Unclear", 0)
    error = counter.get("Error", 0)

    print(f"\nğŸ“Š GPT-4 åˆ¤æ–­ç»“æœç»Ÿè®¡ ({results_path.split('/')[-1]}):")
    print(f"æ€»è®¡: {total}")
    print(f"RPS è·èƒœ: {win_rps} ({win_rps/total*100:.1f}%)")
    print(f"Baseline è·èƒœ: {win_baseline} ({win_baseline/total*100:.1f}%)")
    print(f"å¹³å±€: {tie} ({tie/total*100:.1f}%)")
    print(f"ä¸æ¸…æ¥š: {unclear} ({unclear/total*100:.1f}%)")
    print(f"é”™è¯¯: {error} ({error/total*100:.1f}%)")
    
    if win_rps + win_baseline > 0:
        rps_win_rate = win_rps / (win_rps + win_baseline) * 100
        print(f"\nğŸ† RPS vs Baseline èƒœç‡: {rps_win_rate:.1f}%")
    
    return counter

def merge_and_compare_with_gpt(rps_results, baseline_results, direction_name, output_dir, 
                              run_gpt_judge=True):
    """åˆå¹¶RPSç»“æœå’Œbaselineç»“æœï¼Œå¹¶è¿›è¡ŒGPTåˆ¤æ–­"""
    
    # å°†RPSç»“æœè½¬ä¸ºDataFrame
    df_rps = pd.DataFrame(rps_results)
    df_rps = df_rps.rename(columns={"response": "rps_best_response"})
    
    # å¤„ç†baselineç»“æœ
    if direction_name in baseline_results:
        df_baseline = baseline_results[direction_name].copy()
        
        # ä¸ºbaselineè®¡ç®—å¾—åˆ†ï¼ˆå¦‚æœè¿˜æ²¡æœ‰çš„è¯ï¼‰
        if "dpa_score" not in df_baseline.columns:
            print(f"âš ï¸ {direction_name} baselineç¼ºå°‘è¯„åˆ†ï¼Œè·³è¿‡æ¯”è¾ƒ")
            return None
        
        # é€‰æ‹©æ¯ä¸ªpromptçš„æœ€ä½³baselineå“åº”
        df_baseline_best = df_baseline.loc[df_baseline.groupby("prompt_id")["dpa_score"].idxmax()]
        df_baseline_best = df_baseline_best.rename(columns={
            "response": "baseline_best_response",
            "dpa_score": "baseline_score"
        })
        
        # åˆå¹¶æ•°æ®
        df_merged = pd.merge(
            df_rps[["prompt_id", "prompt", "rps_best_response", "rps_score"]],
            df_baseline_best[["prompt_id", "baseline_best_response", "baseline_score"]],
            on="prompt_id",
            how="inner"
        )
        
        # ä¿å­˜åˆå¹¶ç»“æœ
        comparison_file = os.path.join(output_dir, f"{direction_name}_rps_vs_baseline_comparison.csv")
        df_merged.to_csv(comparison_file, index=False)
        
        print(f"âœ… {direction_name} æ¯”è¾ƒç»“æœä¿å­˜è‡³: {comparison_file}")
        print(f"ğŸ“Š RPSå¹³å‡å¾—åˆ†: {df_merged['rps_score'].mean():.2f}")
        print(f"ğŸ“Š Baselineå¹³å‡å¾—åˆ†: {df_merged['baseline_score'].mean():.2f}")
        
        # è®¡ç®—åŸºäºå¾—åˆ†çš„èƒœç‡
        rps_wins = (df_merged['rps_score'] > df_merged['baseline_score']).sum()
        total = len(df_merged)
        win_rate = rps_wins / total * 100
        print(f"ğŸ“ˆ åŸºäºå¾—åˆ†çš„RPSèƒœç‡: {win_rate:.1f}% ({rps_wins}/{total})")
        
        if run_gpt_judge:
            # ç”ŸæˆGPTåˆ¤æ–­è¾“å…¥ï¼ˆä½¿ç”¨ä¸compare_resultç›¸åŒçš„æ ¼å¼ï¼‰
            judge_input_file = generate_gpt_judge_input(df_merged, direction_name, output_dir)
            
            # è¿è¡ŒGPTåˆ¤æ–­
            print(f"\nğŸ§  å¼€å§‹GPT-4åˆ¤æ–­ {direction_name}...")
            gpt_results_file = run_gpt_judging(judge_input_file, model="gpt-4o-mini")
            
            # åˆ†æGPTåˆ¤æ–­ç»“æœ
            print(f"\nğŸ“ˆ åˆ†æGPTåˆ¤æ–­ç»“æœ {direction_name}:")
            gpt_counter = analyze_gpt_judgment_results(gpt_results_file)
            
            return {
                "comparison_df": df_merged,
                "gpt_results_file": gpt_results_file,
                "gpt_counter": gpt_counter
            }
        
        return {"comparison_df": df_merged}
    
    else:
        print(f"âš ï¸ æœªæ‰¾åˆ° {direction_name} çš„baselineç»“æœ")
        return None

def generate_gpt_judge_input(comparison_df, direction_name, output_dir):
    """ç”ŸæˆGPT-4åˆ¤æ–­çš„è¾“å…¥æ–‡ä»¶ï¼Œæ ¼å¼ä¸compare_resultä¸€è‡´"""
    judge_data = []
    
    for idx, row in comparison_df.iterrows():
        prompt = row["prompt"]
        rps_response = str(row["rps_best_response"]) if pd.notna(row["rps_best_response"]) else ""
        baseline_response = str(row["baseline_best_response"]) if pd.notna(row["baseline_best_response"]) else ""
        
        if rps_response.strip() == baseline_response.strip():
            judge_data.append({
                "pair_id": idx,
                "prompt_id": row["prompt_id"],
                "auto_result": "Tie",
                "baseline_id": None,
                "a_origin": "RPS",
                "b_origin": "Baseline",
                "formatted_prompt": "[Same responses, auto tie]"
            })
            continue
        
        # éšæœºA/Bé¡ºåº
        if random.random() < 0.5:
            response_a, response_b = rps_response, baseline_response
            a_origin, b_origin = "RPS", "Baseline"
        else:
            response_a, response_b = baseline_response, rps_response
            a_origin, b_origin = "Baseline", "RPS"
        
        formatted_prompt = f"""[HH-RLHF]: For the following query to a chatbot, which response is more helpful?

Query: {prompt}

Response A: {response_a}

Response B: {response_b}

FIRST provide a one-sentence comparison of the two responses and explain which you feel is more helpful.
SECOND, on a new line, state only 'A' or 'B' to indicate which response is more helpful.

Format:
Comparison: ...
More helpful: A/B"""
        
        judge_data.append({
            "pair_id": idx,
            "prompt_id": row["prompt_id"],
            "auto_result": None,
            "baseline_id": None,
            "a_origin": a_origin,
            "b_origin": b_origin,
            "formatted_prompt": formatted_prompt
        })
    
    # ä¿å­˜JSONLæ–‡ä»¶ï¼ˆæ–‡ä»¶åæ ¼å¼ä¸compare_resultä¸€è‡´ï¼‰
    output_file = os.path.join(output_dir, f"{direction_name}_pairwise_randomized_rps_judge_input.jsonl")
    with open(output_file, "w", encoding="utf-8") as f:
        for item in judge_data:
            f.write(json.dumps(item, ensure_ascii=False) + "\n")
    
    print(f"âœ… GPTåˆ¤æ–­è¾“å…¥æ–‡ä»¶ä¿å­˜è‡³: {output_file}")
    return output_file

def main():
    """ä¸»å‡½æ•°"""
    # è®¾ç½®ç¯å¢ƒ
    device = setup_environment()
    
    # ğŸ”„ ä¿®å¤è·¯å¾„ï¼ŒæŒ‡å‘æ­£ç¡®çš„baselineç»“æœç›®å½•
    dpo_outputs_dir = "/root/rps/data/dpo_baseline_outputs"  # ä¿®å¤è·¯å¾„
    rps_output_dir = "/root/rps/data/dpo_rps_results"
    comparison_output_dir = "/root/rps/data/dpo_rps_comparisons"
    
    for dir_path in [rps_output_dir, comparison_output_dir]:
        os.makedirs(dir_path, exist_ok=True)
    
    # ğŸ”„ æ”¹ä¸º2000ä¸ªpromptsï¼Œå®Œæ•´å®éªŒ
    print("ğŸ“¦ Loading UltraFeedback dataset...")
    ds = load_dataset("HuggingFaceH4/ultrafeedback_binarized", split="test_prefs")
    prompts = ds["prompt"][:2000]  # ğŸ”„ ä»100æ”¹ä¸º2000
    prompt_ids = list(range(len(prompts)))
    print(f"âœ… åŠ è½½äº† {len(prompts)} ä¸ªprompts")
    
    # åŠ è½½æ¨¡å‹
    dpo_model, dpo_tokenizer, reward_model, reward_tokenizer = load_dpo_models(device)
    if dpo_model is None:
        print("âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œé€€å‡º")
        return
    
    # åŠ è½½DPO baselineç»“æœ
    baseline_results = load_dpo_baseline_results(dpo_outputs_dir)
    
    # ä¸ºæ¯ä¸ªæ–¹å‘è¿è¡ŒRPSå®éªŒ
    # è‡ªåŠ¨åŒ…å«æ‰€æœ‰å®šä¹‰çš„æ–¹å‘
    directions_to_test = list(DPO_PREFERENCE_DIRECTIONS.keys())
    
    all_results = {}
    
    print(f"\nğŸš€ å¼€å§‹åœ¨å‰{len(prompts)}ä¸ªpromptsä¸Šæµ‹è¯•DPO RPSæ–¹æ³•")
    print(f"ğŸ“ å°†æµ‹è¯•æ–¹å‘: {directions_to_test}")
    
    for direction_name in directions_to_test:
        if direction_name not in DPO_PREFERENCE_DIRECTIONS:
            print(f"âš ï¸ è·³è¿‡æœªå®šä¹‰çš„æ–¹å‘: {direction_name}")
            continue
            
        direction_info = DPO_PREFERENCE_DIRECTIONS[direction_name]
        
        print(f"\nğŸš€ å¼€å§‹å¤„ç†æ–¹å‘ {direction_name}")
        
        # è¿è¡ŒRPSå®éªŒï¼ˆæ”¯æŒæ–­ç‚¹ç»­è·‘ï¼‰
        rps_results = run_dpo_rps_for_direction(
            direction_name, direction_info,
            prompts, prompt_ids,
            dpo_model, dpo_tokenizer, reward_model, reward_tokenizer,
            device, rps_output_dir,
            batch_size=8  # ğŸ”„ ä»4æ”¹ä¸º8
        )
        
        # ä¸baselineæ¯”è¾ƒå¹¶è¿›è¡ŒGPTåˆ¤æ–­
        comparison_results = merge_and_compare_with_gpt(
            rps_results, baseline_results, direction_name, comparison_output_dir,
            run_gpt_judge=True
        )
        
        all_results[direction_name] = comparison_results
    
    # æ±‡æ€»æ‰€æœ‰ç»“æœ
    print("\nğŸ‰ æ‰€æœ‰å®éªŒå®Œæˆï¼")
    print(f"ğŸ“ RPSç»“æœä¿å­˜åœ¨: {rps_output_dir}")
    print(f"ğŸ“ æ¯”è¾ƒç»“æœä¿å­˜åœ¨: {comparison_output_dir}")
    
    print("\nğŸ“Š æ€»ä½“ç»“æœæ±‡æ€»:")
    for direction_name, results in all_results.items():
        if results and "gpt_counter" in results:
            counter = results["gpt_counter"]
            total = sum(counter.values())
            win_rps = counter.get("RPS", 0)
            win_baseline = counter.get("Baseline", 0)
            if win_rps + win_baseline > 0:
                rps_win_rate = win_rps / (win_rps + win_baseline) * 100
                print(f"  {direction_name}: RPSèƒœç‡ {rps_win_rate:.1f}% ({win_rps}/{win_rps + win_baseline})")

    # è¾“å‡ºç»“æœæ–‡ä»¶æ ¼å¼è¯´æ˜
    print(f"\nğŸ“„ ç”Ÿæˆçš„ç»“æœæ–‡ä»¶æ ¼å¼:")
    print(f"  - ç±»ä¼¼ä½ çš„compare_resultæ–‡ä»¶å¤¹ä¸­çš„æ ¼å¼")
    print(f"  - æ–‡ä»¶å: {direction_name}_pairwise_randomized_rps_results.jsonl")
    print(f"  - åŒ…å«GPT-4åˆ¤æ–­ç»“æœï¼Œå¯ç›´æ¥ç”¨äºåˆ†æ")

if __name__ == "__main__":
    main()