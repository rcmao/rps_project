 #!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DPO RPS vs Baseline DPO æ¯”è¾ƒè„šæœ¬
ç”Ÿæˆpairwiseæ¯”è¾ƒæ•°æ®ï¼Œç”¨äºGPT-4o-miniè¯„åˆ¤
"""

import os
import json
import random
import pandas as pd
import numpy as np
from pathlib import Path
import time
from tqdm.auto import tqdm

def set_seed(seed=42):
    """è®¾ç½®éšæœºç§å­"""
    random.seed(seed)
    np.random.seed(seed)

def generate_pairwise_jsonl_for_gpt_judging(
    baseline_path, 
    rps_path, 
    output_path, 
    direction_name,
    seed=42,
    max_pairs=None  # æ”¹ä¸ºNoneï¼Œä¸é™åˆ¶æ•°é‡
):
    """
    ç”Ÿæˆç”¨äºGPTè¯„åˆ¤çš„pairwiseæ¯”è¾ƒæ•°æ®
    Args:
        baseline_path: baseline DPOæ–‡ä»¶è·¯å¾„
        rps_path: RPS best responseæ–‡ä»¶è·¯å¾„  
        output_path: è¾“å‡ºjsonlæ–‡ä»¶è·¯å¾„
        direction_name: æ–¹å‘åç§°ï¼ˆå¦‚v3, v4ç­‰ï¼‰
        seed: éšæœºç§å­
        max_pairs: æœ€å¤§æ¯”è¾ƒå¯¹æ•°ï¼ŒNoneè¡¨ç¤ºä¸é™åˆ¶
    """
    print(f"ğŸ”„ Generating pairwise comparisons for {direction_name}")
    
    # è®¾ç½®éšæœºç§å­
    random.seed(seed)
    
    # è¯»å–æ•°æ®
    try:
        baseline_df = pd.read_csv(baseline_path)
        rps_df = pd.read_csv(rps_path)
        print(f" Loaded baseline: {len(baseline_df)} rows")
        print(f"ğŸ“Š Loaded RPS {direction_name}: {len(rps_df)} rows")
    except Exception as e:
        print(f"âŒ Error loading data: {e}")
        return
    
    # ç¡®ä¿åˆ—åä¸€è‡´
    baseline_response_col = 'response' if 'response' in baseline_df.columns else 'best_response'
    rps_response_col = 'best_response' if 'best_response' in rps_df.columns else 'response'
    
    # åˆå¹¶æ•°æ®ï¼Œç¡®ä¿prompt_idåŒ¹é…
    merged_df = pd.merge(
        baseline_df[['prompt_id', 'prompt', baseline_response_col]], 
        rps_df[['prompt_id', 'prompt', rps_response_col]], 
        on='prompt_id', 
        suffixes=('_baseline', '_rps')
    )
    
    print(f" Merged data: {len(merged_df)} matching prompts")
    
    if len(merged_df) == 0:
        print("âŒ No matching prompts found")
        return
    
    # é™åˆ¶æ¯”è¾ƒå¯¹æ•°ï¼ˆå¦‚æœæŒ‡å®šäº†max_pairsï¼‰
    if max_pairs is not None and len(merged_df) > max_pairs:
        merged_df = merged_df.sample(n=max_pairs, random_state=seed)
        print(f" Sampled {max_pairs} pairs for comparison")
    else:
        print(f"ğŸ“Š Using all {len(merged_df)} pairs for comparison")
    
    # ç”Ÿæˆpairwiseæ¯”è¾ƒæ•°æ®
    pairwise_data = []
    
    for idx, row in tqdm(merged_df.iterrows(), total=len(merged_df), desc="Generating pairs"):
        prompt = row['prompt_baseline']  # ä¸¤ä¸ªpromptåº”è¯¥ç›¸åŒ
        
        # éšæœºå†³å®šAå’ŒBçš„ä½ç½®
        if random.random() < 0.5:
            response_a = row[baseline_response_col]
            response_b = row[rps_response_col]
            a_is_baseline = True
        else:
            response_a = row[rps_response_col]
            response_b = row[baseline_response_col]
            a_is_baseline = False
        
        # æ„å»ºæ¯”è¾ƒæ•°æ®
        comparison_item = {
            "id": f"{direction_name}_{idx}",
            "prompt": prompt,
            "response_a": response_a,
            "response_b": response_b,
            "a_is_baseline": a_is_baseline,
            "direction": direction_name,
            "prompt_id": row['prompt_id']
        }
        
        pairwise_data.append(comparison_item)
    
    # ä¿å­˜ä¸ºjsonlæ ¼å¼
    with open(output_path, 'w', encoding='utf-8') as f:
        for item in pairwise_data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
    
    print(f"âœ… Generated {len(pairwise_data)} pairwise comparisons")
    print(f"ğŸ’¾ Saved to: {output_path}")
    
    # ç»Ÿè®¡A/Bä½ç½®åˆ†å¸ƒ
    baseline_as_a = sum(1 for item in pairwise_data if item['a_is_baseline'])
    rps_as_a = len(pairwise_data) - baseline_as_a
    print(f"ğŸ“ˆ Position distribution: Baseline as A: {baseline_as_a}, RPS as A: {rps_as_a}")

def run_gpt_judging(input_path, output_path, model="gpt-4o-mini", sleep_time=1.0, max_retries=3):
    """
    ä½¿ç”¨GPT-4o-miniè¿›è¡Œè¯„åˆ¤
    Args:
        input_path: è¾“å…¥çš„jsonlæ–‡ä»¶è·¯å¾„
        output_path: è¾“å‡ºçš„è¯„åˆ¤ç»“æœæ–‡ä»¶è·¯å¾„
        model: ä½¿ç”¨çš„æ¨¡å‹
        sleep_time: è¯·æ±‚é—´éš”æ—¶é—´
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
    """
    import openai
    
    # è®¾ç½®OpenAI API
    os.environ["OPENAI_API_KEY"] = "sk-XGGe5y0ZvLcQVFp6XnRizs7q47gsVnAbZx0Xr2mfcVlbr99f"
    openai.api_key = os.environ["OPENAI_API_KEY"]
    openai.api_base = "https://api2.aigcbest.top/v1"
    
    print(f"ğŸ¤– Starting GPT judging with {model}")
    
    # è¯»å–è¾“å…¥æ•°æ®
    with open(input_path, 'r', encoding='utf-8') as f:
        data = [json.loads(line) for line in f]
    
    print(f"ï¿½ï¿½ Loaded {len(data)} comparisons to judge")
    
    # è¯„åˆ¤æç¤ºæ¨¡æ¿
    judge_prompt_template = """You are an expert evaluator. Please compare two AI assistant responses to the same user prompt and determine which one is better.

User Prompt: {prompt}

Response A: {response_a}

Response B: {response_b}

Please evaluate based on:
1. Helpfulness: How well does the response address the user's question or request?
2. Accuracy: Is the information provided correct and reliable?
3. Clarity: Is the response clear and easy to understand?
4. Completeness: Does the response provide a comprehensive answer?

Please respond with ONLY one of the following:
- "A" if Response A is better
- "B" if Response B is better  
- "TIE" if they are equally good

Your choice:"""

    results = []
    
    for i, item in tqdm(enumerate(data), total=len(data), desc="Judging"):
        prompt = judge_prompt_template.format(
            prompt=item['prompt'],
            response_a=item['response_a'],
            response_b=item['response_b']
        )
        
        # é‡è¯•æœºåˆ¶
        for attempt in range(max_retries):
            try:
                response = openai.ChatCompletion.create(
                    model=model,
                    messages=[{"role": "user", "content": prompt}],
                    temperature=0.1,
                    max_tokens=10
                )
                
                judgment = response.choices[0].message.content.strip().upper()
                
                # éªŒè¯åˆ¤æ–­ç»“æœ
                if judgment in ['A', 'B', 'TIE']:
                    break
                else:
                    print(f"âš ï¸ Invalid judgment '{judgment}' for item {i}, retrying...")
                    judgment = 'TIE'  # é»˜è®¤å¹³å±€
                    
            except Exception as e:
                print(f"âŒ Error judging item {i} (attempt {attempt + 1}): {e}")
                if attempt == max_retries - 1:
                    judgment = 'TIE'  # æœ€åä¸€æ¬¡å°è¯•å¤±è´¥ï¼Œè®¾ä¸ºå¹³å±€
                else:
                    time.sleep(sleep_time * 2)  # å¤±è´¥æ—¶ç­‰å¾…æ›´é•¿æ—¶é—´
                    continue
        
        # è®°å½•ç»“æœ
        result_item = item.copy()
        result_item['judgment'] = judgment
        result_item['model'] = model
        result_item['judgment_time'] = time.strftime('%Y-%m-%d %H:%M:%S')
        results.append(result_item)
        
        # ä¿å­˜ä¸­é—´ç»“æœ
        if (i + 1) % 50 == 0:
            temp_output_path = output_path.replace('.jsonl', f'_temp_{i+1}.jsonl')
            with open(temp_output_path, 'w', encoding='utf-8') as f:
                for result in results:
                    f.write(json.dumps(result, ensure_ascii=False) + '\n')
            print(f"ï¿½ï¿½ Saved intermediate results: {temp_output_path}")
        
        time.sleep(sleep_time)
    
    # ä¿å­˜æœ€ç»ˆç»“æœ
    with open(output_path, 'w', encoding='utf-8') as f:
        for result in results:
            f.write(json.dumps(result, ensure_ascii=False) + '\n')
    
    print(f"âœ… Judging completed! Results saved to: {output_path}")
    
    # ç»Ÿè®¡ç»“æœ
    judgments = [r['judgment'] for r in results]
    a_wins = judgments.count('A')
    b_wins = judgments.count('B')
    ties = judgments.count('TIE')
    
    print(f"ğŸ“Š Judgment statistics:")
    print(f"   A wins: {a_wins} ({a_wins/len(judgments)*100:.1f}%)")
    print(f"   B wins: {b_wins} ({b_wins/len(judgments)*100:.1f}%)")
    print(f"   Ties: {ties} ({ties/len(judgments)*100:.1f}%)")

def analyze_gpt_judgment_results(results_path, direction_name):
    """
    åˆ†æGPTè¯„åˆ¤ç»“æœ
    Args:
        results_path: è¯„åˆ¤ç»“æœæ–‡ä»¶è·¯å¾„
        direction_name: æ–¹å‘åç§°
    """
    print(f"ğŸ“Š Analyzing results for {direction_name}")
    
    # è¯»å–ç»“æœ
    with open(results_path, 'r', encoding='utf-8') as f:
        results = [json.loads(line) for line in f]
    
    # ç»Ÿè®¡ç»“æœ
    baseline_wins = 0
    rps_wins = 0
    ties = 0
    
    for result in results:
        judgment = result['judgment']
        a_is_baseline = result['a_is_baseline']
        
        if judgment == 'A':
            if a_is_baseline:
                baseline_wins += 1
            else:
                rps_wins += 1
        elif judgment == 'B':
            if a_is_baseline:
                rps_wins += 1
            else:
                baseline_wins += 1
        else:  # TIE
            ties += 1
    
    total = len(results)
    
    print(f"ğŸ“ˆ Analysis for {direction_name}:")
    print(f"   Total comparisons: {total}")
    print(f"   Baseline wins: {baseline_wins} ({baseline_wins/total*100:.1f}%)")
    print(f"   RPS wins: {rps_wins} ({rps_wins/total*100:.1f}%)")
    print(f"   Ties: {ties} ({ties/total*100:.1f}%)")
    
    # è®¡ç®—èƒœç‡
    rps_win_rate = rps_wins / (baseline_wins + rps_wins) if (baseline_wins + rps_wins) > 0 else 0.5
    print(f"   RPS win rate (excluding ties): {rps_win_rate:.3f}")
    
    return {
        'direction': direction_name,
        'total': total,
        'baseline_wins': baseline_wins,
        'rps_wins': rps_wins,
        'ties': ties,
        'rps_win_rate': rps_win_rate
    }

def main():
    """ä¸»å‡½æ•°"""
    # è®¾ç½®è·¯å¾„
    baseline_dir = "/root/rps/data/dpo_merged"
    rps_dir = "/root/rps/data/dpo_rps_best_resposne"
    output_dir = "/root/rps/data/dpo_compare_result"  # ä¿®æ­£è¾“å‡ºç›®å½•
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    # è®¾ç½®éšæœºç§å­
    set_seed(42)
    
    # è¦å¤„ç†çš„æ–¹å‘
    directions = ["v3", "v4", "v5", "v6", "v7", "v8", "v9", "v10"]
    
    print("ğŸš€ Starting DPO RPS vs Baseline comparison")
    print(f" Baseline directory: {baseline_dir}")
    print(f"ğŸ“‚ RPS directory: {rps_dir}")
    print(f"ğŸ“‚ Output directory: {output_dir}")
    
    all_analysis_results = []
    
    for direction in directions:
        print(f"\n{'='*60}")
        print(f"ğŸ¯ Processing direction: {direction}")
        print(f"{'='*60}")
        
        # æ–‡ä»¶è·¯å¾„
        baseline_path = os.path.join(baseline_dir, f"dpo_responses_{direction}_merged.csv")
        rps_path = os.path.join(rps_dir, f"{direction}_best_response.csv")
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(baseline_path):
            print(f"âŒ Baseline file not found: {baseline_path}")
            continue
        if not os.path.exists(rps_path):
            print(f"âŒ RPS file not found: {rps_path}")
            continue
        
        # ç”Ÿæˆpairwiseæ¯”è¾ƒæ•°æ® - ä¸é™åˆ¶æ•°é‡
        pairwise_output = os.path.join(output_dir, f"{direction}_pairwise_randomized_angle_results.jsonl")
        generate_pairwise_jsonl_for_gpt_judging(
            baseline_path=baseline_path,
            rps_path=rps_path,
            output_path=pairwise_output,
            direction_name=direction,
            seed=42,
            max_pairs=None  # æ”¹ä¸ºNoneï¼Œä½¿ç”¨æ‰€æœ‰2000å¯¹
        )
        
        # ä½¿ç”¨GPT-4o-miniè¿›è¡Œè¯„åˆ¤
        judged_output = os.path.join(output_dir, f"{direction}_judged_results.jsonl")
        run_gpt_judging(
            input_path=pairwise_output,
            output_path=judged_output,
            model="gpt-4o-mini",
            sleep_time=1.0,
            max_retries=3
        )
        
        # åˆ†æç»“æœ
        analysis_result = analyze_gpt_judgment_results(judged_output, direction)
        all_analysis_results.append(analysis_result)
    
    # ä¿å­˜æ±‡æ€»åˆ†æç»“æœ
    summary_path = os.path.join(output_dir, "comparison_summary.csv")
    summary_df = pd.DataFrame(all_analysis_results)
    summary_df.to_csv(summary_path, index=False)
    print(f"\nğŸ“Š Summary analysis saved to: {summary_path}")
    
    # æ‰“å°æ±‡æ€»ç»“æœ
    print(f"\n{'='*60}")
    print(" OVERALL SUMMARY")
    print(f"{'='*60}")
    for result in all_analysis_results:
        print(f"{result['direction']}: RPS win rate = {result['rps_win_rate']:.3f} "
              f"({result['rps_wins']}/{result['baseline_wins'] + result['rps_wins']})")
    
    print(f"\nğŸ‰ All comparisons completed!")

if __name__ == "__main__":
    main()