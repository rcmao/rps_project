# -*- coding: utf-8 -*-
"""generate_response_v10.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14eo_Bbiacc5s8rz-OQWyqXPIESD7Yqig
"""

# 🚀 Step 0: 防止 Colab 空闲断线
from IPython.display import Javascript
Javascript('''
function ClickConnect(){
  console.log("🔄 保持连接中 - ClickConnect 被调用了");
  document.querySelector("colab-connect-button").click();
}
setInterval(ClickConnect, 60000);
''')

# ✅ Step 1: 挂载 Google Drive
from google.colab import drive
drive.mount('/content/drive')

# ✅ Step 2: 安装依赖
!pip install -q transformers datasets accelerate

# ✅ 先设置你想跑的 v index
v_indices = [6]

from google.colab import drive
drive.mount('/content/drive')

import os
import numpy as np
import pandas as pd
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM
from tqdm.auto import tqdm
import torch
import time
import random

# ✅ 设置设备
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# 设置随机种子
seed = 42
np.random.seed(seed)
random.seed(seed)
torch.manual_seed(seed)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(seed)

# 输出目录
result_dir = "/content/drive/MyDrive/llm_pre_project/dpa_outputs_v8"
os.makedirs(result_dir, exist_ok=True)

# 加载数据集
print("📦 Loading prompts from UltraFeedback...")
ds = load_dataset("HuggingFaceH4/ultrafeedback_binarized", split="test_prefs")
prompts = ds["prompt"][:2000]

# ✅ 加载模型（统一放到 CUDA 上）
print("🤖 Loading DPA-v1 model...")
model = AutoModelForCausalLM.from_pretrained(
    "Haoxiang-Wang/DPA-v1-Mistral-7B",
    torch_dtype=torch.float16
).to(device)

tokenizer = AutoTokenizer.from_pretrained("Haoxiang-Wang/RewardModel-Mistral-7B-for-DPA-v1")
tokenizer.padding_side = "left"
if tokenizer.pad_token_id is None:
    tokenizer.pad_token = tokenizer.eos_token

# 构造 Prompt
def build_input(prompt, v1, v2):
    h = int(np.round(v1 * 100))
    v = int(np.round(v2 * 100))
    sys_instruction = f"You are a helpful assistant. Your response should maximize weighted rating = helpfulness*{h} + verbosity*{v}."
    return [{"role": "user", "content": f"{sys_instruction}\n\n{prompt}"}]

# ✅ 修复后的 batch 生成函数
def generate_response_batch(prompts_batch, prompt_ids_batch, v1, v2):
    input_ids_list = []
    for prompt in prompts_batch:
        messages = build_input(prompt, v1, v2)
        input_ids = tokenizer.apply_chat_template(
            messages, add_generation_prompt=True, return_tensors="pt"
        )[0].to(device)
        input_ids_list.append(input_ids)

    input_ids_padded = torch.nn.utils.rnn.pad_sequence(
        input_ids_list, batch_first=True, padding_value=tokenizer.pad_token_id
    ).to(device)

    attention_mask = (input_ids_padded != tokenizer.pad_token_id).to(device)

    with torch.no_grad():
        outputs = model.generate(
            input_ids=input_ids_padded,
            attention_mask=attention_mask,
            max_new_tokens=2048,
            temperature=0.7,
            do_sample=True,
            num_return_sequences=3,
            pad_token_id=tokenizer.eos_token_id
        )

    responses = []
    for i in range(len(prompts_batch)):
        prompt_responses = []
        for j in range(3):
            idx = i * 3 + j
            generated_tokens = outputs[idx][input_ids_padded.shape[1]:]
            decoded = tokenizer.decode(generated_tokens, skip_special_tokens=True)
            prompt_responses.append(decoded)
        responses.append({
            "prompt_id": prompt_ids_batch[i],
            "prompt": prompts_batch[i],
            "responses": prompt_responses
        })
    return responses

# 主循环：断点续跑 + 增量保存
angles = np.linspace(0, np.pi / 4, 10)
batch_size = 8

for v_index in v_indices:
    v1, v2 = np.cos(angles[v_index - 1]), np.sin(angles[v_index - 1])
    output_file = os.path.join(result_dir, f"batch_v{v_index}.csv")
    print(f"\n🧭 Running v{v_index}: v = ({v1:.4f}, {v2:.4f})")

    if os.path.exists(output_file):
        existing_df = pd.read_csv(output_file)
        done_prompt_ids = set(existing_df["prompt_id"].unique())
        results = existing_df.to_dict("records")
        print(f"🔁 Loaded {len(done_prompt_ids)} prompts from previous run.")
    else:
        done_prompt_ids = set()
        results = []

    start_time = time.time()

    for start in tqdm(range(0, len(prompts), batch_size), desc=f"Generating batch_v{v_index}"):
        end = min(start + batch_size, len(prompts))
        prompt_batch = prompts[start:end]
        prompt_ids = list(range(start, end))

        if all(pid in done_prompt_ids for pid in prompt_ids):
            continue

        try:
            batch_results = generate_response_batch(prompt_batch, prompt_ids, v1, v2)
            new_rows = []
            for item in batch_results:
                for k, resp in enumerate(item["responses"]):
                    row = {
                        "prompt_id": item["prompt_id"],
                        "v1": float(v1),
                        "v2": float(v2),
                        "prompt": item["prompt"],
                        "response_id": k + 1,
                        "response": resp
                    }
                    results.append(row)
                    new_rows.append(row)

            pd.DataFrame(new_rows).to_csv(output_file, mode='a', header=not os.path.exists(output_file), index=False)
        except Exception as e:
            print(f"⚠️ Error at batch {start}-{end}: {e}")

    print(f"✅ Final saved {len(results)} responses to {output_file}")
    print(f"🕒 Time: {time.time() - start_time:.1f}s")

# ✅ 先设置你想跑的 v index
v_indices = [10,6]

from google.colab import drive
drive.mount('/content/drive')

import os
import numpy as np
import pandas as pd
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM
from tqdm.auto import tqdm
import torch
import time
import random

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# 设置随机种子
seed = 42
np.random.seed(seed)
random.seed(seed)
torch.manual_seed(seed)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(seed)

# 输出目录
result_dir = "/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9"
os.makedirs(result_dir, exist_ok=True)

# 加载数据集
print("📦 Loading prompts from UltraFeedback...")
ds = load_dataset("HuggingFaceH4/ultrafeedback_binarized", split="test_prefs")
prompts = ds["prompt"][:2000]

# 加载模型
print("🤖 Loading DPA-v1 model...")
model = AutoModelForCausalLM.from_pretrained(
    "Haoxiang-Wang/DPA-v1-Mistral-7B",
    torch_dtype=torch.float16
).to(device)

tokenizer = AutoTokenizer.from_pretrained("Haoxiang-Wang/RewardModel-Mistral-7B-for-DPA-v1")
tokenizer.padding_side = "left"
if tokenizer.pad_token_id is None:
    tokenizer.pad_token = tokenizer.eos_token

# 构造 Prompt
def build_input(prompt, v1, v2):
    h = int(np.round(v1 * 100))
    v = int(np.round(v2 * 100))
    sys_instruction = f"You are a helpful assistant. Your response should maximize weighted rating = helpfulness*{h} + verbosity*{v}."
    return [{"role": "user", "content": f"{sys_instruction}\n\n{prompt}"}]

# ✅ 支持动态 max_new_tokens
def generate_response_batch(prompts_batch, prompt_ids_batch, v1, v2):
    input_ids_list = []
    individual_input_ids = []

    for prompt in prompts_batch:
        messages = build_input(prompt, v1, v2)
        input_ids = tokenizer.apply_chat_template(
            messages, add_generation_prompt=True, return_tensors="pt"
        )[0]
        individual_input_ids.append(input_ids)
        input_ids_list.append(input_ids.to(device))

    input_ids_padded = torch.nn.utils.rnn.pad_sequence(
        input_ids_list, batch_first=True, padding_value=tokenizer.pad_token_id
    )
    input_ids_padded = input_ids_padded.to(device)

    attention_mask = (input_ids_padded != tokenizer.pad_token_id).to(device)

    max_input_len = input_ids_padded.shape[1]
    dynamic_max_new_tokens = min(2048, 4096 - max_input_len)

    with torch.no_grad():
        outputs = model.generate(
            input_ids=input_ids_padded,
            attention_mask=attention_mask,
            max_new_tokens=dynamic_max_new_tokens,
            temperature=0.7,
            do_sample=True,
            num_return_sequences=3,
            pad_token_id=tokenizer.eos_token_id
        )

    responses = []
    for i in range(len(prompts_batch)):
        prompt_responses = []
        for j in range(3):
            idx = i * 3 + j
            generated_tokens = outputs[idx][input_ids_padded.shape[1]:]
            decoded = tokenizer.decode(generated_tokens, skip_special_tokens=True)
            prompt_responses.append(decoded)
        responses.append({
            "prompt_id": prompt_ids_batch[i],
            "prompt": prompts_batch[i],
            "responses": prompt_responses
        })
    return responses

# 主循环：断点续跑 + 增量保存
angles = np.linspace(0, np.pi / 4, 10)
batch_size = 8

for v_index in v_indices:
    v1, v2 = np.cos(angles[v_index - 1]), np.sin(angles[v_index - 1])
    output_file = os.path.join(result_dir, f"batch_v{v_index}.csv")
    print(f"\n🧭 Running v{v_index}: v = ({v1:.4f}, {v2:.4f})")

    if os.path.exists(output_file):
        existing_df = pd.read_csv(output_file)
        done_prompt_ids = set(existing_df["prompt_id"].unique())
        results = existing_df.to_dict("records")
        print(f"🔁 Loaded {len(done_prompt_ids)} prompts from previous run.")
    else:
        done_prompt_ids = set()
        results = []

    start_time = time.time()

    for start in tqdm(range(0, len(prompts), batch_size), desc=f"Generating batch_v{v_index}"):
        end = min(start + batch_size, len(prompts))
        prompt_batch = prompts[start:end]
        prompt_ids = list(range(start, end))

        if all(pid in done_prompt_ids for pid in prompt_ids):
            continue

        try:
            batch_results = generate_response_batch(prompt_batch, prompt_ids, v1, v2)
            new_rows = []
            for item in batch_results:
                for k, resp in enumerate(item["responses"]):
                    row = {
                        "prompt_id": item["prompt_id"],
                        "v1": float(v1),
                        "v2": float(v2),
                        "prompt": item["prompt"],
                        "response_id": k + 1,
                        "response": resp
                    }
                    results.append(row)
                    new_rows.append(row)

            pd.DataFrame(new_rows).to_csv(output_file, mode='a', header=not os.path.exists(output_file), index=False)
        except Exception as e:
            print(f"⚠️ Error at batch {start}-{end}: {e}")

    print(f"✅ Final saved {len(results)} responses to {output_file}")
    print(f"🕒 Time: {time.time() - start_time:.1f}s")

import os
import pandas as pd
import numpy as np
import torch
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForCausalLM

# === 模型配置 ===
device = "cuda" if torch.cuda.is_available() else "cpu"

model = AutoModelForCausalLM.from_pretrained(
    "Haoxiang-Wang/DPA-v1-Mistral-7B",
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
    device_map="auto" if torch.cuda.is_available() else None
).to(device)

tokenizer = AutoTokenizer.from_pretrained("Haoxiang-Wang/DPA-v1-Mistral-7B")
tokenizer.padding_side = "left"
if tokenizer.pad_token_id is None:
    tokenizer.pad_token = tokenizer.eos_token

# === Prompt 构造函数 ===
def build_input(prompt, v1, v2):
    h = int(np.round(v1 * 100))
    v = int(np.round(v2 * 100))
    sys_instruction = f"You are a helpful assistant. Your response should maximize weighted rating = helpfulness*{h} + verbosity*{v}."
    return [{"role": "system", "content": sys_instruction}, {"role": "user", "content": prompt}]

# === 生成响应 ===
def regenerate_response(prompt, v1, v2):
    try:
        messages = build_input(prompt, v1, v2)
        input_ids = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors="pt").to(device)

        if input_ids.ndim != 2 or input_ids.shape[1] == 0:
            print(f"⚠️ Invalid input_ids shape {input_ids.shape}, skipping: {prompt[:60]}")
            return ""

        max_input_len = input_ids.shape[1]
        max_new_tokens = min(2048, 4096 - max_input_len)

        outputs = model.generate(
            input_ids=input_ids,
            max_new_tokens=max_new_tokens,
            temperature=0.7,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
        generated = outputs[0][input_ids.shape[1]:]
        return tokenizer.decode(generated, skip_special_tokens=True).strip()

    except Exception as e:
        print(f"❌ Error: {e} \nPrompt: {prompt[:60]}")
        return ""

# === 批量修复 CSV ===
def fix_csv_responses(csv_path, save_dir):
    print(f"\n📂 检查：{csv_path}")
    df = pd.read_csv(csv_path)
    blank_mask = df["response"].isna() | (df["response"].str.strip() == "")
    num_blank = blank_mask.sum()

    if num_blank == 0:
        print("✅ 无空白 response，跳过。")
        return

    print(f"🔧 发现空白 response 数量：{num_blank}")
    for i in tqdm(df[blank_mask].index, desc="修复中"):
        row = df.loc[i]
        new_response = regenerate_response(row["prompt"], row["v1"], row["v2"])
        df.at[i, "response"] = new_response

    # 输出路径
    os.makedirs(save_dir, exist_ok=True)
    file_name = os.path.basename(csv_path).replace(".csv", "_fixed.csv")
    save_path = os.path.join(save_dir, file_name)
    df.to_csv(save_path, index=False)
    print(f"✅ 修复完成 → 保存到: {save_path}")

# === 主逻辑：遍历所有 csv 文件 ===
input_dir = "/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9"
output_dir = os.path.join(input_dir, "fixed_batch")

for file in sorted(os.listdir(input_dir)):
    if file.endswith(".csv") and file.startswith("batch_v"):
        fix_csv_responses(os.path.join(input_dir, file), output_dir)

#下面是只对v7做下测试

import os
import pandas as pd
import numpy as np
import torch
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForCausalLM
from accelerate import Accelerator

# === 模型配置 ===
device = "cuda" if torch.cuda.is_available() else "cpu"
accelerator = Accelerator()

model = AutoModelForCausalLM.from_pretrained(
    "Haoxiang-Wang/DPA-v1-Mistral-7B",
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
    device_map="auto" if torch.cuda.is_available() else None
)

# 使用 accelerator 来处理设备映射
model = accelerator.prepare(model)

tokenizer = AutoTokenizer.from_pretrained("Haoxiang-Wang/DPA-v1-Mistral-7B")
tokenizer.padding_side = "left"
if tokenizer.pad_token_id is None:
    tokenizer.pad_token = tokenizer.eos_token

# === 判断 response 是否需要重生成 ===
def is_invalid_response(resp):
    if pd.isna(resp):
        return True
    resp = str(resp).strip()
    if not resp:
        return True

    invalid_keywords = [
        "[A:]", "[/SOLUTION]", "[ASS]", "[A]:", "[/CONV]", "[/CODE]", "[/USER]",
        "[OUTPUT]", "[CC]", "[OUT]", "[/DATA]", "[/ASST]", "[/ME]", "[/REST]",
        "[RESP]", "<[user]>", "<|user|>", "[Response]", "[Assistant]", "[Answer]",
        "[End of Response]", "<human>", "<|endoftext|>", "<<", "[INST]", "[/INST]",
        "[]", "[USER]"
    ]

    if resp in invalid_keywords:
        return True

    tokens = resp.split()
    if all(token.strip() in invalid_keywords for token in tokens):
        return True

    tag_like_count = sum(1 for t in tokens if (t.startswith("[") and t.endswith("]")) or (t.startswith("<") and t.endswith(">")))
    if len(tokens) > 0 and tag_like_count / len(tokens) > 0.8:
        return True

    return False

# === 构造 Prompt ===
def build_input(prompt, v1, v2):
    h = int(np.round(v1 * 100))
    v = int(np.round(v2 * 100))
    sys_instruction = f"You are a helpful assistant. Your response should maximize weighted rating = helpfulness*{h} + verbosity*{v}."
    return [{"role": "system", "content": sys_instruction}, {"role": "user", "content": prompt}]

# === 重生成响应 ===
def regenerate_response(prompt, v1, v2):
    try:
        messages = build_input(prompt, v1, v2)
        input_ids = tokenizer.apply_chat_template(
            messages, add_generation_prompt=True, return_tensors="pt"
        ).to(device)

        if input_ids.ndim != 2 or input_ids.shape[1] == 0:
            print(f"⚠️ Invalid input_ids shape {input_ids.shape}, skipping: {prompt[:60]}")
            return ""

        outputs = model.generate(
            input_ids=input_ids,
            temperature=0.7,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
        generated = outputs[0][input_ids.shape[1]:]
        return tokenizer.decode(generated, skip_special_tokens=True).strip()

    except Exception as e:
        print(f"❌ Error: {e} \nPrompt: {prompt[:60]}")
        return ""

# === 修复单个文件 ===
def fix_csv_responses(csv_path, save_dir):
    print(f"\n📂 检查：{csv_path}")

    if not os.path.exists(csv_path):
        print(f"❌ 文件不存在: {csv_path}")
        return

    df = pd.read_csv(csv_path)
    bad_mask = df["response"].apply(is_invalid_response)
    num_bad = bad_mask.sum()

    if num_bad == 0:
        print("✅ 无需修复，跳过。")
        return

    print(f"🔧 检测到无效 response 数量：{num_bad}")
    for i in tqdm(bad_mask[bad_mask].index, total=num_bad, desc=os.path.basename(csv_path), dynamic_ncols=True):
        row = df.loc[i]
        new_response = regenerate_response(row["prompt"], row["v1"], row["v2"])
        df.at[i, "response"] = new_response

    os.makedirs(save_dir, exist_ok=True)
    file_name = os.path.basename(csv_path).replace(".csv", "_fixed.csv")
    save_path = os.path.join(save_dir, file_name)
    df.to_csv(save_path, index=False)
    print(f"✅ 修复完成 → 保存到: {save_path}")

# === 主逻辑 ===
input_dir = "drive/MyDrive/llm_pre_project/dpa_outputs_v9"
output_dir = os.path.join(input_dir, "fixed_batch")

# 只修复 batch_v4_2.csv
input_file = "batch_v7_2.csv"

# 修复该文件
fix_csv_responses(os.path.join(input_dir, input_file), output_dir)



# ✅ 设置 v_index 为11，表示 v11 = (0.95, -0.31)
v_indices = [11]

from google.colab import drive
drive.mount('/content/drive')

import os
import numpy as np
import pandas as pd
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM
from tqdm.auto import tqdm
import torch
import time
import random

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# 设置随机种子
seed = 42
np.random.seed(seed)
random.seed(seed)
torch.manual_seed(seed)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(seed)

# 输出目录
result_dir = "/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9"
os.makedirs(result_dir, exist_ok=True)

# 加载数据集
print("📦 Loading prompts from UltraFeedback...")
ds = load_dataset("HuggingFaceH4/ultrafeedback_binarized", split="test_prefs")
prompts = ds["prompt"][:2000]

# 加载模型
print("🤖 Loading DPA-v1 model...")
model = AutoModelForCausalLM.from_pretrained(
    "Haoxiang-Wang/DPA-v1-Mistral-7B",
    torch_dtype=torch.float16
).to(device)

tokenizer = AutoTokenizer.from_pretrained("Haoxiang-Wang/RewardModel-Mistral-7B-for-DPA-v1")
tokenizer.padding_side = "left"
if tokenizer.pad_token_id is None:
    tokenizer.pad_token = tokenizer.eos_token

# 构造 Prompt
def build_input(prompt, v1, v2):
    h = int(np.round(v1 * 100))
    v = int(np.round(v2 * 100))
    sys_instruction = f"You are a helpful assistant. Your response should maximize weighted rating = helpfulness*{h} + verbosity*{v}."
    return [{"role": "user", "content": f"{sys_instruction}\n\n{prompt}"}]

# ✅ 支持动态 max_new_tokens
def generate_response_batch(prompts_batch, prompt_ids_batch, v1, v2):
    input_ids_list = []
    individual_input_ids = []

    for prompt in prompts_batch:
        messages = build_input(prompt, v1, v2)
        input_ids = tokenizer.apply_chat_template(
            messages, add_generation_prompt=True, return_tensors="pt"
        )[0]
        individual_input_ids.append(input_ids)
        input_ids_list.append(input_ids.to(device))

    input_ids_padded = torch.nn.utils.rnn.pad_sequence(
        input_ids_list, batch_first=True, padding_value=tokenizer.pad_token_id
    )
    input_ids_padded = input_ids_padded.to(device)

    attention_mask = (input_ids_padded != tokenizer.pad_token_id).to(device)

    max_input_len = input_ids_padded.shape[1]
    dynamic_max_new_tokens = min(2048, 4096 - max_input_len)

    with torch.no_grad():
        outputs = model.generate(
            input_ids=input_ids_padded,
            attention_mask=attention_mask,
            max_new_tokens=dynamic_max_new_tokens,
            temperature=0.7,
            do_sample=True,
            num_return_sequences=3,
            pad_token_id=tokenizer.eos_token_id
        )

    responses = []
    for i in range(len(prompts_batch)):
        prompt_responses = []
        for j in range(3):
            idx = i * 3 + j
            generated_tokens = outputs[idx][input_ids_padded.shape[1]:]
            decoded = tokenizer.decode(generated_tokens, skip_special_tokens=True)
            prompt_responses.append(decoded)
        responses.append({
            "prompt_id": prompt_ids_batch[i],
            "prompt": prompts_batch[i],
            "responses": prompt_responses
        })
    return responses

# 主循环：断点续跑 + 增量保存
angles = np.linspace(0, np.pi / 4, 10)
batch_size = 8

# 只跑v11 = (0.95, -0.31)
for v_index in v_indices:
    v1, v2 = 0.95, -0.31  # 直接设置v11
    output_file = os.path.join(result_dir, f"batch_v{v_index}.csv")
    print(f"\n🧭 Running v{v_index}: v = ({v1:.4f}, {v2:.4f})")

    if os.path.exists(output_file):
        existing_df = pd.read_csv(output_file)
        done_prompt_ids = set(existing_df["prompt_id"].unique())
        results = existing_df.to_dict("records")
        print(f"🔁 Loaded {len(done_prompt_ids)} prompts from previous run.")
    else:
        done_prompt_ids = set()
        results = []

    start_time = time.time()

    for start in tqdm(range(0, len(prompts), batch_size), desc=f"Generating batch_v{v_index}"):
        end = min(start + batch_size, len(prompts))
        prompt_batch = prompts[start:end]
        prompt_ids = list(range(start, end))

        if all(pid in done_prompt_ids for pid in prompt_ids):
            continue

        try:
            batch_results = generate_response_batch(prompt_batch, prompt_ids, v1, v2)
            new_rows = []
            for item in batch_results:
                for k, resp in enumerate(item["responses"]):
                    row = {
                        "prompt_id": item["prompt_id"],
                        "v1": float(v1),
                        "v2": float(v2),
                        "prompt": item["prompt"],
                        "response_id": k + 1,
                        "response": resp
                    }
                    results.append(row)
                    new_rows.append(row)

            pd.DataFrame(new_rows).to_csv(output_file, mode='a', header=not os.path.exists(output_file), index=False)
        except Exception as e:
            print(f"⚠️ Error at batch {start}-{end}: {e}")

    print(f"✅ Final saved {len(results)} responses to {output_file}")
    print(f"🕒 Time: {time.time() - start_time:.1f}s")

#v11重新跑的版本

from google.colab import drive
drive.mount('/content/drive')

from google.colab import drive
drive.mount('/content/drive')

import os
import numpy as np
import pandas as pd
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM
from tqdm.auto import tqdm
import torch
import time
import random

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# 设置随机种子
seed = 42
np.random.seed(seed)
random.seed(seed)
torch.manual_seed(seed)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(seed)

# 输出目录
result_dir = "/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9"
os.makedirs(result_dir, exist_ok=True)

# 加载数据集
print("📦 Loading prompts from UltraFeedback...")
ds = load_dataset("HuggingFaceH4/ultrafeedback_binarized", split="test_prefs")
prompts = ds["prompt"][:2000]

# torch.cuda.empty_cache()
# torch.cuda.ipc_collect()

# 加载模型
print("🤖 Loading DPA-v1 model...")
model = AutoModelForCausalLM.from_pretrained(
    "Haoxiang-Wang/DPA-v1-Mistral-7B",
    torch_dtype=torch.float16,
    device_map="auto"
).to(device)

tokenizer = AutoTokenizer.from_pretrained("Haoxiang-Wang/RewardModel-Mistral-7B-for-DPA-v1")
tokenizer.padding_side = "left"
if tokenizer.pad_token_id is None:
    tokenizer.pad_token = tokenizer.eos_token

# 构造 Prompt
def build_input(pro mpt, v1, v2):
    h = int(np.round(v1 * 100))
    v = int(np.round(v2 * 100))
    sys_instruction = f"You are a helpful assistant. Your response should maximize weighted rating = helpfulness*{h} + verbosity*{v}."
    return [{"role": "user", "content": f"{sys_instruction}\n\n{prompt}"}]

# ✅ 支持动态 max_new_tokens
def generate_response_batch(prompts_batch, prompt_ids_batch, v1, v2):
    input_ids_list = []
    individual_input_ids = []

    for prompt in prompts_batch:
        messages = build_input(prompt, v1, v2)
        input_ids = tokenizer.apply_chat_template(
            messages, add_generation_prompt=True, return_tensors="pt"
        )[0]
        individual_input_ids.append(input_ids)
        input_ids_list.append(input_ids.to(device))

    input_ids_padded = torch.nn.utils.rnn.pad_sequence(
        input_ids_list, batch_first=True, padding_value=tokenizer.pad_token_id
    )
    input_ids_padded = input_ids_padded.to(device)

    attention_mask = (input_ids_padded != tokenizer.pad_token_id).to(device)

    max_input_len = input_ids_padded.shape[1]
    dynamic_max_new_tokens = min(2048, 4096 - max_input_len)

    with torch.no_grad():
        outputs = model.generate(
            input_ids=input_ids_padded,
            attention_mask=attention_mask,
            max_new_tokens=dynamic_max_new_tokens,
            temperature=0.7,
            do_sample=True,
            num_return_sequences=3,
            pad_token_id=tokenizer.eos_token_id
        )

    responses = []
    for i in range(len(prompts_batch)):
        prompt_responses = []
        for j in range(3):
            idx = i * 3 + j
            generated_tokens = outputs[idx][input_ids_padded.shape[1]:]
            decoded = tokenizer.decode(generated_tokens, skip_special_tokens=True)
            prompt_responses.append(decoded)
        responses.append({
            "prompt_id": prompt_ids_batch[i],
            "prompt": prompts_batch[i],
            "responses": prompt_responses
        })
    return responses

# 主循环：断点续跑 + 增量保存
angles = np.linspace(0, np.pi / 4, 10)
batch_size = 8
v_indices = [11]  # 只跑 v11
v1, v2 = 0.95, -0.31

for v_index in v_indices:
    output_file = os.path.join(result_dir, f"batch_v{v_index}.csv")
    print(f"\n🧭 Running v{v_index}: v = ({v1:.4f}, {v2:.4f})")

    if os.path.exists(output_file):
        existing_df = pd.read_csv(output_file)
        done_prompt_ids = set(existing_df["prompt_id"].unique())
        results = existing_df.to_dict("records")
        print(f"🔁 Loaded {len(done_prompt_ids)} prompts from previous run.")
    else:
        done_prompt_ids = set()
        results = []

    start_time = time.time()

    for start in tqdm(range(0, len(prompts), batch_size), desc=f"Generating batch_v{v_index}"):
        end = min(start + batch_size, len(prompts))
        prompt_batch_all = prompts[start:end]
        prompt_ids_all = list(range(start, end))

        # 🔧 修改点：只处理未完成的 prompts
        unprocessed_indices = [i for i, pid in enumerate(prompt_ids_all) if pid not in done_prompt_ids]
        if not unprocessed_indices:
            continue

        prompt_batch = [prompt_batch_all[i] for i in unprocessed_indices]
        prompt_ids = [prompt_ids_all[i] for i in unprocessed_indices]

        try:
            batch_results = generate_response_batch(prompt_batch, prompt_ids, v1, v2)
            new_rows = []
            for item in batch_results:
                for k, resp in enumerate(item["responses"]):
                    row = {
                        "prompt_id": item["prompt_id"],
                        "v1": float(v1),
                        "v2": float(v2),
                        "prompt": item["prompt"],
                        "response_id": k + 1,
                        "response": resp
                    }
                    results.append(row)
                    new_rows.append(row)

            pd.DataFrame(new_rows).to_csv(output_file, mode='a', header=not os.path.exists(output_file), index=False)
        except Exception as e:
            print(f"⚠️ Error at batch {start}-{end}: {e}")

    print(f"✅ Final saved {len(results)} responses to {output_file}")
    print(f"🕒 Time: {time.time() - start_time:.1f}s")

import os
import pandas as pd
import numpy as np
import torch
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForCausalLM
from accelerate import Accelerator

# === 模型配置 ===
device = "cuda" if torch.cuda.is_available() else "cpu"
accelerator = Accelerator()

model = AutoModelForCausalLM.from_pretrained(
    "Haoxiang-Wang/DPA-v1-Mistral-7B",
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
    device_map="auto" if torch.cuda.is_available() else None
)

# 使用 accelerator 来处理设备映射
model = accelerator.prepare(model)

tokenizer = AutoTokenizer.from_pretrained("Haoxiang-Wang/DPA-v1-Mistral-7B")
tokenizer.padding_side = "left"
if tokenizer.pad_token_id is None:
    tokenizer.pad_token = tokenizer.eos_token

# === 判断 response 是否需要重生成 ===
def is_invalid_response(resp):
    if pd.isna(resp):
        return True
    resp = str(resp).strip()
    if not resp:
        return True

    invalid_keywords = [
        "[A:]", "[/SOLUTION]", "[ASS]", "[A]:", "[/CONV]", "[/CODE]", "[/USER]",
        "[OUTPUT]", "[CC]", "[OUT]", "[/DATA]", "[/ASST]", "[/ME]", "[/REST]",
        "[RESP]", "<[user]>", "<|user|>", "[Response]", "[Assistant]", "[Answer]",
        "[End of Response]", "<human>", "<|endoftext|>", "<<", "[INST]", "[/INST]",
        "[]", "[USER]"
    ]

    if resp in invalid_keywords:
        return True

    tokens = resp.split()
    if all(token.strip() in invalid_keywords for token in tokens):
        return True

    tag_like_count = sum(1 for t in tokens if (t.startswith("[") and t.endswith("]")) or (t.startswith("<") and t.endswith(">")))
    if len(tokens) > 0 and tag_like_count / len(tokens) > 0.8:
        return True

    return False

# === 构造 Prompt ===
def build_input(prompt, v1, v2):
    h = int(np.round(v1 * 100))
    v = int(np.round(v2 * 100))
    sys_instruction = f"You are a helpful assistant. Your response should maximize weighted rating = helpfulness*{h} + verbosity*{v}."
    return [{"role": "system", "content": sys_instruction}, {"role": "user", "content": prompt}]

# === 重生成响应 ===
def regenerate_response(prompt, v1, v2):
    try:
        messages = build_input(prompt, v1, v2)
        input_ids = tokenizer.apply_chat_template(
            messages, add_generation_prompt=True, return_tensors="pt"
        ).to(device)

        if input_ids.ndim != 2 or input_ids.shape[1] == 0:
            print(f"⚠️ Invalid input_ids shape {input_ids.shape}, skipping: {prompt[:60]}")
            return ""

        outputs = model.generate(
            input_ids=input_ids,
            temperature=0.7,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
        generated = outputs[0][input_ids.shape[1]:]
        return tokenizer.decode(generated, skip_special_tokens=True).strip()

    except Exception as e:
        print(f"❌ Error: {e} \nPrompt: {prompt[:60]}")
        return ""

# === 修复单个文件 ===
def fix_csv_responses(csv_path, save_dir):
    print(f"\n📂 检查：{csv_path}")

    if not os.path.exists(csv_path):
        print(f"❌ 文件不存在: {csv_path}")
        return

    df = pd.read_csv(csv_path)
    bad_mask = df["response"].apply(is_invalid_response)
    num_bad = bad_mask.sum()

    if num_bad == 0:
        print("✅ 无需修复，跳过。")
        return

    print(f"🔧 检测到无效 response 数量：{num_bad}")
    for i in tqdm(bad_mask[bad_mask].index, total=num_bad, desc=os.path.basename(csv_path), dynamic_ncols=True):
        row = df.loc[i]
        new_response = regenerate_response(row["prompt"], row["v1"], row["v2"])
        df.at[i, "response"] = new_response

    os.makedirs(save_dir, exist_ok=True)
    file_name = os.path.basename(csv_path).replace(".csv", "_fixed.csv")
    save_path = os.path.join(save_dir, file_name)
    df.to_csv(save_path, index=False)
    print(f"✅ 修复完成 → 保存到: {save_path}")

# === 主逻辑 ===
input_dir = "drive/MyDrive/llm_pre_project/dpa_outputs_v9"
output_dir = os.path.join(input_dir, "fixed_batch")

# 只修复 batch_v4_2.csv
input_file = "batch_v11.csv"

# 修复该文件
fix_csv_responses(os.path.join(input_dir, input_file), output_dir)

#打分

# === 导入库 ===
import os
import time
import pandas as pd
import numpy as np
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
from tqdm.auto import tqdm
from google.colab import drive

# === 挂载 Google Drive ===
drive.mount('/content/drive')

# === 设置输入输出路径 ===
input_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/fixed_batch/batch_v11_fixed.csv"
output_dir = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch"
os.makedirs(output_dir, exist_ok=True)

# === 加载数据 ===
print(f"📦 Loading file: {input_path}")
df = pd.read_csv(input_path)

# === 加载 Reward Model ===
print("🤖 Loading Reward Model...")
device = "cuda" if torch.cuda.is_available() else "cpu"
rm = AutoModelForSequenceClassification.from_pretrained(
    "Haoxiang-Wang/RewardModel-Mistral-7B-for-DPA-v1", trust_remote_code=True
).to(device)
tokenizer = AutoTokenizer.from_pretrained("Haoxiang-Wang/RewardModel-Mistral-7B-for-DPA-v1")

def score_response(prompt, response):
    template = "[INST] You must read the following conversation carefully and rate the assistant's response from score 0-100 in these aspects: helpfulness, correctness, coherence, honesty, complexity, verbosity\n\nUser: {prompt}\n\nAssistant: {response} [/INST]"
    inputs = tokenizer(template.format(prompt=prompt, response=response), return_tensors="pt").to(device)
    with torch.no_grad():
        logits = rm(**inputs).logits.squeeze().cpu().numpy()
    return logits[9], logits[4]  # helpfulness, verbosity

# === 开始打分 ===
print("🧠 Scoring responses...")
start_time = time.time()
scores = []

for i, row in tqdm(df.iterrows(), total=len(df), desc="🔍 Scoring"):
    try:
        h, v = score_response(row["prompt"], row["response"])
        scores.append({"v1": row["v1"], "v2": row["v2"], "helpfulness": h, "verbosity": v})
    except Exception as e:
        print(f"⚠️ Error on row {i}: {e}")
        scores.append({"v1": row["v1"], "v2": row["v2"], "helpfulness": None, "verbosity": None})

df_scores = pd.DataFrame(scores)
df_all = pd.concat([df, df_scores], axis=1)

# === 保存结果 ===
output_file = os.path.join(output_dir, os.path.basename(input_path).replace(".csv", "_scored.csv"))
df_all.to_csv(output_file, index=False)

end_time = time.time()
print(f"\n✅ Scoring complete! Total scored: {len(df_all)}")
print(f"🕒 Time elapsed: {end_time - start_time:.1f} seconds")
print(f"📁 Saved to: {output_file}")

#加干扰

import pandas as pd
import numpy as np
import os
from tqdm import tqdm
from google.colab import drive

# === 挂载 Google Drive ===
drive.mount('/content/drive')

# === 设置输入输出路径 ===
input_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch/batch_v11_fixed_scored.csv"
output_folder = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/generate_perturbe"
os.makedirs(output_folder, exist_ok=True)

# === 加载数据 ===
data = pd.read_csv(input_path)

# === 定义旋转扰动函数（角度范围 ±30°）===
def apply_random_perturbation(v1, v2, max_angle_deg=45):
    theta = np.radians(np.random.uniform(-max_angle_deg, max_angle_deg))  # 角度范围 [-15°, 15°]
    cos_t, sin_t = np.cos(theta), np.sin(theta)
    # 旋转矩阵乘以向量 (v1, v2)
    new_v1 = v1 * cos_t - v2 * sin_t
    new_v2 = v1 * sin_t + v2 * cos_t
    # 单位向量归一化（可选）
    norm = np.sqrt(new_v1**2 + new_v2**2)
    return new_v1 / norm, new_v2 / norm

# === 生成扰动数据 ===
all_perturbed_data = []

for _, row in tqdm(data.iterrows(), total=len(data), desc="🔄 Generating Perturbations"):
    v1, v2 = row['v1'], row['v2']

    for _ in range(10):  # 生成10个扰动方向
        perturbed_v1, perturbed_v2 = apply_random_perturbation(v1, v2)

        perturbed_row = row.copy()
        perturbed_row['perturbed_v1'] = perturbed_v1
        perturbed_row['perturbed_v2'] = perturbed_v2
        all_perturbed_data.append(perturbed_row)

# === 保存结果 ===
perturbed_df = pd.DataFrame(all_perturbed_data)
output_path = os.path.join(output_folder, "batch_v11_30.csv")
perturbed_df.to_csv(output_path, index=False)

print(f"\n✅ 扰动处理完成，保存为：{output_path}")

#加干扰后打分

# === 安装依赖（如需要）===
!pip install -q pandas numpy

# === 导入库 ===
import pandas as pd
import numpy as np
import os
from google.colab import drive

# === 挂载 Google Drive ===
drive.mount('/content/drive')

# === 设置输入输出路径 ===
input_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/generate_perturbe/batch_v11_30.csv"
output_folder = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/generate_perturbe/pertube_score"
os.makedirs(output_folder, exist_ok=True)

# === 读取数据 ===
print(f"📄 正在读取数据文件: {input_path}")
perturbed_df = pd.read_csv(input_path)

# === 定义得分函数 ===
def compute_score(v1, v2, helpfulness, verbosity):
    return v1 * helpfulness + v2 * verbosity

# === 计算 baseline 和扰动得分 ===
perturbed_df['baseline_score'] = perturbed_df.apply(
    lambda row: compute_score(row['v1'], row['v2'], row['helpfulness'], row['verbosity']),
    axis=1
)

perturbed_df['perturbed_score'] = perturbed_df.apply(
    lambda row: compute_score(row['perturbed_v1'], row['perturbed_v2'], row['helpfulness'], row['verbosity']),
    axis=1
)

# === 保存结果 ===
output_path = os.path.join(output_folder, "pertube_score_v11_with_baseline_30.csv")
perturbed_df.to_csv(output_path, index=False)

print(f"\n✅ 打分完成，结果已保存到: {output_path}")

#选出basline和rps的best response

import os
import pandas as pd
import numpy as np
from google.colab import drive

# === 挂载 Google Drive ===
drive.mount('/content/drive')

# === 路径设置 ===
input_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/generate_perturbe/pertube_score/pertube_score_v11_with_baseline_30.csv"
intermediate_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/generate_perturbe/intermediate_with_min_score.csv"
output_folder = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/generate_perturbe/best_response"
os.makedirs(output_folder, exist_ok=True)
output_path = os.path.join(output_folder, "final_best_responses_with_baseline_and_perturbed_v11_30.csv")

# === 加载数据 ===
df = pd.read_csv(input_path)
print("✅ 已加载数据，列如下：")
print(df.columns)

# === 添加每组 response 的最小扰动得分 ===
df['min_perturbed_score'] = df.groupby(['prompt_id', 'response_id'])['perturbed_score'].transform('min')

# === 保存中间数据（可选调试用）===
df.to_csv(intermediate_path, index=False)

# === 重新读取中间文件，确保完整列保留 ===
df = pd.read_csv(intermediate_path)

# === 检查列 ===
if 'min_perturbed_score' not in df.columns:
    raise ValueError("⚠️ min_perturbed_score 列未成功创建！")
else:
    print("✅ min_perturbed_score 列存在")

# === 选择 baseline 最优响应（每个 prompt_id 下 baseline_score 最大）===
baseline_best_responses = df.loc[df.groupby('prompt_id')['baseline_score'].idxmax()].copy()

# === 选择扰动最稳健响应（min_perturbed_score 最大）===
perturbed_best_responses = df.loc[df.groupby('prompt_id')['min_perturbed_score'].idxmax()].copy()

# === 合并结果 ===
final_best_responses = baseline_best_responses[['prompt_id', 'v1', 'v2', 'prompt', 'response_id',
                                                'baseline_score']].copy()

final_best_responses['perturbed_score'] = perturbed_best_responses['min_perturbed_score'].values
final_best_responses['perturbed_best_response'] = perturbed_best_responses['response'].values
final_best_responses['baseline_best_response'] = baseline_best_responses['response'].values
final_best_responses['response_id_perturbed'] = perturbed_best_responses['response_id'].values
final_best_responses['perturbed_v1'] = perturbed_best_responses['perturbed_v1'].values
final_best_responses['perturbed_v2'] = perturbed_best_responses['perturbed_v2'].values

final_best_responses['best_baseline_response_id'] = baseline_best_responses['response_id'].values
final_best_responses['best_perturbed_response_id'] = perturbed_best_responses['response_id'].values

# === 保存最终结果 ===
final_best_responses.to_csv(output_path, index=False)
print(f"✅ 最终结果已保存至：{output_path}")

#check best_response.csv

import pandas as pd

# 路径
csv_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/generate_perturbe/best_response/final_best_responses_with_baseline_and_perturbed_v11_30.csv"

# 读取 CSV
df = pd.read_csv(csv_path)

# 查看列名
print("📋 列名如下：")
print(df.columns.tolist())

#改成gpt4o形式

import pandas as pd
import json
import os

# 输入路径
input_csv = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/generate_perturbe/best_response/final_best_responses_with_baseline_and_perturbed_v11_15.csv"
output_jsonl = input_csv.replace(".csv", "_gpt4o_eval.jsonl")

# 读取数据
df = pd.read_csv(input_csv)

# 构造 pairwise 格式
pairs = []
tie_count = 0

for _, row in df.iterrows():
    prompt = row['prompt']
    baseline_resp = row['baseline_best_response']
    rps_resp = row['perturbed_best_response']
    baseline_id = row['baseline_best_response_id']
    rps_id = row['perturbed_best_response_id']

    # 如果 response id 相同，直接标记为 Tie
    if baseline_id == rps_id:
        pairs.append({
            "prompt": prompt,
            "response_a": baseline_resp,
            "response_b": rps_resp,
            "auto_result": "Tie"
        })
        tie_count += 1
        continue

    # 否则生成 GPT 比较 prompt
    formatted_prompt = f"""For the following query to a chatbot, which response is more helpful?

Query: {prompt}


Response A: {baseline_resp}

Response B: {rps_resp}

FIRST provide a one-sentence comparison of the two responses and explain which you feel is more helpful.
SECOND, on a new line, state only 'A' or 'B' to indicate which response is more helpful.

Your response should use the format:

Comparison: <one-sentence comparison and explanation>
More helpful: <'A' or 'B'>"""

    pairs.append({
        "prompt": formatted_prompt,
        "auto_result": None
    })

# 保存为 JSONL 格式
with open(output_jsonl, 'w', encoding='utf-8') as f:
    for item in pairs:
        f.write(json.dumps(item, ensure_ascii=False) + "\n")

print(f"✅ 已生成 GPT 评估格式文件: {output_jsonl}")
print(f"🤝 已跳过 {tie_count} 条完全相同 response，标记为 Tie")

import openai
import os

# 设置 API Key 和代理地址（如果使用代理）
os.environ["OPENAI_API_KEY"] = "sk-XGGe5y0ZvLcQVFp6XnRizs7q47gsVnAbZx0Xr2mfcVlbr99f"
openai.api_key = os.environ["OPENAI_API_KEY"]
openai.api_base = "https://api2.aigcbest.top/v1"  # 如果用代理就改这里

print("✅ OpenAI 设置完成")

import os

output_path = "gpt4o_comparisons.jsonl"

if os.path.exists(output_path):
    os.remove(output_path)
    print("✅ 已删除旧的评估结果文件，准备重新开始评估")
else:
    print("ℹ️ 没有找到已有评估文件，直接开始评估")

import json
import time
import os

jsonl_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/generate_perturbe/best_response/final_best_responses_with_baseline_and_perturbed_v11_15_gpt4o_eval.jsonl"
output_path = "gpt4o_comparisons.jsonl"

# ✅ 加载输入数据
with open(jsonl_path, 'r', encoding='utf-8') as f:
    data = [json.loads(line) for line in f]

print(f"✅ 读取 {len(data)} 条 prompt")

# ✅ 加载已有结果（支持断点续跑）
seen_prompts = set()
results = []

if os.path.exists(output_path):
    with open(output_path, 'r', encoding='utf-8') as f:
        for line in f:
            item = json.loads(line)
            results.append(item)
            seen_prompts.add(item['prompt'])
    print(f"🔁 已加载 {len(seen_prompts)} 条已完成结果，继续评估未完成部分")

# ✅ GPT 评估函数
def gpt4o_judge(prompt):
    try:
        res = openai.ChatCompletion.create(
            model="gpt-4o-mini",  # 确保你用的是正确模型 ID
            messages=[{"role": "user", "content": prompt}],
            temperature=0.0,
            max_tokens=200
        )
        return res['choices'][0]['message']['content']
    except Exception as e:
        print("❌ GPT 调用失败：", e)  # ← 加这行！
        return f"[ERROR] {e}"


# ✅ 正式开始评估
error_count = 0
start_time = time.time()

with open(output_path, "a", encoding="utf-8") as f_out:
    for idx, item in enumerate(data):
        prompt = item["prompt"]

        # ⏩ 跳过已处理的样本
        if prompt in seen_prompts:
            continue

        # ✅ 判断是否为自动 Tie 样本
        if item.get("auto_result") == "Tie":
            result = {
                "prompt": prompt,
                "choice": "Tie",
                "reason": "Responses are identical, skipped GPT evaluation.",
                "response_raw": "More helpful: Tie"
            }
        else:
            reply = gpt4o_judge(prompt)
            lines = reply.strip().splitlines()
            explanation = ""
            choice = ""

            for line in lines:
                if line.lower().startswith("comparison:"):
                    explanation = line[len("comparison:"):].strip()
                elif line.lower().startswith("more helpful:"):
                    choice = line[len("more helpful:"):].strip().upper()

            if not choice:
                choice = "Unknown"

            result = {
                "prompt": prompt,
                "choice": choice,
                "reason": explanation,
                "response_raw": reply
            }

            if "[ERROR]" in reply:
                error_count += 1

        # 💾 写入结果
        f_out.write(json.dumps(result, ensure_ascii=False) + "\n")
        f_out.flush()
        results.append(result)

        # 🔄 打印进度
        i = len(results)
        if i % 10 == 0:
            print(f"✅ 已评估 {i}/{len(data)} 条")

        if i % 50 == 0:
            elapsed = time.time() - start_time
            speed = elapsed / i
            eta = (len(data) - i) * speed
            print(f"⏱️ 预计剩余时间：{eta/60:.1f} 分钟，平均每条 {speed:.2f}s")

# ✅ 结束信息
duration = time.time() - start_time
print(f"\n🏁 评估完成，总耗时 {duration/60:.1f} 分钟，结果保存至 {output_path}")
print(f"📉 错误条数：{error_count}")

import json
from collections import Counter

output_path = "gpt4o_comparisons.jsonl"

# 读取评估结果
results = []
with open(output_path, 'r', encoding='utf-8') as f:
    for line in f:
        results.append(json.loads(line))

# 统计结果分布
choices = [r.get("choice", "Unknown") for r in results]
counter = Counter(choices)

total = len(results)
a_win = counter["A"]
b_win = counter["B"]
tie = counter["Tie"]
unknown = counter["Unknown"]

# 打印统计结果
print(f"🔍 总共评估样本数：{total}")
print(f"🥇 A 胜出：{a_win} ({a_win/total:.1%})")
print(f"🥈 B 胜出：{b_win} ({b_win/total:.1%})")
print(f"🤝 Tie 平局：{tie} ({tie/total:.1%})")
print(f"❓ Unknown（格式错误等）：{unknown} ({unknown/total:.1%})")

import pandas as pd

df = pd.DataFrame(results)
df.to_csv("gpt4o_eval_result.csv", index=False)
print("✅ 已导出为 gpt4o_eval_result.csv")

#交换下位置

import pandas as pd
import json
import os

# 输入路径
input_csv = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/generate_perturbe/best_response/final_best_responses_with_baseline_and_perturbed_v11_15.csv"
output_jsonl = input_csv.replace(".csv", "_gpt4o_eval_swap.jsonl")

# 读取数据
df = pd.read_csv(input_csv)

# 构造 pairwise 格式
pairs = []
tie_count = 0

for _, row in df.iterrows():
    prompt = row['prompt']
    baseline_resp = row['baseline_best_response']
    rps_resp = row['perturbed_best_response']
    baseline_id = row['baseline_best_response_id']
    rps_id = row['perturbed_best_response_id']

    # 如果 response id 相同，直接标记为 Tie
    if baseline_id == rps_id:
        pairs.append({
            "prompt": prompt,
            "response_a": baseline_resp,
            "response_b": rps_resp,
            "auto_result": "Tie"
        })
        tie_count += 1
        continue

    # 否则生成 GPT 比较 prompt
    formatted_prompt = f"""For the following query to a chatbot, which response is more helpful?

Query: {prompt}

Response A: {rps_resp}

Response B: {baseline_resp}


FIRST provide a one-sentence comparison of the two responses and explain which you feel is more helpful.
SECOND, on a new line, state only 'A' or 'B' to indicate which response is more helpful.

Your response should use the format:

Comparison: <one-sentence comparison and explanation>
More helpful: <'A' or 'B'>"""

    pairs.append({
        "prompt": formatted_prompt,
        "auto_result": None
    })

# 保存为 JSONL 格式
with open(output_jsonl, 'w', encoding='utf-8') as f:
    for item in pairs:
        f.write(json.dumps(item, ensure_ascii=False) + "\n")

print(f"✅ 已生成 GPT 评估格式文件: {output_jsonl}")
print(f"🤝 已跳过 {tie_count} 条完全相同 response，标记为 Tie")

import json
import time
import os

jsonl_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/generate_perturbe/best_response/final_best_responses_with_baseline_and_perturbed_v11_15_gpt4o_eval_swap.jsonl"
output_path = "gpt4o_comparisons_swap.jsonl"

# ✅ 加载输入数据
with open(jsonl_path, 'r', encoding='utf-8') as f:
    data = [json.loads(line) for line in f]

print(f"✅ 读取 {len(data)} 条 prompt")

# ✅ 加载已有结果（支持断点续跑）
seen_prompts = set()
results = []

if os.path.exists(output_path):
    with open(output_path, 'r', encoding='utf-8') as f:
        for line in f:
            item = json.loads(line)
            results.append(item)
            seen_prompts.add(item['prompt'])
    print(f"🔁 已加载 {len(seen_prompts)} 条已完成结果，继续评估未完成部分")

# ✅ GPT 评估函数
def gpt4o_judge(prompt):
    try:
        res = openai.ChatCompletion.create(
            model="gpt-4o-mini",  # 确保你用的是正确模型 ID
            messages=[{"role": "user", "content": prompt}],
            temperature=0.0,
            max_tokens=200
        )
        return res['choices'][0]['message']['content']
    except Exception as e:
        print("❌ GPT 调用失败：", e)  # ← 加这行！
        return f"[ERROR] {e}"


# ✅ 正式开始评估
error_count = 0
start_time = time.time()

with open(output_path, "a", encoding="utf-8") as f_out:
    for idx, item in enumerate(data):
        prompt = item["prompt"]

        # ⏩ 跳过已处理的样本
        if prompt in seen_prompts:
            continue

        # ✅ 判断是否为自动 Tie 样本
        if item.get("auto_result") == "Tie":
            result = {
                "prompt": prompt,
                "choice": "Tie",
                "reason": "Responses are identical, skipped GPT evaluation.",
                "response_raw": "More helpful: Tie"
            }
        else:
            reply = gpt4o_judge(prompt)
            lines = reply.strip().splitlines()
            explanation = ""
            choice = ""

            for line in lines:
                if line.lower().startswith("comparison:"):
                    explanation = line[len("comparison:"):].strip()
                elif line.lower().startswith("more helpful:"):
                    choice = line[len("more helpful:"):].strip().upper()

            if not choice:
                choice = "Unknown"

            result = {
                "prompt": prompt,
                "choice": choice,
                "reason": explanation,
                "response_raw": reply
            }

            if "[ERROR]" in reply:
                error_count += 1

        # 💾 写入结果
        f_out.write(json.dumps(result, ensure_ascii=False) + "\n")
        f_out.flush()
        results.append(result)

        # 🔄 打印进度
        i = len(results)
        if i % 10 == 0:
            print(f"✅ 已评估 {i}/{len(data)} 条")

        if i % 50 == 0:
            elapsed = time.time() - start_time
            speed = elapsed / i
            eta = (len(data) - i) * speed
            print(f"⏱️ 预计剩余时间：{eta/60:.1f} 分钟，平均每条 {speed:.2f}s")

# ✅ 结束信息
duration = time.time() - start_time
print(f"\n🏁 评估完成，总耗时 {duration/60:.1f} 分钟，结果保存至 {output_path}")
print(f"📉 错误条数：{error_count}")

import json
from collections import Counter

output_path = "gpt4o_comparisons_swap.jsonl"

# 读取评估结果
results = []
with open(output_path, 'r', encoding='utf-8') as f:
    for line in f:
        results.append(json.loads(line))

# 统计结果分布
choices = [r.get("choice", "Unknown") for r in results]
counter = Counter(choices)

total = len(results)
a_win = counter["A"]
b_win = counter["B"]
tie = counter["Tie"]
unknown = counter["Unknown"]

# 打印统计结果
print(f"🔍 总共评估样本数：{total}")
print(f"🥇 A 胜出：{a_win} ({a_win/total:.1%})")
print(f"🥈 B 胜出：{b_win} ({b_win/total:.1%})")
print(f"🤝 Tie 平局：{tie} ({tie/total:.1%})")
print(f"❓ Unknown（格式错误等）：{unknown} ({unknown/total:.1%})")

#消除position bias版本+合理prompt

import openai
import os

# 设置 API Key 和代理地址（如果使用代理）
os.environ["OPENAI_API_KEY"] = "sk-XGGe5y0ZvLcQVFp6XnRizs7q47gsVnAbZx0Xr2mfcVlbr99f"
openai.api_key = os.environ["OPENAI_API_KEY"]
openai.api_base = "https://api2.aigcbest.top/v1"  # 如果用代理就改这里

print("✅ OpenAI 设置完成")

import pandas as pd
import json
import os

# 输入 CSV 路径
input_csv = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/generate_perturbe/best_response/final_best_responses_with_baseline_and_perturbed_v11_30.csv"
output_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/generate_perturbe/best_response/gpt4o_compare/pairwise_prompts_30.jsonl"

df = pd.read_csv(input_csv)
pairs = []
tie_count = 0

for idx, row in df.iterrows():
    prompt = row["prompt"]
    baseline = row["baseline_best_response"]
    rps = row["perturbed_best_response"]
    v1 = row["v1"]
    v2 = row["v2"]
    baseline_id = row["best_baseline_response_id"]
    rps_id = row["best_perturbed_response_id"]

    # 自动 Tie（ID 相同）
    if baseline_id == rps_id:
        for order, base_text, rps_text in [("original", baseline, rps), ("flipped", rps, baseline)]:
            pairs.append({
                "pair_id": idx,
                "order": order,
                "prompt": prompt,
                "baseline_response": base_text,
                "rps_response": rps_text,
                "auto_result": "Tie",
                "v1": v1,
                "v2": v2
            })
        tie_count += 1
        continue

    # 双向构造 prompt
    for order, base_text, rps_text in [("original", baseline, rps), ("flipped", rps, baseline)]:
        full_prompt = f"""You are an assistant evaluating two chatbot responses based on user preferences.

The user prefers responses that score higher under this formula:
score = helpfulness × {v1:.2f} + verbosity × {v2:.2f}

Note: The order of responses is randomized. Do not assume A is better than B.

Query: {prompt}

Response A: {base_text}

Response B: {rps_text}

FIRST, write one sentence comparing both responses.
SECOND, on a new line, state only 'A' or 'B'.

Format:
Comparison: ...
More aligned: A/B"""

        pairs.append({
            "pair_id": idx,
            "order": order,
            "prompt": full_prompt,
            "auto_result": None,
            "v1": v1,
            "v2": v2
        })

# 保存 JSONL
with open(output_path, "w", encoding="utf-8") as f:
    for item in pairs:
        f.write(json.dumps(item, ensure_ascii=False) + "\n")

print(f"✅ 构造完成：{len(pairs)} 条（双向 + Tie）")
print(f"🤝 自动 Tie 对数：{tie_count}")
print(f"📁 文件已保存至：{output_path}")

!pip install openai==0.28 --upgrade

!rm "drive/MyDrive/llm_pre_project/dpa_outputs_v9/generate_perturbe/best_response/gpt4o_compare/results/gpt4o_scored_30.jsonl"

import openai
import json
import time
import os

# === 可配置部分 ===
input_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/generate_perturbe/best_response/gpt4o_compare/pairwise_prompts_30.jsonl"
output_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/generate_perturbe/best_response/gpt4o_compare/results/gpt4o_scored_30.jsonl"
FORCE_RERUN = False  # ⬅️ 是否强制重新打分（True 则忽略历史）

# === 创建输出目录 ===
os.makedirs(os.path.dirname(output_path), exist_ok=True)

# === 加载输入数据 ===
with open(input_path, "r", encoding="utf-8") as f:
    data = [json.loads(line) for line in f]

# === 加载已处理样本 ID（pair_id, order） ===
seen_ids = set()
if os.path.exists(output_path) and not FORCE_RERUN:
    with open(output_path, "r", encoding="utf-8") as f:
        for line in f:
            obj = json.loads(line)
            seen_ids.add((obj["pair_id"], obj["order"]))
    print(f"🔁 已加载 {len(seen_ids)} 条已评估样本")

# === GPT 打分函数 ===
def gpt_score(prompt):
    try:
        res = openai.ChatCompletion.create(
            model="gpt-4o",  # ← 改为你自己的模型名或代理模型名
            messages=[{"role": "user", "content": prompt}],
            temperature=0,
            max_tokens=200
        )
        return res['choices'][0]['message']['content']
    except Exception as e:
        return f"[ERROR] {e}"

# === 开始打分 ===
start_time = time.time()
error_count, processed = 0, 0
total_to_process = len(data) if FORCE_RERUN else len([d for d in data if (d["pair_id"], d["order"]) not in seen_ids])

print(f"📦 总样本数：{len(data)}，待评估：{total_to_process}")

with open(output_path, "a", encoding="utf-8") as f_out:
    for item in data:
        pair_id = item["pair_id"]
        order = item["order"]
        prompt = item["prompt"]
        auto = item.get("auto_result")

        # 跳过已处理
        if (pair_id, order) in seen_ids:
            continue

        # 自动 Tie 的直接写入
        if auto == "Tie":
            result = {
                "prompt": prompt,
                "choice": "Tie",
                "reason": "Identical response IDs, skipped GPT.",
                "pair_id": pair_id,
                "order": order
            }
        else:
            reply = gpt_score(prompt)
            lines = reply.strip().splitlines()
            explanation, choice = "", ""

            for line in lines:
                if line.lower().startswith("comparison:"):
                    explanation = line[len("comparison:"):].strip()
                elif line.lower().startswith("more aligned:"):
                    choice = line[len("more aligned:"):].strip().upper()

            if choice not in ["A", "B"]:
                choice = "Unknown"

            result = {
                "prompt": prompt,
                "response_raw": reply,
                "choice": choice,
                "reason": explanation,
                "pair_id": pair_id,
                "order": order
            }

            if "[ERROR]" in reply:
                error_count += 1

        # 写入结果
        f_out.write(json.dumps(result, ensure_ascii=False) + "\n")
        f_out.flush()
        processed += 1

        # 打印进度
        if processed % 10 == 0 or processed == total_to_process:
            print(f"✅ 已评估 {processed}/{total_to_process}")

        if processed % 50 == 0:
            elapsed = time.time() - start_time
            avg = elapsed / processed
            eta = (total_to_process - processed) * avg
            print(f"⏱️ 平均 {avg:.2f}s/条，预计剩余 {eta/60:.1f} 分钟")

# === 完成信息 ===
print(f"\n🏁 打分完成，总处理 {processed} 条")
print(f"📉 错误数：{error_count}")
print(f"📁 保存于：{output_path}")

from collections import defaultdict
import json

input_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/generate_perturbe/best_response/gpt4o_compare/results/gpt4o_scored_30.jsonl"

results = defaultdict(dict)
with open(input_path, 'r', encoding='utf-8') as f:
    for line in f:
        item = json.loads(line)
        pid = str(item.get("pair_id"))
        order = item.get("order")
        choice = item.get("choice", "Unknown")
        results[pid][order] = choice

baseline_win = 0
rps_win = 0
tie = 0
unknown = 0

for pid, pair in results.items():
    orig = pair.get("original")
    flip = pair.get("flipped")

    if orig == "A" and flip == "B":
        baseline_win += 1
    elif orig == "B" and flip == "A":
        rps_win += 1
    elif orig == "Tie" and flip == "Tie":
        tie += 1
    else:
        unknown += 1

total = baseline_win + rps_win + tie + unknown

print("\n📊 双向归并统计")
print(f"总样本对：{total}")
print(f"🥇 Baseline 更好：{baseline_win} ({baseline_win/total:.1%})")
print(f"🥈 RPS 更好：{rps_win} ({rps_win/total:.1%})")
print(f"🤝 完全 Tie：{tie} ({tie/total:.1%})")
print(f"❓ Unknown / 不一致：{unknown} ({unknown/total:.1%})")

import json
from collections import defaultdict

input_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/generate_perturbe/best_response/gpt4o_compare/results/gpt4o_scored.jsonl"

# 加载所有打分结果
results = defaultdict(dict)
with open(input_path, "r", encoding="utf-8") as f:
    for line in f:
        item = json.loads(line)
        pair_id = str(item.get("pair_id"))
        order = item.get("order")
        choice = item.get("choice", "Unknown")
        results[pair_id][order] = choice

# 统计各种异常情况
missing_sides = 0
format_errors = 0
conflicts = 0
conflict_examples = []

for pid, pair in results.items():
    orig = pair.get("original")
    flip = pair.get("flipped")

    if orig is None or flip is None:
        missing_sides += 1
    elif orig == "A" and flip == "A":
        conflicts += 1
        conflict_examples.append((pid, "original: A", "flipped: A"))
    elif orig == "B" and flip == "B":
        conflicts += 1
        conflict_examples.append((pid, "original: B", "flipped: B"))
    elif "Unknown" in [orig, flip]:
        format_errors += 1

# 输出总结
total_unknown = missing_sides + format_errors + conflicts
print("\n📊 Unknown 样本分析报告：共", total_unknown, "条\n")
print(f"❗ 缺失 original/flipped：{missing_sides}")
print(f"❗ GPT 格式错误（输出缺少 More aligned）：{format_errors}")
print(f"❗ 冲突选择（模型两次都选 A 或 B）：{conflicts}")

# 显示前 10 个冲突样本
print("\n🧾 示例（前 10 条）：")
for ex in conflict_examples[:10]:
    print(f"• pair_id: {ex[0]} → 冲突选择 → {ex[1]}, {ex[2]}")

#再改prompt

import pandas as pd
import json
import os

# 输入 CSV 路径
input_csv = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/generate_perturbe/best_response/final_best_responses_with_baseline_and_perturbed_v11_30.csv"
output_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/generate_perturbe/best_response/gpt4o_compare/pairwise_prompts_30.jsonl"

df = pd.read_csv(input_csv)
pairs = []
tie_count = 0

for idx, row in df.iterrows():
    prompt = row["prompt"]
    baseline = row["baseline_best_response"]
    rps = row["perturbed_best_response"]
    v1 = row["v1"]
    v2 = row["v2"]
    baseline_id = row["best_baseline_response_id"]
    rps_id = row["best_perturbed_response_id"]

    # 自动 Tie（ID 相同）
    if baseline_id == rps_id:
        for order, base_text, rps_text in [("original", baseline, rps), ("flipped", rps, baseline)]:
            pairs.append({
                "pair_id": idx,
                "order": order,
                "prompt": prompt,
                "baseline_response": base_text,
                "rps_response": rps_text,
                "auto_result": "Tie",
                "v1": v1,
                "v2": v2
            })
        tie_count += 1
        continue

    # 双向构造 prompt
    for order, base_text, rps_text in [("original", baseline, rps), ("flipped", rps, baseline)]:
        full_prompt = f"""You are an assistant evaluating two chatbot responses.

The user generally prefers responses that score higher under this formula:
score = helpfulness × {v1:.2f} + verbosity × {v2:.2f}

However, in real-world settings, users often express their preferences imprecisely, and their expectations may vary slightly depending on context.

Please consider which response is more robust and likely to remain aligned with the user's intent, even if their preferences shift a little.

The order of responses is randomized. Do not assume A is better than B.

Query: {prompt}

Response A: {base_text}

Response B: {rps_text}

First, briefly compare the two responses.

Then, on a new line, write only 'A' or 'B' to indicate which is more robust and aligned overall.

Format:
Comparison: ...
More aligned: A/B"""

        pairs.append({
            "pair_id": idx,
            "order": order,
            "prompt": full_prompt,
            "auto_result": None,
            "v1": v1,
            "v2": v2
        })

# 保存 JSONL
with open(output_path, "w", encoding="utf-8") as f:
    for item in pairs:
        f.write(json.dumps(item, ensure_ascii=False) + "\n")

print(f"✅ 构造完成：{len(pairs)} 条（双向 + Tie）")
print(f"🤝 自动 Tie 对数：{tie_count}")
print(f"📁 文件已保存至：{output_path}")

!rm "drive/MyDrive/llm_pre_project/dpa_outputs_v9/generate_perturbe/best_response/gpt4o_compare/results/gpt4o_scored_30.jsonl"

import openai
import json
import time
import os

# === 可配置部分 ===
input_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/generate_perturbe/best_response/gpt4o_compare/pairwise_prompts_30.jsonl"
output_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/generate_perturbe/best_response/gpt4o_compare/results/gpt4o_scored_30.jsonl"
FORCE_RERUN = False  # ⬅️ 是否强制重新打分（True 则忽略历史）

# === 创建输出目录 ===
os.makedirs(os.path.dirname(output_path), exist_ok=True)

# === 加载输入数据 ===
with open(input_path, "r", encoding="utf-8") as f:
    data = [json.loads(line) for line in f]

# === 加载已处理样本 ID（pair_id, order） ===
seen_ids = set()
if os.path.exists(output_path) and not FORCE_RERUN:
    with open(output_path, "r", encoding="utf-8") as f:
        for line in f:
            obj = json.loads(line)
            seen_ids.add((obj["pair_id"], obj["order"]))
    print(f"🔁 已加载 {len(seen_ids)} 条已评估样本")

# === GPT 打分函数 ===
def gpt_score(prompt):
    try:
        res = openai.ChatCompletion.create(
            model="gpt-4o",  # ← 改为你自己的模型名或代理模型名
            messages=[{"role": "user", "content": prompt}],
            temperature=0,
            max_tokens=200
        )
        return res['choices'][0]['message']['content']
    except Exception as e:
        return f"[ERROR] {e}"

# === 开始打分 ===
start_time = time.time()
error_count, processed = 0, 0
total_to_process = len(data) if FORCE_RERUN else len([d for d in data if (d["pair_id"], d["order"]) not in seen_ids])

print(f"📦 总样本数：{len(data)}，待评估：{total_to_process}")

with open(output_path, "a", encoding="utf-8") as f_out:
    for item in data:
        pair_id = item["pair_id"]
        order = item["order"]
        prompt = item["prompt"]
        auto = item.get("auto_result")

        # 跳过已处理
        if (pair_id, order) in seen_ids:
            continue

        # 自动 Tie 的直接写入
        if auto == "Tie":
            result = {
                "prompt": prompt,
                "choice": "Tie",
                "reason": "Identical response IDs, skipped GPT.",
                "pair_id": pair_id,
                "order": order
            }
        else:
            reply = gpt_score(prompt)
            lines = reply.strip().splitlines()
            explanation, choice = "", ""

            for line in lines:
                if line.lower().startswith("comparison:"):
                    explanation = line[len("comparison:"):].strip()
                elif line.lower().startswith("more aligned:"):
                    choice = line[len("more aligned:"):].strip().upper()

            if choice not in ["A", "B"]:
                choice = "Unknown"

            result = {
                "prompt": prompt,
                "response_raw": reply,
                "choice": choice,
                "reason": explanation,
                "pair_id": pair_id,
                "order": order
            }

            if "[ERROR]" in reply:
                error_count += 1

        # 写入结果
        f_out.write(json.dumps(result, ensure_ascii=False) + "\n")
        f_out.flush()
        processed += 1

        # 打印进度
        if processed % 10 == 0 or processed == total_to_process:
            print(f"✅ 已评估 {processed}/{total_to_process}")

        if processed % 50 == 0:
            elapsed = time.time() - start_time
            avg = elapsed / processed
            eta = (total_to_process - processed) * avg
            print(f"⏱️ 平均 {avg:.2f}s/条，预计剩余 {eta/60:.1f} 分钟")

# === 完成信息 ===
print(f"\n🏁 打分完成，总处理 {processed} 条")
print(f"📉 错误数：{error_count}")
print(f"📁 保存于：{output_path}")

from collections import defaultdict
import json

input_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/generate_perturbe/best_response/gpt4o_compare/results/gpt4o_scored_30.jsonl"

results = defaultdict(dict)
with open(input_path, 'r', encoding='utf-8') as f:
    for line in f:
        item = json.loads(line)
        pid = str(item.get("pair_id"))
        order = item.get("order")
        choice = item.get("choice", "Unknown")
        results[pid][order] = choice

baseline_win = 0
rps_win = 0
tie = 0
unknown = 0

for pid, pair in results.items():
    orig = pair.get("original")
    flip = pair.get("flipped")

    if orig == "A" and flip == "B":
        baseline_win += 1
    elif orig == "B" and flip == "A":
        rps_win += 1
    elif orig == "Tie" and flip == "Tie":
        tie += 1
    else:
        unknown += 1

total = baseline_win + rps_win + tie + unknown

print("\n📊 双向归并统计")
print(f"总样本对：{total}")
print(f"🥇 Baseline 更好：{baseline_win} ({baseline_win/total:.1%})")
print(f"🥈 RPS 更好：{rps_win} ({rps_win/total:.1%})")
print(f"🤝 完全 Tie：{tie} ({tie/total:.1%})")
print(f"❓ Unknown / 不一致：{unknown} ({unknown/total:.1%})")











#F-DIV方法

import pandas as pd

df = pd.read_csv("/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/batch_v11.csv")
sample = df[df["prompt_id"] == 42]  # 任选一行
print(sample["prompt"].iloc[0])
print(sample["response"].tolist())  # 三个 response



#f-div

#V5:  v = (0.9397, 0.3420)  为主方向测试

import numpy as np

v_main = np.array([0.9397, 0.3420])  # 主方向向量 v₅ ≈ (cos20°, sin20°)

angle_offsets = [-60,-45, -30, -15, 15, 30, 45,60]
perturbed_vs = []
perturbed_angles = []

for offset in angle_offsets:
    angle_deg = 20 + offset
    angle_rad = np.radians(angle_deg)
    v = np.array([np.cos(angle_rad), np.sin(angle_rad)])
    perturbed_vs.append(v)
    perturbed_angles.append(angle_deg)

print(perturbed_angles)

#KL divergence

def kl_divergence(p, q):
    p = np.abs(p) / np.sum(np.abs(p))  # 归一化成伪概率分布
    q = np.abs(q) / np.sum(np.abs(q))
    return np.sum(p * np.log(p / (q + 1e-12)))  # 防止除零

delta = 0.1  # KL 散度容差
kl_values = []

for v in perturbed_vs:
    kl = kl_divergence(v, v_main)
    kl_values.append(kl)

#筛选方向，KL ≤ δ
valid_vs = []
valid_angles = []

for v, angle, kl in zip(perturbed_vs, perturbed_angles, kl_values):
    if kl <= delta:
        valid_vs.append(v)
        valid_angles.append(angle)

print(valid_vs)
print(valid_angles)

import matplotlib.pyplot as plt

fig, ax = plt.subplots(figsize=(6, 6))
ax.add_artist(plt.Circle((0, 0), 1, color='lightgray', fill=False))
ax.quiver(0, 0, v_main[0], v_main[1], angles='xy', scale_units='xy', scale=1, color='blue', label='Main v (20°)')

for v, angle, kl in zip(perturbed_vs, perturbed_angles, kl_values):
    color = 'orange' if kl <= delta else 'gray'
    ax.quiver(0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1, color=color, alpha=0.8)
    ax.text(v[0]*1.1, v[1]*1.1, f"{angle}°", fontsize=8, ha='center', color=color)

ax.set_xlim(-1.2, 1.2)
ax.set_ylim(-1.2, 1.2)
ax.set_aspect('equal')
ax.set_title("f-div Ball (KL ≤ δ) around v5 (20°)")
ax.legend()
plt.grid(True)
plt.show()

#生成回答
#v5的前500
#第二次跑就是完整的2000了

from google.colab import drive
drive.mount('/content/drive')

import os
import numpy as np
import pandas as pd
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM
from tqdm.auto import tqdm
import torch
import time
import random

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
seed = 42
np.random.seed(seed)
random.seed(seed)
torch.manual_seed(seed)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(seed)

# ✅ STEP 2: 保存路径
result_dir = "/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v5"
os.makedirs(result_dir, exist_ok=True)


# ✅ STEP 3: 加载前 2000 条 UltraFeedback prompt
ds = load_dataset("HuggingFaceH4/ultrafeedback_binarized", split="test_prefs")
prompts = ds["prompt"][:2000]
prompt_ids = list(range(2000))

# ✅ STEP 4: 加载 DPA 模型和 tokenizer
model = AutoModelForCausalLM.from_pretrained(
    "Haoxiang-Wang/DPA-v1-Mistral-7B",
    torch_dtype=torch.float16,
    device_map="auto"
).to(device)

tokenizer = AutoTokenizer.from_pretrained("Haoxiang-Wang/RewardModel-Mistral-7B-for-DPA-v1")
tokenizer.padding_side = "left"
if tokenizer.pad_token_id is None:
    tokenizer.pad_token = tokenizer.eos_token

# ✅ STEP 5: 构造 DPA Prompt
def build_input(prompt, v1, v2):
    h = int(np.round(v1 * 100))
    v = int(np.round(v2 * 100))
    sys_instruction = f"You are a helpful assistant. Your response should maximize weighted rating = helpfulness*{h} + verbosity*{v}."
    return [{"role": "user", "content": f"{sys_instruction}\n\n{prompt}"}]

# ✅ STEP 6: 批量生成 response
def generate_response_batch(prompts_batch, prompt_ids_batch, v1, v2):
    input_ids_list = []

    for prompt in prompts_batch:
        messages = build_input(prompt, v1, v2)
        input_ids = tokenizer.apply_chat_template(
            messages, add_generation_prompt=True, return_tensors="pt"
        )[0]
        input_ids_list.append(input_ids.to(device))

    input_ids_padded = torch.nn.utils.rnn.pad_sequence(
        input_ids_list, batch_first=True, padding_value=tokenizer.pad_token_id
    ).to(device)

    attention_mask = (input_ids_padded != tokenizer.pad_token_id).to(device)
    max_input_len = input_ids_padded.shape[1]
    max_new_tokens = min(2048, 4096 - max_input_len)

    with torch.no_grad():
        outputs = model.generate(
            input_ids=input_ids_padded,
            attention_mask=attention_mask,
            max_new_tokens=max_new_tokens,
            temperature=0.7,
            do_sample=True,
            num_return_sequences=1,
            pad_token_id=tokenizer.eos_token_id
        )

    responses = []
    for i, input_ids in enumerate(input_ids_list):
        generated_tokens = outputs[i][input_ids.shape[0]:]
        decoded = tokenizer.decode(generated_tokens, skip_special_tokens=True)
        responses.append({
            "prompt_id": prompt_ids_batch[i],
            "prompt": prompts_batch[i],
            "response": decoded
        })
    return responses

# ✅ STEP 7: 你自己的 valid_vs 和 valid_angles（请替换为你的真实值）
valid_vs = [
    np.array([0.76604444, -0.64278761]),
    np.array([0.90630779, -0.42261826]),
    np.array([0.98480775, -0.17364818]),
    np.array([0.81915204, 0.57357644])
]
valid_angles = [-40, -25, -10, 35]  # 与 valid_vs 顺序一致

# ✅ 主方向（v5）
main_v1 = 0.9397
main_v2 = 0.3420

# ✅ STEP 8: 主生成循环
batch_size = 8

for i, (v_vec, angle_deg) in enumerate(zip(valid_vs, valid_angles)):
    v1, v2 = v_vec[0], v_vec[1]
    output_file = os.path.join(result_dir, f"fdiv_v5_dir{i}.csv")
    print(f"\n🚀 Generating for direction {i}: angle ≈ {angle_deg}°, v = ({v1:.4f}, {v2:.4f})")

    if os.path.exists(output_file):
        existing_df = pd.read_csv(output_file)
        done_prompt_ids = set(existing_df["prompt_id"].unique())
        results = existing_df.to_dict("records")
        print(f"🔁 Resuming from previous run: {len(done_prompt_ids)} prompts already completed.")
    else:
        done_prompt_ids = set()
        results = []

    for start in tqdm(range(0, len(prompts), batch_size), desc=f"Generating fdiv_dir{i}"):
        end = min(start + batch_size, len(prompts))
        batch_prompts_all = prompts[start:end]
        batch_ids_all = prompt_ids[start:end]

        unprocessed_indices = [j for j, pid in enumerate(batch_ids_all) if pid not in done_prompt_ids]
        if not unprocessed_indices:
            continue

        batch_prompts = [batch_prompts_all[j] for j in unprocessed_indices]
        batch_ids = [batch_ids_all[j] for j in unprocessed_indices]

        try:
            batch_outputs = generate_response_batch(batch_prompts, batch_ids, v1, v2)
            for item in batch_outputs:
                item.update({
                    "v1_p": round(v1, 4),
                    "v2_p": round(v2, 4),
                    "direction_index": i,
                    "valid_angle": round(angle_deg, 1),
                    "main_v1": round(main_v1, 4),
                    "main_v2": round(main_v2, 4)
                })
                results.append(item)

            pd.DataFrame(batch_outputs).to_csv(
                output_file, mode='a', index=False,
                header=not os.path.exists(output_file)
            )

        except Exception as e:
            print(f"⚠️ Error at batch {start}-{end}: {e}")

    print(f"✅ Final saved {len(results)} responses to {output_file}")

#检查空和修复v5

import os
import pandas as pd
import numpy as np
import torch
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForCausalLM
from accelerate import Accelerator

# === 模型准备 ===
device = "cuda" if torch.cuda.is_available() else "cpu"
accelerator = Accelerator()

model = AutoModelForCausalLM.from_pretrained(
    "Haoxiang-Wang/DPA-v1-Mistral-7B",
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
    device_map="auto" if torch.cuda.is_available() else None
)
model = accelerator.prepare(model)

tokenizer = AutoTokenizer.from_pretrained("Haoxiang-Wang/DPA-v1-Mistral-7B")
tokenizer.padding_side = "left"
if tokenizer.pad_token_id is None:
    tokenizer.pad_token = tokenizer.eos_token

# === 判断是否无效 response ===
def is_invalid_response(resp):
    if pd.isna(resp):
        return True
    resp = str(resp).strip()
    if not resp:
        return True

    invalid_keywords = [
        "[A:]", "[/SOLUTION]", "[ASS]", "[A]:", "[/CONV]", "[/CODE]", "[/USER]",
        "[OUTPUT]", "[CC]", "[OUT]", "[/DATA]", "[/ASST]", "[/ME]", "[/REST]",
        "[RESP]", "<[user]>", "<|user|>", "[Response]", "[Assistant]", "[Answer]",
        "[End of Response]", "<human>", "<|endoftext|>", "<<", "[INST]", "[/INST]",
        "[]", "[USER]"
    ]
    tokens = resp.split()
    if all(token.strip() in invalid_keywords for token in tokens):
        return True
    tag_like_count = sum(1 for t in tokens if (t.startswith("[") and t.endswith("]")) or (t.startswith("<") and t.endswith(">")))
    if len(tokens) > 0 and tag_like_count / len(tokens) > 0.8:
        return True
    return False

# === 构造输入 prompt ===
def build_input(prompt, v1, v2):
    h = int(np.round(v1 * 100))
    v = int(np.round(v2 * 100))
    sys_instruction = f"You are a helpful assistant. Your response should maximize weighted rating = helpfulness*{h} + verbosity*{v}."
    return [{"role": "system", "content": sys_instruction}, {"role": "user", "content": prompt}]

# === 用模型重新生成 response ===
def regenerate_response(prompt, v1, v2):
    try:
        messages = build_input(prompt, v1, v2)
        input_ids = tokenizer.apply_chat_template(
            messages, add_generation_prompt=True, return_tensors="pt"
        ).to(device)

        if input_ids.ndim != 2 or input_ids.shape[1] == 0:
            print(f"⚠️ Invalid input_ids shape {input_ids.shape}, skipping: {prompt[:60]}")
            return ""

        outputs = model.generate(
            input_ids=input_ids,
            temperature=0.7,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
        generated = outputs[0][input_ids.shape[1]:]
        return tokenizer.decode(generated, skip_special_tokens=True).strip()

    except Exception as e:
        print(f"❌ Error: {e} \nPrompt: {prompt[:60]}")
        return ""

# === 修复单个 CSV 文件 ===
def fix_csv_responses(csv_path, save_dir):
    print(f"\n📂 正在检查: {csv_path}")

    if not os.path.exists(csv_path):
        print(f"❌ 文件不存在: {csv_path}")
        return

    df = pd.read_csv(csv_path)
    if "v1_p" not in df.columns or "v2_p" not in df.columns:
        print(f"❌ 缺少 v1_p/v2_p 列，跳过: {csv_path}")
        return

    bad_mask = df["response"].apply(is_invalid_response)
    num_bad = bad_mask.sum()

    if num_bad == 0:
        print("✅ 无需修复，跳过。")
        return

    print(f"🔧 发现 {num_bad} 条无效 response，开始重生成...")
    for i in tqdm(bad_mask[bad_mask].index, total=num_bad, desc=os.path.basename(csv_path), dynamic_ncols=True):
        row = df.loc[i]
        new_response = regenerate_response(row["prompt"], row["v1_p"], row["v2_p"])
        df.at[i, "response"] = new_response

    os.makedirs(save_dir, exist_ok=True)
    file_name = os.path.basename(csv_path).replace(".csv", "_fixed.csv")
    save_path = os.path.join(save_dir, file_name)
    df.to_csv(save_path, index=False)
    print(f"✅ 修复完成 → 已保存: {save_path}")

# === 主逻辑：修复所有 CSV ===
input_dir = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v5"
output_dir = os.path.join(input_dir, "fixed")

for file in os.listdir(input_dir):
    if file.endswith(".csv") and not file.endswith("_fixed.csv"):
        fix_csv_responses(os.path.join(input_dir, file), output_dir)

#v5 f div 打分

# === 导入库 ===
import os
import time
import pandas as pd
import numpy as np
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
from tqdm.auto import tqdm
from google.colab import drive

# === 挂载 Google Drive ===
drive.mount('/content/drive')

# === 设置路径 ===
input_dir = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v5/fixed"
output_dir = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v5/scored"
os.makedirs(output_dir, exist_ok=True)

# === 加载 Reward Model ===
print("🤖 Loading Reward Model...")
device = "cuda" if torch.cuda.is_available() else "cpu"
rm = AutoModelForSequenceClassification.from_pretrained(
    "Haoxiang-Wang/RewardModel-Mistral-7B-for-DPA-v1", trust_remote_code=True
).to(device)
tokenizer = AutoTokenizer.from_pretrained("Haoxiang-Wang/RewardModel-Mistral-7B-for-DPA-v1")

# === 构造输入模板并打分 ===
def score_response(prompt, response):
    template = "[INST] You must read the following conversation carefully and rate the assistant's response from score 0-100 in these aspects: helpfulness, correctness, coherence, honesty, complexity, verbosity\n\nUser: {prompt}\n\nAssistant: {response} [/INST]"
    inputs = tokenizer(template.format(prompt=prompt, response=response), return_tensors="pt").to(device)
    with torch.no_grad():
        logits = rm(**inputs).logits.squeeze().cpu().numpy()
    return logits[9], logits[4]  # helpfulness, verbosity

# === 打分整个 CSV 文件 ===
def score_file(file_path):
    print(f"\n📂 Scoring file: {file_path}")
    df = pd.read_csv(file_path)
    scores = []

    for i, row in tqdm(df.iterrows(), total=len(df), desc=os.path.basename(file_path), dynamic_ncols=True):
        try:
            h, v = score_response(row["prompt"], row["response"])
        except Exception as e:
            print(f"⚠️ Error on row {i}: {e}")
            h, v = None, None
        scores.append({"helpfulness": h, "verbosity": v})

    df_scores = pd.DataFrame(scores)
    df_all = pd.concat([df, df_scores], axis=1)

    file_name = os.path.basename(file_path).replace(".csv", "_scored.csv")
    save_path = os.path.join(output_dir, file_name)
    df_all.to_csv(save_path, index=False)
    print(f"✅ Scoring done → Saved to: {save_path}")
    return df_all

# === 批量打分目录中的所有 _fixed.csv 文件 ===
for file in os.listdir(input_dir):
    if file.endswith("_fixed.csv"):
        score_file(os.path.join(input_dir, file))

#选出f-div v5 best resposne

from google.colab import drive
drive.mount('/content/drive')

import os
import pandas as pd

# === 设置路径（改为 Colab 下的绝对路径）===
input_dir = "/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v5/scored"
output_file = os.path.join(input_dir, "fdiv_v5_all_best_response.csv")

# === 主方向设置（v5）
main_v1 = 0.9397
main_v2 = 0.3420

# === 合并所有 scored 文件
dfs = []
for file in os.listdir(input_dir):
    if file.endswith("_scored.csv"):
        df = pd.read_csv(os.path.join(input_dir, file))
        df["main_v1"] = main_v1
        df["main_v2"] = main_v2
        df["score_total"] = df["main_v1"] * df["helpfulness"] + df["main_v2"] * df["verbosity"]
        dfs.append(df)

df_all = pd.concat(dfs, ignore_index=True)
df_best = df_all.loc[df_all.groupby("prompt_id")["score_total"].idxmax()]
df_best = df_best.rename(columns={"response": "f_best_response"})

# === 保留字段
cols_to_keep = [
    "prompt_id", "prompt", "f_best_response",
    "v1_p", "v2_p", "valid_angle",
    "main_v1", "main_v2",
    "helpfulness", "verbosity", "score_total"
]
df_best_clean = df_best[cols_to_keep]

# === 保存结果
df_best_clean.to_csv(output_file, index=False)
print(f"✅ 已保存到: {output_file}")



#修复v5 baseline

import os
import pandas as pd
import numpy as np
import torch
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForCausalLM
from accelerate import Accelerator

# === 模型配置 ===
device = "cuda" if torch.cuda.is_available() else "cpu"
accelerator = Accelerator()

model = AutoModelForCausalLM.from_pretrained(
    "Haoxiang-Wang/DPA-v1-Mistral-7B",
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
    device_map="auto" if torch.cuda.is_available() else None
)

# 使用 accelerator 来处理设备映射
model = accelerator.prepare(model)

tokenizer = AutoTokenizer.from_pretrained("Haoxiang-Wang/DPA-v1-Mistral-7B")
tokenizer.padding_side = "left"
if tokenizer.pad_token_id is None:
    tokenizer.pad_token = tokenizer.eos_token

# === 判断 response 是否需要重生成 ===
def is_invalid_response(resp):
    if pd.isna(resp):
        return True
    resp = str(resp).strip()
    if not resp:
        return True

    invalid_keywords = [
        "[A:]", "[/SOLUTION]", "[ASS]", "[A]:", "[/CONV]", "[/CODE]", "[/USER]",
        "[OUTPUT]", "[CC]", "[OUT]", "[/DATA]", "[/ASST]", "[/ME]", "[/REST]",
        "[RESP]", "<[user]>", "<|user|>", "[Response]", "[Assistant]", "[Answer]",
        "[End of Response]", "<human>", "<|endoftext|>", "<<", "[INST]", "[/INST]",
        "[]", "[USER]"
    ]

    if resp in invalid_keywords:
        return True

    tokens = resp.split()
    if all(token.strip() in invalid_keywords for token in tokens):
        return True

    tag_like_count = sum(1 for t in tokens if (t.startswith("[") and t.endswith("]")) or (t.startswith("<") and t.endswith(">")))
    if len(tokens) > 0 and tag_like_count / len(tokens) > 0.8:
        return True

    return False

# === 构造 Prompt ===
def build_input(prompt, v1, v2):
    h = int(np.round(v1 * 100))
    v = int(np.round(v2 * 100))
    sys_instruction = f"You are a helpful assistant. Your response should maximize weighted rating = helpfulness*{h} + verbosity*{v}."
    return [{"role": "system", "content": sys_instruction}, {"role": "user", "content": prompt}]

# === 重生成响应 ===
def regenerate_response(prompt, v1, v2):
    try:
        messages = build_input(prompt, v1, v2)
        input_ids = tokenizer.apply_chat_template(
            messages, add_generation_prompt=True, return_tensors="pt"
        ).to(device)

        if input_ids.ndim != 2 or input_ids.shape[1] == 0:
            print(f"⚠️ Invalid input_ids shape {input_ids.shape}, skipping: {prompt[:60]}")
            return ""

        outputs = model.generate(
            input_ids=input_ids,
            temperature=0.7,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
        generated = outputs[0][input_ids.shape[1]:]
        return tokenizer.decode(generated, skip_special_tokens=True).strip()

    except Exception as e:
        print(f"❌ Error: {e} \nPrompt: {prompt[:60]}")
        return ""

# === 修复单个文件 ===
def fix_csv_responses(csv_path, save_dir):
    print(f"\n📂 检查：{csv_path}")

    if not os.path.exists(csv_path):
        print(f"❌ 文件不存在: {csv_path}")
        return

    df = pd.read_csv(csv_path)
    bad_mask = df["response"].apply(is_invalid_response)
    num_bad = bad_mask.sum()

    if num_bad == 0:
        print("✅ 无需修复，跳过。")
        return

    print(f"🔧 检测到无效 response 数量：{num_bad}")
    for i in tqdm(bad_mask[bad_mask].index, total=num_bad, desc=os.path.basename(csv_path), dynamic_ncols=True):
        row = df.loc[i]
        new_response = regenerate_response(row["prompt"], row["v1"], row["v2"])
        df.at[i, "response"] = new_response

    os.makedirs(save_dir, exist_ok=True)
    file_name = os.path.basename(csv_path).replace(".csv", "_fixed.csv")
    save_path = os.path.join(save_dir, file_name)
    df.to_csv(save_path, index=False)
    print(f"✅ 修复完成 → 保存到: {save_path}")

# === 主逻辑 ===
input_dir = "drive/MyDrive/llm_pre_project/dpa_outputs_v9"
output_dir = os.path.join(input_dir, "fixed_batch")

# 只修复 batch_v5.csv
input_file = "batch_v5.csv"

# 修复该文件
fix_csv_responses(os.path.join(input_dir, input_file), output_dir)

#baslinev5打分（verbosity和helpfulness)

# === 导入库 ===
import os
import time
import pandas as pd
import numpy as np
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
from tqdm.auto import tqdm
from google.colab import drive

# === 挂载 Google Drive ===
drive.mount('/content/drive')

# === 设置输入输出路径 ===
input_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/fixed_batch/batch_v5_fixed.csv"
output_dir = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch"
os.makedirs(output_dir, exist_ok=True)

# === 加载数据 ===
print(f"📦 Loading file: {input_path}")
df = pd.read_csv(input_path)

# === 加载 Reward Model ===
print("🤖 Loading Reward Model...")
device = "cuda" if torch.cuda.is_available() else "cpu"
rm = AutoModelForSequenceClassification.from_pretrained(
    "Haoxiang-Wang/RewardModel-Mistral-7B-for-DPA-v1", trust_remote_code=True
).to(device)
tokenizer = AutoTokenizer.from_pretrained("Haoxiang-Wang/RewardModel-Mistral-7B-for-DPA-v1")

def score_response(prompt, response):
    template = "[INST] You must read the following conversation carefully and rate the assistant's response from score 0-100 in these aspects: helpfulness, correctness, coherence, honesty, complexity, verbosity\n\nUser: {prompt}\n\nAssistant: {response} [/INST]"
    inputs = tokenizer(template.format(prompt=prompt, response=response), return_tensors="pt").to(device)
    with torch.no_grad():
        logits = rm(**inputs).logits.squeeze().cpu().numpy()
    return logits[9], logits[4]  # helpfulness, verbosity

# === 开始打分 ===
print("🧠 Scoring responses...")
start_time = time.time()
scores = []

for i, row in tqdm(df.iterrows(), total=len(df), desc="🔍 Scoring"):
    try:
        h, v = score_response(row["prompt"], row["response"])
        scores.append({"v1": row["v1"], "v2": row["v2"], "helpfulness": h, "verbosity": v})
    except Exception as e:
        print(f"⚠️ Error on row {i}: {e}")
        scores.append({"v1": row["v1"], "v2": row["v2"], "helpfulness": None, "verbosity": None})

df_scores = pd.DataFrame(scores)
df_all = pd.concat([df, df_scores], axis=1)

# === 保存结果 ===
output_file = os.path.join(output_dir, os.path.basename(input_path).replace(".csv", "_scored.csv"))
df_all.to_csv(output_file, index=False)

end_time = time.time()
print(f"\n✅ Scoring complete! Total scored: {len(df_all)}")
print(f"🕒 Time elapsed: {end_time - start_time:.1f} seconds")
print(f"📁 Saved to: {output_file}")

import pandas as pd
import os

# === 设置输入文件路径 ===
input_file = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch/batch_v5_fixed_scored.csv"

# === 设置输出路径 ===
output_dir = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch"
output_file_with_total = os.path.join(output_dir, "batch_v5_fixed_scored_with_total.csv")

# best response 输出目录和文件名
best_response_dir = os.path.join(output_dir, "best_response")
os.makedirs(best_response_dir, exist_ok=True)
output_best_file = os.path.join(best_response_dir, "best_response_v5_baseline.csv")

# === 加载已有评分文件 ===
df = pd.read_csv(input_file)

# === 计算总分 ===
df["total_score"] = df["v1"] * df["helpfulness"] + df["v2"] * df["verbosity"]

# === 保存含总分的新文件 ===
df.to_csv(output_file_with_total, index=False)
print(f"✅ Saved file with total_score: {output_file_with_total}")

# === 选出每个 prompt_id 下 total_score 最高的 response ===
df_best = df.loc[df.groupby("prompt_id")["total_score"].idxmax()].copy()

# 可选：将 response 列重命名为 best_response
df_best = df_best.rename(columns={"response": "best_response"})

# === 保存每个 prompt_id 的最佳回答到指定路径 ===
df_best.to_csv(output_best_file, index=False)
print(f"🏆 Best responses per prompt saved to: {output_best_file}")

#比较前500 baseline和fdiv

#step2:按prompt_id对齐两组数据

import pandas as pd

# === 路径设置 ===
baseline_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch/best_response/best_response_v5_baseline.csv"
fdiv_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v5/scored/fdiv_v5_all_best_response.csv"

# === 加载数据 ===
df_baseline = pd.read_csv(baseline_path)
df_fdiv = pd.read_csv(fdiv_path)

# === 合并数据（按 prompt_id）
df_merged = pd.merge(
    df_baseline,
    df_fdiv,
    on="prompt_id",
    suffixes=("_baseline", "_fdiv")
)

# === 字段重命名 ===
df_merged = df_merged.rename(columns={
    "prompt_baseline": "prompt",
    "best_response": "baseline_best_response",
    "score_total": "f_score_total",                # 来自 f-div
    "total_score": "baseline_score_total"          # 来自 baseline
})

# === 保留字段，包括 baseline 的 response_id
cols_to_show = [
    "prompt_id", "prompt",
    "baseline_best_response", "f_best_response",
    "baseline_score_total", "f_score_total",
    "response_id",                       # ✅ 加上 response_id
    "v1_p", "v2_p", "valid_angle",
    "main_v1", "main_v2"
]
df_clean = df_merged[cols_to_show]

# === 保存结果 ===
output_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/v5_final_merged_for_judge.csv"
df_clean.to_csv(output_path, index=False)

print(f"✅ 合并完成，文件保存至：{output_path}")

#step3 构造双向

import pandas as pd
import json
import os

# === 路径设置 ===
input_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/v5_final_merged_for_judge.csv"
output_dir = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/gpt4_judge_inputs/"
os.makedirs(output_dir, exist_ok=True)

output_run1 = os.path.join(output_dir, "pairwise_run1_baselineA.jsonl")
output_run2 = os.path.join(output_dir, "pairwise_run2_fdivA.jsonl")

# === 读取合并数据 ===
df = pd.read_csv(input_path)

# === 构造两个版本的 JSONL ===
run1, run2 = [], []
tie_count = 0

for idx, row in df.iterrows():
    prompt = row["prompt"]
    baseline = row["baseline_best_response"]
    fdiv = row["f_best_response"]
    v1 = row["main_v1"]
    v2 = row["main_v2"]
    pid = row["prompt_id"]
    baseline_id = row["response_id"]

    # 标准化构造函数
    def build_prompt(a_text, b_text):
        return f"""[HH-RLHF]: For the following query to a chatbot, which response is more helpful?

Query: {prompt}

Response A: {a_text}

Response B: {b_text}

FIRST provide a one-sentence comparison of the two responses and explain which you feel is more helpful.
SECOND, on a new line, state only 'A' or 'B' to indicate which response is more helpful.

Format:
Comparison: ...
More helpful: A/B"""

    # 自动 Tie 情况
    if baseline.strip() == fdiv.strip():
        for group, a_text, b_text, tag in [
            (run1, baseline, fdiv, "baseline_A"),
            (run2, fdiv, baseline, "fdiv_A")
        ]:
            group.append({
                "pair_id": idx,
                "prompt_id": pid,
                "order": tag,
                "auto_result": "Tie",
                "baseline_id": baseline_id,
                "v1": v1,
                "v2": v2,
                "formatted_prompt": build_prompt(a_text, b_text)
            })
        tie_count += 1
        continue

    # Run1: baseline A
    run1.append({
        "pair_id": idx,
        "prompt_id": pid,
        "order": "baseline_A",
        "auto_result": None,
        "baseline_id": baseline_id,
        "v1": v1,
        "v2": v2,
        "formatted_prompt": build_prompt(baseline, fdiv)
    })

    # Run2: f-div A
    run2.append({
        "pair_id": idx,
        "prompt_id": pid,
        "order": "fdiv_A",
        "auto_result": None,
        "baseline_id": baseline_id,
        "v1": v1,
        "v2": v2,
        "formatted_prompt": build_prompt(fdiv, baseline)
    })

# === 保存为 JSONL（ASCII 安全）===
with open(output_run1, "w", encoding="utf-8") as f:
    for item in run1:
        f.write(json.dumps(item, ensure_ascii=True) + "\n")

with open(output_run2, "w", encoding="utf-8") as f:
    for item in run2:
        f.write(json.dumps(item, ensure_ascii=True) + "\n")

print(f"✅ 构造完成：Run1 条数 = {len(run1)}, Run2 条数 = {len(run2)}, 自动 Tie = {tie_count}")
print(f"📁 保存至：\n- {output_run1}\n- {output_run2}")

#gpt4o开始比较

import openai
import os

# 设置 API Key 和代理地址（如果使用代理）
os.environ["OPENAI_API_KEY"] = "sk-XGGe5y0ZvLcQVFp6XnRizs7q47gsVnAbZx0Xr2mfcVlbr99f"
openai.api_key = os.environ["OPENAI_API_KEY"]
openai.api_base = "https://api2.aigcbest.top/v1"  # 如果用代理就改这里

print("✅ OpenAI 设置完成")

import openai
import json
import time
import os
from tqdm import tqdm

# === 配置参数 ===
input_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/gpt4_judge_inputs/pairwise_run1_baselineA.jsonl"
output_path = input_path.replace(".jsonl", "_results.jsonl")
model = "gpt-4o"
sleep_time = 1.0
max_retries = 3

# === 加载输入数据 ===
with open(input_path, "r", encoding="utf-8") as f:
    all_prompts = [json.loads(line) for line in f]

# === 断点续跑（读取已有结果）
completed_ids = set()
results = []
if os.path.exists(output_path):
    with open(output_path, "r", encoding="utf-8") as f:
        for line in f:
            item = json.loads(line)
            results.append(item)
            completed_ids.add(item["pair_id"])
    print(f"🔁 检测到已有结果，已跳过 {len(completed_ids)} 条")

# === 开始评估 ===
start_time = time.time()

for item in tqdm(all_prompts, desc="🧠 GPT-4o 评估中"):
    pid = item["pair_id"]
    if pid in completed_ids:
        continue

    if item.get("auto_result") == "Tie":
        item["gpt_judgment"] = "Tie"
        print(f"🤝 pair_id={pid} → Auto-Tie")
        results.append(item)
        continue

    prompt = item["formatted_prompt"]
    for attempt in range(max_retries):
        try:
            response = openai.ChatCompletion.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.0,
            )
            reply = response.choices[0].message["content"].strip()
            item["gpt_raw_response"] = reply

            last_line = reply.strip().splitlines()[-1].strip().upper()
            if "MORE HELPFUL: A" in last_line or last_line == "A":
                item["gpt_judgment"] = "A"
            elif "MORE HELPFUL: B" in last_line or last_line == "B":
                item["gpt_judgment"] = "B"
            else:
                item["gpt_judgment"] = "Unclear"

            print(f"✅ pair_id={pid} → {item['gpt_judgment']}")
            break
        except Exception as e:
            item["error"] = str(e)
            print(f"❌ pair_id={pid} → Error: {str(e)}")
            time.sleep(sleep_time)
    else:
        item["gpt_judgment"] = "Error"
        print(f"❌ pair_id={pid} → Failed after {max_retries} attempts")

    results.append(item)
    time.sleep(sleep_time)

# === 保存所有结果 ===
with open(output_path, "w", encoding="utf-8") as f:
    for r in results:
        f.write(json.dumps(r, ensure_ascii=False) + "\n")

end_time = time.time()
print(f"\n✅ 评估完成，共计评估 {len(results)} 条")
print(f"🕒 总耗时：{end_time - start_time:.1f} 秒")
print(f"📁 保存结果到：{output_path}")

#交换位置

import openai
import json
import time
import os
from tqdm import tqdm

# === 配置参数 ===
input_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/gpt4_judge_inputs/pairwise_run2_fdivA.jsonl"
output_path = input_path.replace(".jsonl", "_results.jsonl")
model = "gpt-4o"
sleep_time = 1.0
max_retries = 3

# === 加载输入数据 ===
with open(input_path, "r", encoding="utf-8") as f:
    all_prompts = [json.loads(line) for line in f]

# === 断点续跑（读取已有结果）
completed_ids = set()
results = []
if os.path.exists(output_path):
    with open(output_path, "r", encoding="utf-8") as f:
        for line in f:
            item = json.loads(line)
            results.append(item)
            completed_ids.add(item["pair_id"])
    print(f"🔁 检测到已有结果，已跳过 {len(completed_ids)} 条")

# === 开始评估 ===
start_time = time.time()

for item in tqdm(all_prompts, desc="🧠 GPT-4o 评估中"):
    pid = item["pair_id"]
    if pid in completed_ids:
        continue

    if item.get("auto_result") == "Tie":
        item["gpt_judgment"] = "Tie"
        print(f"🤝 pair_id={pid} → Auto-Tie")
        results.append(item)
        continue

    prompt = item["formatted_prompt"]
    for attempt in range(max_retries):
        try:
            response = openai.ChatCompletion.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.0,
            )
            reply = response.choices[0].message["content"].strip()
            item["gpt_raw_response"] = reply

            last_line = reply.strip().splitlines()[-1].strip().upper()
            if "MORE HELPFUL: A" in last_line or last_line == "A":
                item["gpt_judgment"] = "A"
            elif "MORE HELPFUL: B" in last_line or last_line == "B":
                item["gpt_judgment"] = "B"
            else:
                item["gpt_judgment"] = "Unclear"

            print(f"✅ pair_id={pid} → {item['gpt_judgment']}")
            break
        except Exception as e:
            item["error"] = str(e)
            print(f"❌ pair_id={pid} → Error: {str(e)}")
            time.sleep(sleep_time)
    else:
        item["gpt_judgment"] = "Error"
        print(f"❌ pair_id={pid} → Failed after {max_retries} attempts")

    results.append(item)
    time.sleep(sleep_time)

# === 保存所有结果 ===
with open(output_path, "w", encoding="utf-8") as f:
    for r in results:
        f.write(json.dumps(r, ensure_ascii=False) + "\n")

end_time = time.time()
print(f"\n✅ 评估完成，共计评估 {len(results)} 条")
print(f"🕒 总耗时：{end_time - start_time:.1f} 秒")
print(f"📁 保存结果到：{output_path}")

#胜率统计

import json
import pandas as pd
from collections import Counter

# ✅ 路径配置
run1_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/gpt4_judge_inputs/pairwise_run1_baselineA_results.jsonl"
run2_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/gpt4_judge_inputs/pairwise_run2_fdivA_results.jsonl"

# ✅ 加载 JSONL 文件
def load_jsonl(path):
    with open(path, "r", encoding="utf-8") as f:
        return [json.loads(line) for line in f]

run1 = load_jsonl(run1_path)
run2 = load_jsonl(run2_path)

# ✅ 核心判断逻辑（谁赢了？）
def resolve_winner(entry):
    judge = entry.get("gpt_judgment", "").strip().upper()
    order = entry.get("order")

    if judge == "TIE":
        return "Tie"
    elif judge == "ERROR" or judge == "UNCLEAR":
        return "Invalid"

    # 根据 A/B 判断谁赢
    if order == "baseline_A":
        return "Baseline" if judge == "A" else "f-div"
    elif order == "fdiv_A":
        return "f-div" if judge == "A" else "Baseline"
    else:
        return "Invalid"

# ✅ 汇总统计
def count_results(entries, run_name):
    counter = Counter()
    for e in entries:
        winner = resolve_winner(e)
        counter[winner] += 1
    total = sum(counter.values())
    win_rate = {
        "Run": run_name,
        "Total": total,
        "f-div wins": counter["f-div"],
        "baseline wins": counter["Baseline"],
        "Ties": counter["Tie"],
        "Invalid": counter["Invalid"],
        "f-div win rate (%)": round(counter["f-div"] / total * 100, 2) if total else 0,
        "baseline win rate (%)": round(counter["Baseline"] / total * 100, 2) if total else 0,
        "Tie rate (%)": round(counter["Tie"] / total * 100, 2) if total else 0
    }
    return win_rate

# ✅ 输出为 DataFrame
df_stats = pd.DataFrame([
    count_results(run1, "Run1 (baseline in A)"),
    count_results(run2, "Run2 (f-div in A)")
])

# ✅ 显示统计结果
from IPython.display import display
display(df_stats)

#随机打乱顺序评估

import pandas as pd
import json
import os
import random

# === 设置路径 ===
input_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/v5_final_merged_for_judge.csv"
output_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/gpt4_judge_inputs/pairwise_randomized.jsonl"

# === 读取合并数据 ===
df = pd.read_csv(input_path)

# === 设置随机种子（可选）===
random.seed(42)

# === 构造单轮随机化 JSONL ===
output = []
tie_count = 0

for idx, row in df.iterrows():
    prompt = row["prompt"]
    baseline = row["baseline_best_response"]
    fdiv = row["f_best_response"]
    pid = row["prompt_id"]
    baseline_id = row["response_id"]

    # 如果两响应完全一样，自动 Tie
    if baseline.strip() == fdiv.strip():
        output.append({
            "pair_id": idx,
            "prompt_id": pid,
            "auto_result": "Tie",
            "baseline_id": baseline_id,
            "a_origin": "baseline",
            "b_origin": "f-div",
            "formatted_prompt": "[HH-RLHF]: (Responses identical, auto tie)"
        })
        tie_count += 1
        continue

    # 随机打乱顺序
    if random.random() < 0.5:
        response_a, response_b = baseline, fdiv
        a_origin, b_origin = "baseline", "f-div"
    else:
        response_a, response_b = fdiv, baseline
        a_origin, b_origin = "f-div", "baseline"

    full_prompt = f"""[HH-RLHF]: For the following query to a chatbot, which response is more helpful?

Query: {prompt}

Response A: {response_a}

Response B: {response_b}

FIRST provide a one-sentence comparison of the two responses and explain which you feel is more helpful.
SECOND, on a new line, state only 'A' or 'B' to indicate which response is more helpful.

Format:
Comparison: ...
More helpful: A/B"""

    output.append({
        "pair_id": idx,
        "prompt_id": pid,
        "auto_result": None,
        "baseline_id": baseline_id,
        "a_origin": a_origin,
        "b_origin": b_origin,
        "formatted_prompt": full_prompt
    })

# === 保存为 JSONL ===
with open(output_path, "w", encoding="utf-8") as f:
    for item in output:
        f.write(json.dumps(item, ensure_ascii=False) + "\n")

print(f"✅ 已构造 JSONL，共 {len(output)} 条，自动 Tie: {tie_count}")
print(f"📁 保存至：{output_path}")

#gpt4o开始评估

import openai
import os

# 设置 API Key 和代理地址（如果使用代理）
os.environ["OPENAI_API_KEY"] = "sk-XGGe5y0ZvLcQVFp6XnRizs7q47gsVnAbZx0Xr2mfcVlbr99f"
openai.api_key = os.environ["OPENAI_API_KEY"]
openai.api_base = "https://api2.aigcbest.top/v1"  # 如果用代理就改这里

print("✅ OpenAI 设置完成")

import openai
import json
import time
import os
from tqdm import tqdm

# === 参数配置 ===
input_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/gpt4_judge_inputs/pairwise_randomized.jsonl"
output_path = input_path.replace(".jsonl", "_results.jsonl")
model = "gpt-4o"
sleep_time = 1.0
max_retries = 3

# === 加载输入数据 ===
with open(input_path, "r", encoding="utf-8") as f:
    all_prompts = [json.loads(line) for line in f]

# === 读取已完成项（支持断点续跑）===
completed_ids = set()
results = []
if os.path.exists(output_path):
    with open(output_path, "r", encoding="utf-8") as f:
        for line in f:
            item = json.loads(line)
            results.append(item)
            completed_ids.add(item["pair_id"])
    print(f"🔁 已加载 {len(completed_ids)} 条历史结果，跳过")

# === 开始评估 ===
start_time = time.time()

for item in tqdm(all_prompts, desc="🧠 GPT-4o 评估中"):
    pid = item["pair_id"]
    if pid in completed_ids:
        continue

    if item.get("auto_result") == "Tie":
        item["gpt_judgment"] = "Tie"
        print(f"🤝 pair_id={pid} → Auto-Tie")
        results.append(item)
        continue

    prompt = item["formatted_prompt"]
    for attempt in range(max_retries):
        try:
            response = openai.ChatCompletion.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.0,
            )
            reply = response.choices[0].message["content"].strip()
            item["gpt_raw_response"] = reply

            last_line = reply.strip().splitlines()[-1].strip().upper()
            if "MORE HELPFUL: A" in last_line or last_line == "A":
                item["gpt_judgment"] = "A"
            elif "MORE HELPFUL: B" in last_line or last_line == "B":
                item["gpt_judgment"] = "B"
            else:
                item["gpt_judgment"] = "Unclear"

            print(f"✅ pair_id={pid} → {item['gpt_judgment']}")
            break
        except Exception as e:
            item["error"] = str(e)
            print(f"❌ pair_id={pid} → Error: {str(e)}")
            time.sleep(sleep_time)
    else:
        item["gpt_judgment"] = "Error"
        print(f"❌ pair_id={pid} → Failed after {max_retries} attempts")

    results.append(item)
    time.sleep(sleep_time)

# === 保存评估结果 ===
with open(output_path, "w", encoding="utf-8") as f:
    for r in results:
        f.write(json.dumps(r, ensure_ascii=False) + "\n")

end_time = time.time()
print(f"\n✅ 评估完成，共计 {len(results)} 条")
print(f"🕒 耗时：{end_time - start_time:.1f} 秒")
print(f"📁 输出结果路径：{output_path}")

#统计胜率

import json
from collections import Counter
import pandas as pd
import matplotlib.pyplot as plt

# === 替换为你的结果文件路径 ===
results_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/gpt4_judge_inputs/pairwise_randomized_results.jsonl"

# === 加载数据 ===
with open(results_path, "r", encoding="utf-8") as f:
    data = [json.loads(line) for line in f]

# === 统计核心函数 ===
def judge_winner(item):
    judgment = item.get("gpt_judgment", "")
    a_origin = item.get("a_origin")
    b_origin = item.get("b_origin")

    if judgment == "Tie":
        return "Tie"
    elif judgment == "A":
        return a_origin  # 谁在A位胜
    elif judgment == "B":
        return b_origin  # 谁在B位胜
    elif judgment == "Unclear":
        return "Unclear"
    elif judgment == "Error":
        return "Error"
    else:
        return "Invalid"

# === 应用统计 ===
outcomes = [judge_winner(item) for item in data]
counter = Counter(outcomes)

# === 整理为 DataFrame 展示 ===
total = sum(counter.values())
win_fdiv = counter.get("f-div", 0)
win_baseline = counter.get("baseline", 0)
tie = counter.get("Tie", 0)
unclear = counter.get("Unclear", 0)
error = counter.get("Error", 0)

summary = pd.DataFrame([{
    "Total": total,
    "f-div wins": win_fdiv,
    "baseline wins": win_baseline,
    "Ties": tie,
    "Unclear": unclear,
    "Error": error,
    "f-div win rate (%)": round(win_fdiv / total * 100, 2),
    "baseline win rate (%)": round(win_baseline / total * 100, 2),
    "Tie rate (%)": round(tie / total * 100, 2),
    "Unclear/Error rate (%)": round((unclear + error) / total * 100, 2)
}])

# === 打印结果 ===
print(summary)

# === 可视化胜率条形图 ===
summary_plot = {
    "f-div": win_fdiv,
    "baseline": win_baseline,
    "Tie": tie,
    "Unclear": unclear,
    "Error": error
}

# === 带百分比标注的可视化 ===
plt.figure(figsize=(8, 4))
bars = plt.bar(summary_plot.keys(), summary_plot.values(), color=["#2ca02c", "#1f77b4", "#ff7f0e", "#999999", "#d62728"])

plt.title("GPT-4o Judged Outcome (Randomized A/B)", fontsize=14)
plt.ylabel("Count")
plt.xticks(fontsize=10)
plt.grid(axis="y", linestyle="--", alpha=0.5)

# ✅ 在每个柱子上方标注百分比
for bar, label in zip(bars, summary_plot.keys()):
    count = bar.get_height()
    pct = count / total * 100
    plt.text(bar.get_x() + bar.get_width()/2, count + 2, f"{pct:.1f}%", ha="center", va="bottom", fontsize=10)

plt.tight_layout()
plt.show()



#v10 2000条

import numpy as np

v_main = np.array([0.7071, 0.7071])  # 主方向 v₁₀ = 45°
angle_offsets = [-60, -45, -30, -15, 15, 30, 45, 60]

perturbed_vs = []
perturbed_angles = []

for offset in angle_offsets:
    angle_deg = 45 + offset  # 围绕 45° 做扰动
    angle_rad = np.radians(angle_deg)
    v = np.array([np.cos(angle_rad), np.sin(angle_rad)])
    perturbed_vs.append(v)
    perturbed_angles.append(angle_deg)

def kl_divergence(p, q):
    p = np.abs(p) / np.sum(np.abs(p))  # 归一化成伪概率分布
    q = np.abs(q) / np.sum(np.abs(q))
    return np.sum(p * np.log(p / (q + 1e-12)))  # 防止除零

delta = 0.1 # KL 散度容差
kl_values = []

for v in perturbed_vs:
    kl = kl_divergence(v, v_main)
    kl_values.append(kl)

# ✅ 筛选方向，KL ≤ δ
valid_vs = []
valid_angles = []

for v, angle, kl in zip(perturbed_vs, perturbed_angles, kl_values):
    if kl <= delta:
        valid_vs.append(v)
        valid_angles.append(angle)

print(valid_vs)
print(valid_angles)

import matplotlib.pyplot as plt

fig, ax = plt.subplots(figsize=(6, 6))
ax.add_artist(plt.Circle((0, 0), 1, color='lightgray', fill=False))
ax.quiver(0, 0, v_main[0], v_main[1], angles='xy', scale_units='xy', scale=1, color='blue', label='Main v (45°)')

for v, angle, kl in zip(perturbed_vs, perturbed_angles, kl_values):
    color = 'orange' if kl <= delta else 'gray'
    ax.quiver(0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1, color=color, alpha=0.8)
    ax.text(v[0]*1.1, v[1]*1.1, f"{angle}°", fontsize=8, ha='center', color=color)

ax.set_xlim(-1.2, 1.2)
ax.set_ylim(-1.2, 1.2)
ax.set_aspect('equal')
ax.set_title("f-div Ball (KL ≤ δ) around v5 (45°)")
ax.legend()
plt.grid(True)
plt.show()

#生成v10 2000

from google.colab import drive
drive.mount('/content/drive')

import os
import numpy as np
import pandas as pd
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM
from tqdm.auto import tqdm
import torch
import time
import random
·
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
seed = 42
np.random.seed(seed)
random.seed(seed)
torch.manual_seed(seed)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(seed)

# ✅ STEP 2: 保存路径
result_dir = "/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v10"
os.makedirs(result_dir, exist_ok=True)


# ✅ STEP 3: 加载前 500 条 UltraFeedback prompt
ds = load_dataset("HuggingFaceH4/ultrafeedback_binarized", split="test_prefs")
prompts = ds["prompt"][:2000]
prompt_ids = list(range(2000))

# ✅ STEP 4: 加载 DPA 模型和 tokenizer
model = AutoModelForCausalLM.from_pretrained(
    "Haoxiang-Wang/DPA-v1-Mistral-7B",
    torch_dtype=torch.float16,
    device_map="auto"
).to(device)

tokenizer = AutoTokenizer.from_pretrained("Haoxiang-Wang/RewardModel-Mistral-7B-for-DPA-v1")
tokenizer.padding_side = "left"
if tokenizer.pad_token_id is None:
    tokenizer.pad_token = tokenizer.eos_token

# ✅ STEP 5: 构造 DPA Prompt
def build_input(prompt, v1, v2):
    h = int(np.round(v1 * 100))
    v = int(np.round(v2 * 100))
    sys_instruction = f"You are a helpful assistant. Your response should maximize weighted rating = helpfulness*{h} + verbosity*{v}."
    return [{"role": "user", "content": f"{sys_instruction}\n\n{prompt}"}]

# ✅ STEP 6: 批量生成 response
def generate_response_batch(prompts_batch, prompt_ids_batch, v1, v2):
    input_ids_list = []

    for prompt in prompts_batch:
        messages = build_input(prompt, v1, v2)
        input_ids = tokenizer.apply_chat_template(
            messages, add_generation_prompt=True, return_tensors="pt"
        )[0]
        input_ids_list.append(input_ids.to(device))

    input_ids_padded = torch.nn.utils.rnn.pad_sequence(
        input_ids_list, batch_first=True, padding_value=tokenizer.pad_token_id
    ).to(device)

    attention_mask = (input_ids_padded != tokenizer.pad_token_id).to(device)
    max_input_len = input_ids_padded.shape[1]
    max_new_tokens = min(2048, 4096 - max_input_len)

    with torch.no_grad():
        outputs = model.generate(
            input_ids=input_ids_padded,
            attention_mask=attention_mask,
            max_new_tokens=max_new_tokens,
            temperature=0.7,
            do_sample=True,
            num_return_sequences=1,
            pad_token_id=tokenizer.eos_token_id
        )

    responses = []
    for i, input_ids in enumerate(input_ids_list):
        generated_tokens = outputs[i][input_ids.shape[0]:]
        decoded = tokenizer.decode(generated_tokens, skip_special_tokens=True)
        responses.append({
            "prompt_id": prompt_ids_batch[i],
            "prompt": prompts_batch[i],
            "response": decoded
        })
    return responses

# ✅ STEP 7: 你自己的 valid_vs 和 valid_angles（请替换为你的真实值）
valid_vs = [
    np.array([0.8660, 0.5000]),  # 角度 ≈ 30°
    np.array([0.5000, 0.8660])   # 角度 ≈ 60°
]

valid_angles = [30, 60]  # 与 valid_vs 顺序一致

# ✅ 主方向（v10）
main_v1 = 0.7071
main_v2 = 0.7071

# ✅ STEP 8: 主生成循环
batch_size = 8

for i, (v_vec, angle_deg) in enumerate(zip(valid_vs, valid_angles)):
    v1, v2 = v_vec[0], v_vec[1]
    output_file = os.path.join(result_dir, f"fdiv_v10_dir{i}.csv")
    print(f"\n🚀 Generating for direction {i}: angle ≈ {angle_deg}°, v = ({v1:.4f}, {v2:.4f})")

    if os.path.exists(output_file):
        existing_df = pd.read_csv(output_file)
        done_prompt_ids = set(existing_df["prompt_id"].unique())
        results = existing_df.to_dict("records")
        print(f"🔁 Resuming from previous run: {len(done_prompt_ids)} prompts already completed.")
    else:
        done_prompt_ids = set()
        results = []

    for start in tqdm(range(0, len(prompts), batch_size), desc=f"Generating fdiv_dir{i}"):
        end = min(start + batch_size, len(prompts))
        batch_prompts_all = prompts[start:end]
        batch_ids_all = prompt_ids[start:end]

        unprocessed_indices = [j for j, pid in enumerate(batch_ids_all) if pid not in done_prompt_ids]
        if not unprocessed_indices:
            continue

        batch_prompts = [batch_prompts_all[j] for j in unprocessed_indices]
        batch_ids = [batch_ids_all[j] for j in unprocessed_indices]

        try:
            batch_outputs = generate_response_batch(batch_prompts, batch_ids, v1, v2)
            for item in batch_outputs:
                item.update({
                    "v1_p": round(v1, 4),
                    "v2_p": round(v2, 4),
                    "direction_index": i,
                    "valid_angle": round(angle_deg, 1),
                    "main_v1": round(main_v1, 4),
                    "main_v2": round(main_v2, 4)
                })
                results.append(item)

            pd.DataFrame(batch_outputs).to_csv(
                output_file, mode='a', index=False,
                header=not os.path.exists(output_file)
            )

        except Exception as e:
            print(f"⚠️ Error at batch {start}-{end}: {e}")

    print(f"✅ Final saved {len(results)} responses to {output_file}")

#修复v10

import os
import pandas as pd
import numpy as np
import torch
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForCausalLM
from accelerate import Accelerator

# === 模型准备 ===
device = "cuda" if torch.cuda.is_available() else "cpu"
accelerator = Accelerator()

model = AutoModelForCausalLM.from_pretrained(
    "Haoxiang-Wang/DPA-v1-Mistral-7B",
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
    device_map="auto" if torch.cuda.is_available() else None
)
model = accelerator.prepare(model)

tokenizer = AutoTokenizer.from_pretrained("Haoxiang-Wang/DPA-v1-Mistral-7B")
tokenizer.padding_side = "left"
if tokenizer.pad_token_id is None:
    tokenizer.pad_token = tokenizer.eos_token

# === 判断是否无效 response ===
def is_invalid_response(resp):
    if pd.isna(resp):
        return True
    resp = str(resp).strip()
    if not resp:
        return True

    invalid_keywords = [
        "[A:]", "[/SOLUTION]", "[ASS]", "[A]:", "[/CONV]", "[/CODE]", "[/USER]",
        "[OUTPUT]", "[CC]", "[OUT]", "[/DATA]", "[/ASST]", "[/ME]", "[/REST]",
        "[RESP]", "<[user]>", "<|user|>", "[Response]", "[Assistant]", "[Answer]",
        "[End of Response]", "<human>", "<|endoftext|>", "<<", "[INST]", "[/INST]",
        "[]", "[USER]"
    ]
    tokens = resp.split()
    if all(token.strip() in invalid_keywords for token in tokens):
        return True
    tag_like_count = sum(1 for t in tokens if (t.startswith("[") and t.endswith("]")) or (t.startswith("<") and t.endswith(">")))
    if len(tokens) > 0 and tag_like_count / len(tokens) > 0.8:
        return True
    return False

# === 构造输入 prompt ===
def build_input(prompt, v1, v2):
    h = int(np.round(v1 * 100))
    v = int(np.round(v2 * 100))
    sys_instruction = f"You are a helpful assistant. Your response should maximize weighted rating = helpfulness*{h} + verbosity*{v}."
    return [{"role": "system", "content": sys_instruction}, {"role": "user", "content": prompt}]

# === 用模型重新生成 response ===
def regenerate_response(prompt, v1, v2):
    try:
        messages = build_input(prompt, v1, v2)
        input_ids = tokenizer.apply_chat_template(
            messages, add_generation_prompt=True, return_tensors="pt"
        ).to(device)

        if input_ids.ndim != 2 or input_ids.shape[1] == 0:
            print(f"⚠️ Invalid input_ids shape {input_ids.shape}, skipping: {prompt[:60]}")
            return ""

        outputs = model.generate(
            input_ids=input_ids,
            temperature=0.7,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
        generated = outputs[0][input_ids.shape[1]:]
        return tokenizer.decode(generated, skip_special_tokens=True).strip()

    except Exception as e:
        print(f"❌ Error: {e} \nPrompt: {prompt[:60]}")
        return ""

# === 修复单个 CSV 文件 ===
def fix_csv_responses(csv_path, save_dir):
    print(f"\n📂 正在检查: {csv_path}")

    if not os.path.exists(csv_path):
        print(f"❌ 文件不存在: {csv_path}")
        return

    df = pd.read_csv(csv_path)
    if "v1_p" not in df.columns or "v2_p" not in df.columns:
        print(f"❌ 缺少 v1_p/v2_p 列，跳过: {csv_path}")
        return

    bad_mask = df["response"].apply(is_invalid_response)
    num_bad = bad_mask.sum()

    if num_bad == 0:
        print("✅ 无需修复，跳过。")
        return

    print(f"🔧 发现 {num_bad} 条无效 response，开始重生成...")
    for i in tqdm(bad_mask[bad_mask].index, total=num_bad, desc=os.path.basename(csv_path), dynamic_ncols=True):
        row = df.loc[i]
        new_response = regenerate_response(row["prompt"], row["v1_p"], row["v2_p"])
        df.at[i, "response"] = new_response

    os.makedirs(save_dir, exist_ok=True)
    file_name = os.path.basename(csv_path).replace(".csv", "_fixed.csv")
    save_path = os.path.join(save_dir, file_name)
    df.to_csv(save_path, index=False)
    print(f"✅ 修复完成 → 已保存: {save_path}")

# === 主逻辑：修复所有 CSV ===
input_dir = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v10"
output_dir = os.path.join(input_dir, "fixed")

for file in os.listdir(input_dir):
    if file.endswith(".csv") and not file.endswith("_fixed.csv"):
        fix_csv_responses(os.path.join(input_dir, file), output_dir)

#10 fdiv 打分

# === 导入库 ===
import os
import time
import pandas as pd
import numpy as np
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
from tqdm.auto import tqdm
from google.colab import drive

# === 挂载 Google Drive ===
drive.mount('/content/drive')

# === 设置路径 ===
input_dir = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v10/fixed"
output_dir = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v10/scored"
os.makedirs(output_dir, exist_ok=True)

# === 加载 Reward Model ===
print("🤖 Loading Reward Model...")
device = "cuda" if torch.cuda.is_available() else "cpu"
rm = AutoModelForSequenceClassification.from_pretrained(
    "Haoxiang-Wang/RewardModel-Mistral-7B-for-DPA-v1", trust_remote_code=True
).to(device)
tokenizer = AutoTokenizer.from_pretrained("Haoxiang-Wang/RewardModel-Mistral-7B-for-DPA-v1")

# === 构造输入模板并打分 ===
def score_response(prompt, response):
    template = "[INST] You must read the following conversation carefully and rate the assistant's response from score 0-100 in these aspects: helpfulness, correctness, coherence, honesty, complexity, verbosity\n\nUser: {prompt}\n\nAssistant: {response} [/INST]"
    inputs = tokenizer(template.format(prompt=prompt, response=response), return_tensors="pt").to(device)
    with torch.no_grad():
        logits = rm(**inputs).logits.squeeze().cpu().numpy()
    return logits[9], logits[4]  # helpfulness, verbosity

# === 打分整个 CSV 文件 ===
def score_file(file_path):
    print(f"\n📂 Scoring file: {file_path}")
    df = pd.read_csv(file_path)
    scores = []

    for i, row in tqdm(df.iterrows(), total=len(df), desc=os.path.basename(file_path), dynamic_ncols=True):
        try:
            h, v = score_response(row["prompt"], row["response"])
        except Exception as e:
            print(f"⚠️ Error on row {i}: {e}")
            h, v = None, None
        scores.append({"helpfulness": h, "verbosity": v})

    df_scores = pd.DataFrame(scores)
    df_all = pd.concat([df, df_scores], axis=1)

    file_name = os.path.basename(file_path).replace(".csv", "_scored.csv")
    save_path = os.path.join(output_dir, file_name)
    df_all.to_csv(save_path, index=False)
    print(f"✅ Scoring done → Saved to: {save_path}")
    return df_all

# === 批量打分目录中的所有 _fixed.csv 文件 ===
for file in os.listdir(input_dir):
    if file.endswith("_fixed.csv"):
        score_file(os.path.join(input_dir, file))

#选出f_div v10 best response

from google.colab import drive
drive.mount('/content/drive')

import os
import pandas as pd

# === 设置路径（改为 Colab 下的绝对路径）===
input_dir = "/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v10/scored"
output_file = os.path.join(input_dir, "fdiv_v10_all_best_response.csv")

# === 主方向设置（v10）
main_v1 = 0.7071
main_v2 = 0.7071

# === 合并所有 scored 文件
dfs = []
for file in os.listdir(input_dir):
    if file.endswith("_scored.csv"):
        df = pd.read_csv(os.path.join(input_dir, file))
        df["main_v1"] = main_v1
        df["main_v2"] = main_v2
        df["score_total"] = df["main_v1"] * df["helpfulness"] + df["main_v2"] * df["verbosity"]
        dfs.append(df)

df_all = pd.concat(dfs, ignore_index=True)
df_best = df_all.loc[df_all.groupby("prompt_id")["score_total"].idxmax()]
df_best = df_best.rename(columns={"response": "f_best_response"})

# === 保留字段
cols_to_keep = [
    "prompt_id", "prompt", "f_best_response",
    "v1_p", "v2_p", "valid_angle",
    "main_v1", "main_v2",
    "helpfulness", "verbosity", "score_total"
]
df_best_clean = df_best[cols_to_keep]

# === 保存结果
df_best_clean.to_csv(output_file, index=False)
print(f"✅ 已保存到: {output_file}")

#baseline v10 打分（helpfulness,verbosity)

# === 导入库 ===
import os
import time
import pandas as pd
import numpy as np
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
from tqdm.auto import tqdm
from google.colab import drive

# === 挂载 Google Drive ===
drive.mount('/content/drive')

# === 设置输入输出路径 ===
input_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/fixed_batch/batch_v10_fixed.csv"
output_dir = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch"
os.makedirs(output_dir, exist_ok=True)

# === 加载数据 ===
print(f"📦 Loading file: {input_path}")
df = pd.read_csv(input_path)

# === 加载 Reward Model ===
print("🤖 Loading Reward Model...")
device = "cuda" if torch.cuda.is_available() else "cpu"
rm = AutoModelForSequenceClassification.from_pretrained(
    "Haoxiang-Wang/RewardModel-Mistral-7B-for-DPA-v1", trust_remote_code=True
).to(device)
tokenizer = AutoTokenizer.from_pretrained("Haoxiang-Wang/RewardModel-Mistral-7B-for-DPA-v1")

def score_response(prompt, response):
    template = "[INST] You must read the following conversation carefully and rate the assistant's response from score 0-100 in these aspects: helpfulness, correctness, coherence, honesty, complexity, verbosity\n\nUser: {prompt}\n\nAssistant: {response} [/INST]"
    inputs = tokenizer(template.format(prompt=prompt, response=response), return_tensors="pt").to(device)
    with torch.no_grad():
        logits = rm(**inputs).logits.squeeze().cpu().numpy()
    return logits[9], logits[4]  # helpfulness, verbosity

# === 开始打分 ===
print("🧠 Scoring responses...")
start_time = time.time()
scores = []

for i, row in tqdm(df.iterrows(), total=len(df), desc="🔍 Scoring"):
    try:
        h, v = score_response(row["prompt"], row["response"])
        scores.append({"v1": row["v1"], "v2": row["v2"], "helpfulness": h, "verbosity": v})
    except Exception as e:
        print(f"⚠️ Error on row {i}: {e}")
        scores.append({"v1": row["v1"], "v2": row["v2"], "helpfulness": None, "verbosity": None})

df_scores = pd.DataFrame(scores)
df_all = pd.concat([df, df_scores], axis=1)

# === 保存结果 ===
output_file = os.path.join(output_dir, os.path.basename(input_path).replace(".csv", "_scored.csv"))
df_all.to_csv(output_file, index=False)

end_time = time.time()
print(f"\n✅ Scoring complete! Total scored: {len(df_all)}")
print(f"🕒 Time elapsed: {end_time - start_time:.1f} seconds")
print(f"📁 Saved to: {output_file}")

#选出两组最好的答案

import pandas as pd
import os

# === 设置输入文件路径 ===
input_file = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch/batch_v10_fixed_scored.csv"

# === 设置输出路径 ===
output_dir = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch"
output_file_with_total = os.path.join(output_dir, "batch_v10_fixed_scored_with_total.csv")

# best response 输出目录和文件名
best_response_dir = os.path.join(output_dir, "best_response")
os.makedirs(best_response_dir, exist_ok=True)
output_best_file = os.path.join(best_response_dir, "best_response_v10_baseline.csv")

# === 加载已有评分文件 ===
df = pd.read_csv(input_file)

# === 计算总分 ===
df["total_score"] = df["v1"] * df["helpfulness"] + df["v2"] * df["verbosity"]

# === 保存含总分的新文件 ===
df.to_csv(output_file_with_total, index=False)
print(f"✅ Saved file with total_score: {output_file_with_total}")

# === 选出每个 prompt_id 下 total_score 最高的 response ===
df_best = df.loc[df.groupby("prompt_id")["total_score"].idxmax()].copy()

# 可选：将 response 列重命名为 best_response
df_best = df_best.rename(columns={"response": "best_response"})

# === 保存每个 prompt_id 的最佳回答到指定路径 ===
df_best.to_csv(output_best_file, index=False)
print(f"🏆 Best responses per prompt saved to: {output_best_file}")

import pandas as pd

# === 路径设置 ===
baseline_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch/best_response/best_response_v10_baseline.csv"
fdiv_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v10/scored/fdiv_v10_all_best_response.csv"

# === 加载数据 ===
df_baseline = pd.read_csv(baseline_path)
df_fdiv = pd.read_csv(fdiv_path)

# === 合并数据（按 prompt_id）
df_merged = pd.merge(
    df_baseline,
    df_fdiv,
    on="prompt_id",
    suffixes=("_baseline", "_fdiv")
)

# === 字段重命名 ===
df_merged = df_merged.rename(columns={
    "prompt_baseline": "prompt",
    "best_response": "baseline_best_response",
    "score_total": "f_score_total",                # 来自 f-div
    "total_score": "baseline_score_total"          # 来自 baseline
})

# === 保留字段，包括 baseline 的 response_id
cols_to_show = [
    "prompt_id", "prompt",
    "baseline_best_response", "f_best_response",
    "baseline_score_total", "f_score_total",
    "response_id",                       # ✅ 加上 response_id
    "v1_p", "v2_p", "valid_angle",
    "main_v1", "main_v2"
]
df_clean = df_merged[cols_to_show]

# === 保存结果 ===
output_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/v10_final_merged_for_judge.csv"
df_clean.to_csv(output_path, index=False)

print(f"✅ 合并完成，文件保存至：{output_path}")

#gpt4 对v10打分

import pandas as pd
import json
import os
import random

# === 设置路径 ===
input_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/v10_final_merged_for_judge.csv"
output_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/gpt4_judge_inputs/v10_pairwise_randomized.jsonl"

# === 读取合并数据 ===
df = pd.read_csv(input_path)

# === 设置随机种子（可选）===
random.seed(42)

# === 构造单轮随机化 JSONL ===
output = []
tie_count = 0

for idx, row in df.iterrows():
    prompt = row["prompt"]
    baseline = row["baseline_best_response"]
    fdiv = row["f_best_response"]
    pid = row["prompt_id"]
    baseline_id = row["response_id"]

    # === 缺失值或非字符串处理 ===
    if not isinstance(baseline, str):
        baseline = str(baseline) if pd.notna(baseline) else ""
    if not isinstance(fdiv, str):
        fdiv = str(fdiv) if pd.notna(fdiv) else ""

    # 如果两响应完全一样，自动 Tie
    if baseline.strip() == fdiv.strip():
        output.append({
            "pair_id": idx,
            "prompt_id": pid,
            "auto_result": "Tie",
            "baseline_id": baseline_id,
            "a_origin": "baseline",
            "b_origin": "f-div",
            "formatted_prompt": "[HH-RLHF]: (Responses identical, auto tie)"
        })
        tie_count += 1
        continue

    # 随机打乱顺序
    if random.random() < 0.5:
        response_a, response_b = baseline, fdiv
        a_origin, b_origin = "baseline", "f-div"
    else:
        response_a, response_b = fdiv, baseline
        a_origin, b_origin = "f-div", "baseline"

    full_prompt = f"""[HH-RLHF]: For the following query to a chatbot, which response is more helpful?

Query: {prompt}

Response A: {response_a}

Response B: {response_b}

FIRST provide a one-sentence comparison of the two responses and explain which you feel is more helpful.
SECOND, on a new line, state only 'A' or 'B' to indicate which response is more helpful.

Format:
Comparison: ...
More helpful: A/B"""

    output.append({
        "pair_id": idx,
        "prompt_id": pid,
        "auto_result": None,
        "baseline_id": baseline_id,
        "a_origin": a_origin,
        "b_origin": b_origin,
        "formatted_prompt": full_prompt
    })

# === 保存为 JSONL ===
with open(output_path, "w", encoding="utf-8") as f:
    for item in output:
        f.write(json.dumps(item, ensure_ascii=False) + "\n")

print(f"✅ 已构造 JSONL，共 {len(output)} 条，自动 Tie: {tie_count}")
print(f"📁 保存至：{output_path}")

import openai
import os

# 设置 API Key 和代理地址（如果使用代理）
os.environ["OPENAI_API_KEY"] = "sk-XGGe5y0ZvLcQVFp6XnRizs7q47gsVnAbZx0Xr2mfcVlbr99f"
openai.api_key = os.environ["OPENAI_API_KEY"]
openai.api_base = "https://api2.aigcbest.top/v1"  # 如果用代理就改这里

print("✅ OpenAI 设置完成")

import openai
import json
import time
import os
from tqdm import tqdm

# === 参数配置 ===
input_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/gpt4_judge_inputs/v10_pairwise_randomized.jsonl"
output_path = input_path.replace(".jsonl", "_results.jsonl")
model = "gpt-4o"
sleep_time = 1.0
max_retries = 3

# === 加载输入数据 ===
with open(input_path, "r", encoding="utf-8") as f:
    all_prompts = [json.loads(line) for line in f]

# === 读取已完成项（支持断点续跑）===
completed_ids = set()
results = []
if os.path.exists(output_path):
    with open(output_path, "r", encoding="utf-8") as f:
        for line in f:
            item = json.loads(line)
            results.append(item)
            completed_ids.add(item["pair_id"])
    print(f"🔁 已加载 {len(completed_ids)} 条历史结果，跳过")

# === 开始评估 ===
start_time = time.time()

for item in tqdm(all_prompts, desc="🧠 GPT-4o 评估中"):
    pid = item["pair_id"]
    if pid in completed_ids:
        continue

    if item.get("auto_result") == "Tie":
        item["gpt_judgment"] = "Tie"
        print(f"🤝 pair_id={pid} → Auto-Tie")
        results.append(item)
        continue

    prompt = item["formatted_prompt"]
    for attempt in range(max_retries):
        try:
            response = openai.ChatCompletion.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.0,
            )
            reply = response.choices[0].message["content"].strip()
            item["gpt_raw_response"] = reply

            last_line = reply.strip().splitlines()[-1].strip().upper()
            if "MORE HELPFUL: A" in last_line or last_line == "A":
                item["gpt_judgment"] = "A"
            elif "MORE HELPFUL: B" in last_line or last_line == "B":
                item["gpt_judgment"] = "B"
            else:
                item["gpt_judgment"] = "Unclear"

            print(f"✅ pair_id={pid} → {item['gpt_judgment']}")
            break
        except Exception as e:
            item["error"] = str(e)
            print(f"❌ pair_id={pid} → Error: {str(e)}")
            time.sleep(sleep_time)
    else:
        item["gpt_judgment"] = "Error"
        print(f"❌ pair_id={pid} → Failed after {max_retries} attempts")

    results.append(item)
    time.sleep(sleep_time)

# === 保存评估结果 ===
with open(output_path, "w", encoding="utf-8") as f:
    for r in results:
        f.write(json.dumps(r, ensure_ascii=False) + "\n")

end_time = time.time()
print(f"\n✅ 评估完成，共计 {len(results)} 条")
print(f"🕒 耗时：{end_time - start_time:.1f} 秒")
print(f"📁 输出结果路径：{output_path}")

import json
from collections import Counter
import pandas as pd
import matplotlib.pyplot as plt

# === 替换为你的结果文件路径 ===
results_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/gpt4_judge_inputs/v10_pairwise_randomized_results.jsonl"

# === 加载数据 ===
with open(results_path, "r", encoding="utf-8") as f:
    data = [json.loads(line) for line in f]

# === 统计核心函数 ===
def judge_winner(item):
    judgment = item.get("gpt_judgment", "")
    a_origin = item.get("a_origin")
    b_origin = item.get("b_origin")

    if judgment == "Tie":
        return "Tie"
    elif judgment == "A":
        return a_origin  # 谁在A位胜
    elif judgment == "B":
        return b_origin  # 谁在B位胜
    elif judgment == "Unclear":
        return "Unclear"
    elif judgment == "Error":
        return "Error"
    else:
        return "Invalid"

# === 应用统计 ===
outcomes = [judge_winner(item) for item in data]
counter = Counter(outcomes)

# === 整理为 DataFrame 展示 ===
total = sum(counter.values())
win_fdiv = counter.get("f-div", 0)
win_baseline = counter.get("baseline", 0)
tie = counter.get("Tie", 0)
unclear = counter.get("Unclear", 0)
error = counter.get("Error", 0)

summary = pd.DataFrame([{
    "Total": total,
    "f-div wins": win_fdiv,
    "baseline wins": win_baseline,
    "Ties": tie,
    "Unclear": unclear,
    "Error": error,
    "f-div win rate (%)": round(win_fdiv / total * 100, 2),
    "baseline win rate (%)": round(win_baseline / total * 100, 2),
    "Tie rate (%)": round(tie / total * 100, 2),
    "Unclear/Error rate (%)": round((unclear + error) / total * 100, 2)
}])

# === 打印结果 ===
print(summary)

# === 可视化胜率条形图 ===
summary_plot = {
    "f-div": win_fdiv,
    "baseline": win_baseline,
    "Tie": tie,
    "Unclear": unclear,
    "Error": error
}

# === 带百分比标注的可视化 ===
plt.figure(figsize=(8, 4))
bars = plt.bar(summary_plot.keys(), summary_plot.values(), color=["#2ca02c", "#1f77b4", "#ff7f0e", "#999999", "#d62728"])

plt.title("GPT-4o Judged Outcome (Randomized A/B)", fontsize=14)
plt.ylabel("Count")
plt.xticks(fontsize=10)
plt.grid(axis="y", linestyle="--", alpha=0.5)

# ✅ 在每个柱子上方标注百分比
for bar, label in zip(bars, summary_plot.keys()):
    count = bar.get_height()
    pct = count / total * 100
    plt.text(bar.get_x() + bar.get_width()/2, count + 2, f"{pct:.1f}%", ha="center", va="bottom", fontsize=10)

plt.tight_layout()
plt.show()

#v1 f-div

import numpy as np

v_main = np.array([1,0])  # 主方向 v₁₀ = 0°
angle_offsets = [-60, -45, -30, -15, 0, 15, 30, 45, 60]

perturbed_vs = []
perturbed_angles = []

for offset in angle_offsets:
    angle_deg = 0 + offset  # 围绕 0° 做扰动
    angle_rad = np.radians(angle_deg)
    v = np.array([np.cos(angle_rad), np.sin(angle_rad)])
    perturbed_vs.append(v)
    perturbed_angles.append(angle_deg)

def kl_divergence(p, q):
    p = np.abs(p) / np.sum(np.abs(p))  # 归一化成伪概率分布
    q = np.abs(q) / np.sum(np.abs(q))
    return np.sum(p * np.log(p / (q + 1e-12)))  # 防止除零

delta = 0.1 # KL 散度容差
kl_values = []

print(perturbed_vs)

for v in perturbed_vs:
    kl = kl_divergence(v, v_main)
    kl_values.append(kl)

# ✅ 筛选方向，KL ≤ δ
valid_vs = []
valid_angles = []

for v, angle, kl in zip(perturbed_vs, perturbed_angles, kl_values):
    if kl <= delta:
        valid_vs.append(v)
        valid_angles.append(angle)



print(valid_vs)
print(valid_angles)

import numpy as np

v_main = np.array([(0.9962, 0.0872) ])  # 主方向 v₁₀ = 0°
angle_offsets = [-60, -45, -30, -15, 15, 30, 45, 60]

perturbed_vs = []
perturbed_angles = []

for offset in angle_offsets:
    angle_deg = 5 + offset  # 围绕 0° 做扰动
    angle_rad = np.radians(angle_deg)
    v = np.array([np.cos(angle_rad), np.sin(angle_rad)])
    perturbed_vs.append(v)
    perturbed_angles.append(angle_deg)

def kl_divergence(p, q):
    p = np.abs(p) / np.sum(np.abs(p))  # 归一化成伪概率分布
    q = np.abs(q) / np.sum(np.abs(q))
    return np.sum(p * np.log(p / (q + 1e-12)))  # 防止除零

delta = 0.1 # KL 散度容差
kl_values = []

print(perturbed_vs)

for v in perturbed_vs:
    kl = kl_divergence(v, v_main)
    kl_values.append(kl)

# ✅ 筛选方向，KL ≤ δ
valid_vs = []
valid_angles = []

for v, angle, kl in zip(perturbed_vs, perturbed_angles, kl_values):
    if kl <= delta:
        valid_vs.append(v)
        valid_angles.append(angle)



print(valid_vs)
print(valid_angles)

import numpy as np

v_main = np.array([(0.9962, 0.0872) ])  # 主方向 v₁₀ = 0°
angle_offsets = [-60, -45, -30, -15, 15, 30, 45, 60]

perturbed_vs = []
perturbed_angles = []

for offset in angle_offsets:
    angle_deg =  + offset  # 围绕 0° 做扰动
    angle_rad = np.radians(angle_deg)
    v = np.array([np.cos(angle_rad), np.sin(angle_rad)])
    perturbed_vs.append(v)
    perturbed_angles.append(angle_deg)

def kl_divergence(p, q):
    p = np.abs(p) / np.sum(np.abs(p))  # 归一化成伪概率分布
    q = np.abs(q) / np.sum(np.abs(q))
    return np.sum(p * np.log(p / (q + 1e-12)))  # 防止除零

delta = 0.1 # KL 散度容差
kl_values = []

print(perturbed_vs)

for v in perturbed_vs:
    kl = kl_divergence(v, v_main)
    kl_values.append(kl)

# ✅ 筛选方向，KL ≤ δ
valid_vs = []
valid_angles = []

for v, angle, kl in zip(perturbed_vs, perturbed_angles, kl_values):
    if kl <= delta:
        valid_vs.append(v)
        valid_angles.append(angle)



print(valid_vs)
print(valid_angles)















#Test-time Preference Shift

#Step 1｜定义多个扰动方向

import numpy as np

# 设置主方向角度（v10 = 45°）
main_angle = 45
offsets = [-30, -20, -10, 0, 10, 20, 30]

# 构造偏移角度对应的方向向量和角度记录
valid_vs = []
valid_angles = []

for offset in offsets:
    angle = main_angle + offset
    rad = np.radians(angle)
    v = np.array([np.cos(rad), np.sin(rad)])
    valid_vs.append(v)
    valid_angles.append(angle)

print(valid_vs)
print(valid_angles)

#Step 2｜使用已有脚本生成每个角度方向下的响应

from google.colab import drive
drive.mount('/content/drive')

import os
import numpy as np
import pandas as pd
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM
from tqdm.auto import tqdm
import torch
import time
import random

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
seed = 42
np.random.seed(seed)
random.seed(seed)
torch.manual_seed(seed)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(seed)

# ✅ STEP 2: 保存路径
result_dir = "/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/v10_direction_sensitivity"
os.makedirs(result_dir, exist_ok=True)


# ✅ STEP 3: 加载前 500 条 UltraFeedback prompt
ds = load_dataset("HuggingFaceH4/ultrafeedback_binarized", split="test_prefs")
prompts = ds["prompt"][:100]
prompt_ids = list(range(100))

# ✅ STEP 4: 加载 DPA 模型和 tokenizer
model = AutoModelForCausalLM.from_pretrained(
    "Haoxiang-Wang/DPA-v1-Mistral-7B",
    torch_dtype=torch.float16,
    device_map="auto"
).to(device)

tokenizer = AutoTokenizer.from_pretrained("Haoxiang-Wang/RewardModel-Mistral-7B-for-DPA-v1")
tokenizer.padding_side = "left"
if tokenizer.pad_token_id is None:
    tokenizer.pad_token = tokenizer.eos_token

# ✅ STEP 5: 构造 DPA Prompt
def build_input(prompt, v1, v2):
    h = int(np.round(v1 * 100))
    v = int(np.round(v2 * 100))
    sys_instruction = f"You are a helpful assistant. Your response should maximize weighted rating = helpfulness*{h} + verbosity*{v}."
    return [{"role": "user", "content": f"{sys_instruction}\n\n{prompt}"}]

# ✅ STEP 6: 批量生成 response
def generate_response_batch(prompts_batch, prompt_ids_batch, v1, v2):
    input_ids_list = []

    for prompt in prompts_batch:
        messages = build_input(prompt, v1, v2)
        input_ids = tokenizer.apply_chat_template(
            messages, add_generation_prompt=True, return_tensors="pt"
        )[0]
        input_ids_list.append(input_ids.to(device))

    input_ids_padded = torch.nn.utils.rnn.pad_sequence(
        input_ids_list, batch_first=True, padding_value=tokenizer.pad_token_id
    ).to(device)

    attention_mask = (input_ids_padded != tokenizer.pad_token_id).to(device)
    max_input_len = input_ids_padded.shape[1]
    max_new_tokens = min(2048, 4096 - max_input_len)

    with torch.no_grad():
        outputs = model.generate(
            input_ids=input_ids_padded,
            attention_mask=attention_mask,
            max_new_tokens=max_new_tokens,
            temperature=0.7,
            do_sample=True,
            num_return_sequences=1,
            pad_token_id=tokenizer.eos_token_id
        )

    responses = []
    for i, input_ids in enumerate(input_ids_list):
        generated_tokens = outputs[i][input_ids.shape[0]:]
        decoded = tokenizer.decode(generated_tokens, skip_special_tokens=True)
        responses.append({
            "prompt_id": prompt_ids_batch[i],
            "prompt": prompts_batch[i],
            "response": decoded
        })
    return responses

# ✅ STEP 7: 你自己的 valid_vs 和 valid_angles（请替换为你的真实值）
valid_vs = [
    np.array([0.8660, 0.5000]),  # 角度 ≈ 30°
    np.array([0.5000, 0.8660])   # 角度 ≈ 60°
]

valid_angles = [30, 60]  # 与 valid_vs 顺序一致

# ✅ 主方向（v10）
main_v1 = 0.7071
main_v2 = 0.7071
main_angle = 45  # ✅ 补上这个定义


# ✅ STEP 7: 定义扰动方向（±30°, ±20°, ±10°, 0°）
# 上面 valid_vs 和 valid_angles 已准备好
# ✅ STEP 7: 定义扰动方向（±30°, ±20°, ±10°, 0°, +10°, +20°, +30°）
valid_vs = [
    np.array([0.96592583, 0.25881905]),  # 15°
    np.array([0.90630779, 0.42261826]),  # 25°
    np.array([0.81915204, 0.57357644]),  # 35°
    np.array([0.70710678, 0.70710678]),  # 45° (主方向)
    np.array([0.57357644, 0.81915204]),  # 55°
    np.array([0.42261826, 0.90630779]),  # 65°
    np.array([0.25881905, 0.96592583])   # 75°
]
valid_angles = [15, 25, 35, 45, 55, 65, 75]


# ✅ STEP 8: 生成响应
batch_size = 8
for i, (v_vec, angle_deg) in enumerate(zip(valid_vs, valid_angles)):
    v1, v2 = v_vec[0], v_vec[1]
    output_file = os.path.join(result_dir, f"v10_angle_{int(angle_deg)}.csv")
    print(f"\n🚀 Generating for direction {i}: angle = {angle_deg}°, v = ({v1:.4f}, {v2:.4f})")

    if os.path.exists(output_file):
        existing_df = pd.read_csv(output_file)
        done_prompt_ids = set(existing_df["prompt_id"].unique())
        results = existing_df.to_dict("records")
        print(f"🔁 Resuming from previous run: {len(done_prompt_ids)} prompts already completed.")
    else:
        done_prompt_ids = set()
        results = []

    for start in tqdm(range(0, len(prompts), batch_size), desc=f"Generating angle {angle_deg}°"):
        end = min(start + batch_size, len(prompts))
        batch_prompts_all = prompts[start:end]
        batch_ids_all = prompt_ids[start:end]

        unprocessed_indices = [j for j, pid in enumerate(batch_ids_all) if pid not in done_prompt_ids]
        if not unprocessed_indices:
            continue

        batch_prompts = [batch_prompts_all[j] for j in unprocessed_indices]
        batch_ids = [batch_ids_all[j] for j in unprocessed_indices]

        try:
            batch_outputs = generate_response_batch(batch_prompts, batch_ids, v1, v2)
            for item in batch_outputs:
                item.update({
                    "v1_p": round(v1, 4),
                    "v2_p": round(v2, 4),
                    "direction_index": i,
                    "angle_deg": round(angle_deg, 1),
                    "main_v1": round(np.cos(np.radians(main_angle)), 4),
                    "main_v2": round(np.sin(np.radians(main_angle)), 4)
                })
                results.append(item)

            pd.DataFrame(batch_outputs).to_csv(
                output_file, mode='a', index=False,
                header=not os.path.exists(output_file)
            )

        except Exception as e:
            print(f"⚠️ Error at batch {start}-{end}: {e}")

    print(f"✅ Final saved {len(results)} responses to {output_file}")

import os
import pandas as pd
import numpy as np
import torch
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForCausalLM
from accelerate import Accelerator

# === 模型准备 ===
device = "cuda" if torch.cuda.is_available() else "cpu"
accelerator = Accelerator()

model = AutoModelForCausalLM.from_pretrained(
    "Haoxiang-Wang/DPA-v1-Mistral-7B",
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
    device_map="auto" if torch.cuda.is_available() else None
)
model = accelerator.prepare(model)

tokenizer = AutoTokenizer.from_pretrained("Haoxiang-Wang/DPA-v1-Mistral-7B")
tokenizer.padding_side = "left"
if tokenizer.pad_token_id is None:
    tokenizer.pad_token = tokenizer.eos_token

# === 判断是否无效 response ===
def is_invalid_response(resp):
    if pd.isna(resp):
        return True
    resp = str(resp).strip()
    if not resp:
        return True

    invalid_keywords = [
        "[A:]", "[/SOLUTION]", "[ASS]", "[A]:", "[/CONV]", "[/CODE]", "[/USER]",
        "[OUTPUT]", "[CC]", "[OUT]", "[/DATA]", "[/ASST]", "[/ME]", "[/REST]",
        "[RESP]", "<[user]>", "<|user|>", "[Response]", "[Assistant]", "[Answer]",
        "[End of Response]", "<human>", "<|endoftext|>", "<<", "[INST]", "[/INST]",
        "[]", "[USER]"
    ]
    tokens = resp.split()
    if all(token.strip() in invalid_keywords for token in tokens):
        return True
    tag_like_count = sum(1 for t in tokens if (t.startswith("[") and t.endswith("]")) or (t.startswith("<") and t.endswith(">")))
    if len(tokens) > 0 and tag_like_count / len(tokens) > 0.8:
        return True
    return False

# === 构造输入 prompt ===
def build_input(prompt, v1, v2):
    h = int(np.round(v1 * 100))
    v = int(np.round(v2 * 100))
    sys_instruction = f"You are a helpful assistant. Your response should maximize weighted rating = helpfulness*{h} + verbosity*{v}."
    return [{"role": "system", "content": sys_instruction}, {"role": "user", "content": prompt}]

# === 用模型重新生成 response ===
def regenerate_response(prompt, v1, v2):
    try:
        messages = build_input(prompt, v1, v2)
        input_ids = tokenizer.apply_chat_template(
            messages, add_generation_prompt=True, return_tensors="pt"
        ).to(device)

        if input_ids.ndim != 2 or input_ids.shape[1] == 0:
            print(f"⚠️ Invalid input_ids shape {input_ids.shape}, skipping: {prompt[:60]}")
            return ""

        outputs = model.generate(
            input_ids=input_ids,
            temperature=0.7,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
        generated = outputs[0][input_ids.shape[1]:]
        return tokenizer.decode(generated, skip_special_tokens=True).strip()

    except Exception as e:
        print(f"❌ Error: {e} \nPrompt: {prompt[:60]}")
        return ""

# === 修复单个 CSV 文件 ===
def fix_csv_responses(csv_path, save_dir):
    print(f"\n📂 正在检查: {csv_path}")

    if not os.path.exists(csv_path):
        print(f"❌ 文件不存在: {csv_path}")
        return

    df = pd.read_csv(csv_path)
    if "v1_p" not in df.columns or "v2_p" not in df.columns:
        print(f"❌ 缺少 v1_p/v2_p 列，跳过: {csv_path}")
        return

    bad_mask = df["response"].apply(is_invalid_response)
    num_bad = bad_mask.sum()

    if num_bad == 0:
        print("✅ 无需修复，跳过。")
        return

    print(f"🔧 发现 {num_bad} 条无效 response，开始重生成...")
    for i in tqdm(bad_mask[bad_mask].index, total=num_bad, desc=os.path.basename(csv_path), dynamic_ncols=True):
        row = df.loc[i]
        new_response = regenerate_response(row["prompt"], row["v1_p"], row["v2_p"])
        df.at[i, "response"] = new_response

    os.makedirs(save_dir, exist_ok=True)
    file_name = os.path.basename(csv_path).replace(".csv", "_fixed.csv")
    save_path = os.path.join(save_dir, file_name)
    df.to_csv(save_path, index=False)
    print(f"✅ 修复完成 → 已保存: {save_path}")

# === 主逻辑：修复所有 CSV ===
input_dir = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/v10_direction_sensitivity"
output_dir = os.path.join(input_dir, "fixed")

for file in os.listdir(input_dir):
    if file.endswith(".csv") and not file.endswith("_fixed.csv"):
        fix_csv_responses(os.path.join(input_dir, file), output_dir)

import pandas as pd
import os

# 结果目录
result_dir = "/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/v10_direction_sensitivity/fixed"
merged_path = os.path.join(result_dir, "merged_all_angles.csv")

# 合并所有 CSV 文件
all_data = []
for fname in os.listdir(result_dir):
    if fname.endswith(".csv") and not fname.startswith(".") and "merged" not in fname:
        fpath = os.path.join(result_dir, fname)
        try:
            df = pd.read_csv(fpath)
            all_data.append(df)
            print(f"📄 Loaded {fname} with {len(df)} rows.")
        except Exception as e:
            print(f"⚠️ Failed to load {fname}: {e}")

if all_data:
    df_all = pd.concat(all_data, ignore_index=True)
    df_all.to_csv(merged_path, index=False)
    print(f"\n✅ Merged total {len(df_all)} responses.")
    print(f"📁 Saved to: {merged_path}")
else:
    print("❌ No valid CSV files found to merge.")

import os
import time
import pandas as pd
import numpy as np
import torch
from tqdm.auto import tqdm
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from google.colab import drive

# === 挂载 Google Drive ===
drive.mount('/content/drive')

# === 设置路径 ===
input_path = "/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/v10_direction_sensitivity/fixed/merged_all_angles.csv"
output_path = "/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/v10_direction_sensitivity/merged_all_angles_scored.csv"

# === 加载合并数据 ===
df = pd.read_csv(input_path)
print(f"📦 Loaded {len(df)} samples")

# === 加载 Reward Model ===
print("🤖 Loading Reward Model...")
device = "cuda" if torch.cuda.is_available() else "cpu"
rm = AutoModelForSequenceClassification.from_pretrained(
    "Haoxiang-Wang/RewardModel-Mistral-7B-for-DPA-v1", trust_remote_code=True
).to(device)
tokenizer = AutoTokenizer.from_pretrained("Haoxiang-Wang/RewardModel-Mistral-7B-for-DPA-v1")

def score_response(prompt, response):
    template = "[INST] You must read the following conversation carefully and rate the assistant's response from score 0-100 in these aspects: helpfulness, correctness, coherence, honesty, complexity, verbosity\n\nUser: {prompt}\n\nAssistant: {response} [/INST]"
    inputs = tokenizer(template.format(prompt=prompt, response=response), return_tensors="pt").to(device)
    with torch.no_grad():
        logits = rm(**inputs).logits.squeeze().cpu().numpy()
    return logits[9], logits[4]  # helpfulness, verbosity

# === 批量打分 ===
help_scores, verb_scores = [], []

print("🧠 Scoring all responses...")
for i, row in tqdm(df.iterrows(), total=len(df)):
    try:
        h, v = score_response(row["prompt"], row["response"])
    except Exception as e:
        print(f"⚠️ Error on row {i}: {e}")
        h, v = None, None
    help_scores.append(h)
    verb_scores.append(v)

df["reward_score_helpfulness"] = help_scores
df["reward_score_verbosity"] = verb_scores

# === 保存打分结果 ===
df.to_csv(output_path, index=False)
print(f"✅ All scored results saved to {output_path}")

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# === 加载打分后的 CSV ===
input_path = "/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/v10_direction_sensitivity/merged_all_angles_scored.csv"
df = pd.read_csv(input_path)

# === 聚合不同方向的平均分 ===
df_grouped = df.groupby("angle_deg")[["reward_score_helpfulness", "reward_score_verbosity"]].agg(["mean", "std"]).reset_index()
df_grouped.columns = ["angle", "help_mean", "help_std", "verb_mean", "verb_std"]

# === 可视化：helpfulness 和 verbosity 随角度变化 ===
plt.figure(figsize=(10, 6))
plt.errorbar(df_grouped["angle"], df_grouped["help_mean"], yerr=df_grouped["help_std"], label="Helpfulness", fmt='-o')
plt.errorbar(df_grouped["angle"], df_grouped["verb_mean"], yerr=df_grouped["verb_std"], label="Verbosity", fmt='-o', color='orange')
plt.axvline(x=45, color='gray', linestyle='--', label="Main Direction (45°)")
plt.xlabel("Angle (degrees)")
plt.ylabel("Reward Score")
plt.title("Reward Score vs Direction Angle (±pertuation)")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

#v11

main_v1 = 0.9500
main_v2 = -0.3100
main_angle = np.degrees(np.arctan2(main_v2, main_v1))  # 可视化时用

angles = [-30, -20, -10, 0, 10, 20, 30]  # 相对主方向
valid_vs = []
valid_angles = []

for delta in angles:
    angle = np.arctan2(main_v2, main_v1) + np.radians(delta)
    v = np.array([np.cos(angle), np.sin(angle)])
    valid_vs.append(v)
    valid_angles.append(np.degrees(angle))  # 实际角度

print(valid_vs)
print(valid_angles)
print(main_angle)

#Step 2｜使用已有脚本生成每个角度方向下的响应

from google.colab import drive
drive.mount('/content/drive')

import os
import numpy as np
import pandas as pd
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM
from tqdm.auto import tqdm
import torch
import time
import random

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
seed = 42
np.random.seed(seed)
random.seed(seed)
torch.manual_seed(seed)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(seed)

# ✅ STEP 2: 保存路径
result_dir = "/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/v11_direction_sensitivity"
os.makedirs(result_dir, exist_ok=True)


# ✅ STEP 3: 加载前 500 条 UltraFeedback prompt
ds = load_dataset("HuggingFaceH4/ultrafeedback_binarized", split="test_prefs")
prompts = ds["prompt"][:100]
prompt_ids = list(range(100))

# ✅ STEP 4: 加载 DPA 模型和 tokenizer
model = AutoModelForCausalLM.from_pretrained(
    "Haoxiang-Wang/DPA-v1-Mistral-7B",
    torch_dtype=torch.float16,
    device_map="auto"
).to(device)

tokenizer = AutoTokenizer.from_pretrained("Haoxiang-Wang/RewardModel-Mistral-7B-for-DPA-v1")
tokenizer.padding_side = "left"
if tokenizer.pad_token_id is None:
    tokenizer.pad_token = tokenizer.eos_token

# ✅ STEP 5: 构造 DPA Prompt
def build_input(prompt, v1, v2):
    h = int(np.round(v1 * 100))
    v = int(np.round(v2 * 100))
    sys_instruction = f"You are a helpful assistant. Your response should maximize weighted rating = helpfulness*{h} + verbosity*{v}."
    return [{"role": "user", "content": f"{sys_instruction}\n\n{prompt}"}]

# ✅ STEP 6: 批量生成 response
def generate_response_batch(prompts_batch, prompt_ids_batch, v1, v2):
    input_ids_list = []

    for prompt in prompts_batch:
        messages = build_input(prompt, v1, v2)
        input_ids = tokenizer.apply_chat_template(
            messages, add_generation_prompt=True, return_tensors="pt"
        )[0]
        input_ids_list.append(input_ids.to(device))

    input_ids_padded = torch.nn.utils.rnn.pad_sequence(
        input_ids_list, batch_first=True, padding_value=tokenizer.pad_token_id
    ).to(device)

    attention_mask = (input_ids_padded != tokenizer.pad_token_id).to(device)
    max_input_len = input_ids_padded.shape[1]
    max_new_tokens = min(2048, 4096 - max_input_len)

    with torch.no_grad():
        outputs = model.generate(
            input_ids=input_ids_padded,
            attention_mask=attention_mask,
            max_new_tokens=max_new_tokens,
            temperature=0.7,
            do_sample=True,
            num_return_sequences=1,
            pad_token_id=tokenizer.eos_token_id
        )

    responses = []
    for i, input_ids in enumerate(input_ids_list):
        generated_tokens = outputs[i][input_ids.shape[0]:]
        decoded = tokenizer.decode(generated_tokens, skip_special_tokens=True)
        responses.append({
            "prompt_id": prompt_ids_batch[i],
            "prompt": prompts_batch[i],
            "response": decoded
        })
    return responses

# ✅ 主方向（v10）
main_v1 = 0.9500
main_v2 = -0.3100
main_angle = np.degrees(np.arctan2(main_v2, main_v1)) # ✅ 补上这个定义


# ✅ STEP 7: 定义扰动方向（±30°, ±20°, ±10°, 0°）
# 上面 valid_vs 和 valid_angles 已准备好
# ✅ STEP 7: 定义扰动方向（±30°, ±20°, ±10°, 0°, +10°, +20°, +30°）
valid_vs = [
    np.array([0.66819203, -0.74398885]),  # -30°
    np.array([0.78723300, -0.61665566]),  # -20°
    np.array([0.88235429, -0.47058570]),  # -10°
    np.array([0.95066570, -0.31021723]),  # 0° 主方向
    np.array([0.99009161, -0.14042297]),  # +10°
    np.array([0.99943408, +0.03363798]),  # +20°
    np.array([0.97840926, +0.20667685])   # +30°
]

valid_angles = [
    np.float64(-48.07232214895949),
    np.float64(-38.0723221489595),
    np.float64(-28.072322148959497),
    np.float64(-18.072322148959497),
    np.float64(-8.072322148959499),
    np.float64(1.9276778510405013),
    np.float64(11.9276778510405)
]



# ✅ STEP 8: 生成响应
batch_size = 8
for i, (v_vec, angle_deg) in enumerate(zip(valid_vs, valid_angles)):
    v1, v2 = v_vec[0], v_vec[1]
    output_file = os.path.join(result_dir, f"v10_angle_{int(angle_deg)}.csv")
    print(f"\n🚀 Generating for direction {i}: angle = {angle_deg}°, v = ({v1:.4f}, {v2:.4f})")

    if os.path.exists(output_file):
        existing_df = pd.read_csv(output_file)
        done_prompt_ids = set(existing_df["prompt_id"].unique())
        results = existing_df.to_dict("records")
        print(f"🔁 Resuming from previous run: {len(done_prompt_ids)} prompts already completed.")
    else:
        done_prompt_ids = set()
        results = []

    for start in tqdm(range(0, len(prompts), batch_size), desc=f"Generating angle {angle_deg}°"):
        end = min(start + batch_size, len(prompts))
        batch_prompts_all = prompts[start:end]
        batch_ids_all = prompt_ids[start:end]

        unprocessed_indices = [j for j, pid in enumerate(batch_ids_all) if pid not in done_prompt_ids]
        if not unprocessed_indices:
            continue

        batch_prompts = [batch_prompts_all[j] for j in unprocessed_indices]
        batch_ids = [batch_ids_all[j] for j in unprocessed_indices]

        try:
            batch_outputs = generate_response_batch(batch_prompts, batch_ids, v1, v2)
            for item in batch_outputs:
                item.update({
                    "v1_p": round(v1, 4),
                    "v2_p": round(v2, 4),
                    "direction_index": i,
                    "angle_deg": round(angle_deg, 1),
                    "main_v1": round(np.cos(np.radians(main_angle)), 4),
                    "main_v2": round(np.sin(np.radians(main_angle)), 4)
                })
                results.append(item)

            pd.DataFrame(batch_outputs).to_csv(
                output_file, mode='a', index=False,
                header=not os.path.exists(output_file)
            )

        except Exception as e:
            print(f"⚠️ Error at batch {start}-{end}: {e}")

    print(f"✅ Final saved {len(results)} responses to {output_file}")

import os
import pandas as pd
import numpy as np
import torch
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForCausalLM
from accelerate import Accelerator

# === 模型准备 ===
device = "cuda" if torch.cuda.is_available() else "cpu"
accelerator = Accelerator()

model = AutoModelForCausalLM.from_pretrained(
    "Haoxiang-Wang/DPA-v1-Mistral-7B",
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
    device_map="auto" if torch.cuda.is_available() else None
)
model = accelerator.prepare(model)

tokenizer = AutoTokenizer.from_pretrained("Haoxiang-Wang/DPA-v1-Mistral-7B")
tokenizer.padding_side = "left"
if tokenizer.pad_token_id is None:
    tokenizer.pad_token = tokenizer.eos_token

# === 判断是否无效 response ===
def is_invalid_response(resp):
    if pd.isna(resp):
        return True
    resp = str(resp).strip()
    if not resp:
        return True

    invalid_keywords = [
        "[A:]", "[/SOLUTION]", "[ASS]", "[A]:", "[/CONV]", "[/CODE]", "[/USER]",
        "[OUTPUT]", "[CC]", "[OUT]", "[/DATA]", "[/ASST]", "[/ME]", "[/REST]",
        "[RESP]", "<[user]>", "<|user|>", "[Response]", "[Assistant]", "[Answer]",
        "[End of Response]", "<human>", "<|endoftext|>", "<<", "[INST]", "[/INST]",
        "[]", "[USER]"
    ]
    tokens = resp.split()
    if all(token.strip() in invalid_keywords for token in tokens):
        return True
    tag_like_count = sum(1 for t in tokens if (t.startswith("[") and t.endswith("]")) or (t.startswith("<") and t.endswith(">")))
    if len(tokens) > 0 and tag_like_count / len(tokens) > 0.8:
        return True
    return False

# === 构造输入 prompt ===
def build_input(prompt, v1, v2):
    h = int(np.round(v1 * 100))
    v = int(np.round(v2 * 100))
    sys_instruction = f"You are a helpful assistant. Your response should maximize weighted rating = helpfulness*{h} + verbosity*{v}."
    return [{"role": "system", "content": sys_instruction}, {"role": "user", "content": prompt}]

# === 用模型重新生成 response ===
def regenerate_response(prompt, v1, v2):
    try:
        messages = build_input(prompt, v1, v2)
        input_ids = tokenizer.apply_chat_template(
            messages, add_generation_prompt=True, return_tensors="pt"
        ).to(device)

        if input_ids.ndim != 2 or input_ids.shape[1] == 0:
            print(f"⚠️ Invalid input_ids shape {input_ids.shape}, skipping: {prompt[:60]}")
            return ""

        outputs = model.generate(
            input_ids=input_ids,
            temperature=0.7,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
        generated = outputs[0][input_ids.shape[1]:]
        return tokenizer.decode(generated, skip_special_tokens=True).strip()

    except Exception as e:
        print(f"❌ Error: {e} \nPrompt: {prompt[:60]}")
        return ""

# === 修复单个 CSV 文件 ===
def fix_csv_responses(csv_path, save_dir):
    print(f"\n📂 正在检查: {csv_path}")

    if not os.path.exists(csv_path):
        print(f"❌ 文件不存在: {csv_path}")
        return

    df = pd.read_csv(csv_path)
    if "v1_p" not in df.columns or "v2_p" not in df.columns:
        print(f"❌ 缺少 v1_p/v2_p 列，跳过: {csv_path}")
        return

    bad_mask = df["response"].apply(is_invalid_response)
    num_bad = bad_mask.sum()

    if num_bad == 0:
        print("✅ 无需修复，跳过。")
        return

    print(f"🔧 发现 {num_bad} 条无效 response，开始重生成...")
    for i in tqdm(bad_mask[bad_mask].index, total=num_bad, desc=os.path.basename(csv_path), dynamic_ncols=True):
        row = df.loc[i]
        new_response = regenerate_response(row["prompt"], row["v1_p"], row["v2_p"])
        df.at[i, "response"] = new_response

    os.makedirs(save_dir, exist_ok=True)
    file_name = os.path.basename(csv_path).replace(".csv", "_fixed.csv")
    save_path = os.path.join(save_dir, file_name)
    df.to_csv(save_path, index=False)
    print(f"✅ 修复完成 → 已保存: {save_path}")

# === 主逻辑：修复所有 CSV ===
input_dir = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/v11_direction_sensitivity"
output_dir = os.path.join(input_dir, "fixed")

for file in os.listdir(input_dir):
    if file.endswith(".csv") and not file.endswith("_fixed.csv"):
        fix_csv_responses(os.path.join(input_dir, file), output_dir)

import pandas as pd
import os

# 结果目录
result_dir = "/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/v11_direction_sensitivity/fixed"
merged_path = os.path.join(result_dir, "merged_all_angles.csv")

# 合并所有 CSV 文件
all_data = []
for fname in os.listdir(result_dir):
    if fname.endswith(".csv") and not fname.startswith(".") and "merged" not in fname:
        fpath = os.path.join(result_dir, fname)
        try:
            df = pd.read_csv(fpath)
            all_data.append(df)
            print(f"📄 Loaded {fname} with {len(df)} rows.")
        except Exception as e:
            print(f"⚠️ Failed to load {fname}: {e}")

if all_data:
    df_all = pd.concat(all_data, ignore_index=True)
    df_all.to_csv(merged_path, index=False)
    print(f"\n✅ Merged total {len(df_all)} responses.")
    print(f"📁 Saved to: {merged_path}")
else:
    print("❌ No valid CSV files found to merge.")

import os
import time
import pandas as pd
import numpy as np
import torch
from tqdm.auto import tqdm
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from google.colab import drive

# === 挂载 Google Drive ===
drive.mount('/content/drive')

# === 设置路径 ===
input_path = "/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/v11_direction_sensitivity/fixed/merged_all_angles.csv"
output_path = "/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/v11_direction_sensitivity/merged_all_angles_scored.csv"

# === 加载合并数据 ===
df = pd.read_csv(input_path)
print(f"📦 Loaded {len(df)} samples")

# === 加载 Reward Model ===
print("🤖 Loading Reward Model...")
device = "cuda" if torch.cuda.is_available() else "cpu"
rm = AutoModelForSequenceClassification.from_pretrained(
    "Haoxiang-Wang/RewardModel-Mistral-7B-for-DPA-v1", trust_remote_code=True
).to(device)
tokenizer = AutoTokenizer.from_pretrained("Haoxiang-Wang/RewardModel-Mistral-7B-for-DPA-v1")

def score_response(prompt, response):
    template = "[INST] You must read the following conversation carefully and rate the assistant's response from score 0-100 in these aspects: helpfulness, correctness, coherence, honesty, complexity, verbosity\n\nUser: {prompt}\n\nAssistant: {response} [/INST]"
    inputs = tokenizer(template.format(prompt=prompt, response=response), return_tensors="pt").to(device)
    with torch.no_grad():
        logits = rm(**inputs).logits.squeeze().cpu().numpy()
    return logits[9], logits[4]  # helpfulness, verbosity

# === 批量打分 ===
help_scores, verb_scores = [], []

print("🧠 Scoring all responses...")
for i, row in tqdm(df.iterrows(), total=len(df)):
    try:
        h, v = score_response(row["prompt"], row["response"])
    except Exception as e:
        print(f"⚠️ Error on row {i}: {e}")
        h, v = None, None
    help_scores.append(h)
    verb_scores.append(v)

df["reward_score_helpfulness"] = help_scores
df["reward_score_verbosity"] = verb_scores

# === 保存打分结果 ===
df.to_csv(output_path, index=False)
print(f"✅ All scored results saved to {output_path}")

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# === 加载打分后的 CSV ===
input_path = "/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/v11_direction_sensitivity/merged_all_angles_scored.csv"
df = pd.read_csv(input_path)

# === 聚合不同方向的平均分 ===
df_grouped = df.groupby("angle_deg")[["reward_score_helpfulness", "reward_score_verbosity"]].agg(["mean", "std"]).reset_index()
df_grouped.columns = ["angle", "help_mean", "help_std", "verb_mean", "verb_std"]

# === 可视化：helpfulness 和 verbosity 随角度变化 ===
plt.figure(figsize=(10, 6))
plt.errorbar(df_grouped["angle"], df_grouped["help_mean"], yerr=df_grouped["help_std"], label="Helpfulness", fmt='-o')
plt.errorbar(df_grouped["angle"], df_grouped["verb_mean"], yerr=df_grouped["verb_std"], label="Verbosity", fmt='-o', color='orange')
plt.axvline(x=-18.072322148959497, color='gray', linestyle='--', label="Main Direction")
plt.xlabel("Angle (degrees)")
plt.ylabel("Reward Score")
plt.title("Reward Score vs Direction Angle (±pertuation)")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# 角度设置：0° 到 45°
angles_deg = np.arange(0, 46, 5)
angles_rad = np.radians(angles_deg)

# 计算单位方向向量坐标
x = np.cos(angles_rad)
y = np.sin(angles_rad)

# 创建图形
fig, ax = plt.subplots(figsize=(6, 6))
ax.set_aspect('equal')

# 绘制单位圆
theta_full = np.linspace(0, 2 * np.pi, 500)
circle_x = np.cos(theta_full)
circle_y = np.sin(theta_full)
ax.plot(circle_x, circle_y, color='lightgray', linestyle='--', linewidth=1)

# 绘制方向向量和角度标注
for i in range(len(x)):
    ax.arrow(0, 0, x[i], y[i],
             head_width=0.03, head_length=0.05,
             fc='black', ec='black', length_includes_head=True)
    ax.text(x[i] * 1.15, y[i] * 1.15,
            f'{angles_deg[i]}°',
            ha='center', va='center', fontsize=10)

# 设置坐标轴范围和标签
ax.set_xlim(-1.1, 1.1)
ax.set_ylim(-1.1, 1.1)
ax.set_xlabel(r'$V_{\text{helpfulness}}$', fontsize=14)
ax.set_ylabel(r'$V_{\text{verbosity}}$', fontsize=14)
ax.set_title('Preference Directions on the Unit Circle', fontsize=13)
ax.axhline(0, color='gray', linewidth=0.5)
ax.axvline(0, color='gray', linewidth=0.5)
ax.grid(True, linestyle=':', linewidth=0.5)
ax.set_xticks([])
ax.set_yticks([])

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# 采样角度：-45° 到 0°
angles_deg = np.linspace(-45, 0, 30)  # 可调采样密度
angles_rad = np.radians(angles_deg)

# 单位向量坐标
x = np.cos(angles_rad)
y = np.sin(angles_rad)

# 创建图形
fig, ax = plt.subplots(figsize=(6, 6))
ax.set_aspect('equal')

# 绘制单位圆
theta_full = np.linspace(0, 2 * np.pi, 500)
circle_x = np.cos(theta_full)
circle_y = np.sin(theta_full)
ax.plot(circle_x, circle_y, color='lightgray', linestyle='--', linewidth=1)

# 绘制训练方向向量 (v1, v2)
for i in range(len(x)):
    ax.arrow(0, 0, x[i], y[i],
             head_width=0.025, head_length=0.04,
             fc='darkblue', ec='darkblue', alpha=0.7,
             length_includes_head=True)

# 可视化采样扇形区域边界
arc_angles = np.radians(np.linspace(-45, 0, 100))
arc_x = np.cos(arc_angles)
arc_y = np.sin(arc_angles)
ax.plot(arc_x, arc_y, color='blue', linestyle=':', linewidth=1)
ax.fill_between(arc_x, arc_y, 0, where=(arc_y <= 0), color='blue', alpha=0.05)

# 标注
ax.text(0.8, -0.1, r'$v = (1, 0)$', fontsize=10)
ax.text(0.45, -0.55, r'$v = \left(\frac{\sqrt{2}}{2}, -\frac{\sqrt{2}}{2}\right)$', fontsize=10)
ax.text(0.3, -0.3, r'DPA training range', fontsize=11, color='blue')

# 坐标轴设置
ax.set_xlim(-1.05, 1.05)
ax.set_ylim(-1.05, 1.05)
ax.axhline(0, color='gray', linewidth=0.5)
ax.axvline(0, color='gray', linewidth=0.5)
ax.set_xlabel(r'$V_{\text{helpfulness}}$', fontsize=14)
ax.set_ylabel(r'$V_{\text{verbosity}}$', fontsize=14)
ax.set_title('Training-Time Direction Sampling in DPA', fontsize=13)
ax.grid(True, linestyle=':', linewidth=0.5)
ax.set_xticks([])
ax.set_yticks([])

plt.tight_layout()
plt.show()





























