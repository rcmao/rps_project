# steerlm_nemo_generation_fixed.py - åŠ è½½çœŸæ­£çš„Nemotron-3-8B-SteerLMæ¨¡å‹
import os
import numpy as np
import pandas as pd
from datasets import load_dataset
from tqdm.auto import tqdm
import torch
import time
import random
import json

# =============================================================================
# ğŸŒ ç½‘ç»œé…ç½® - æ”¯æŒå›½å†…é•œåƒ
# =============================================================================

# å›½å†…é•œåƒé€‰é¡¹
MIRROR_OPTIONS = {
    "official": {
        "hf_endpoint": None,
        "name": "Official HuggingFace",
        "description": "ç›´æ¥è®¿é—®å®˜æ–¹HuggingFace"
    },
    "hf_mirror": {
        "hf_endpoint": "https://hf-mirror.com", 
        "name": "HF Mirror",
        "description": "å›½å†…HFé•œåƒ (æ¨è)"
    },
    "modelfun": {
        "hf_endpoint": "https://www.modelfun.cn",
        "name": "ModelFun",
        "description": "æ¨¡å‹ä¹å›­é•œåƒ"
    }
}

def setup_mirror(mirror_choice="hf_mirror"):
    """è®¾ç½®é•œåƒé…ç½®"""
    if mirror_choice in MIRROR_OPTIONS:
        mirror = MIRROR_OPTIONS[mirror_choice]
        if mirror["hf_endpoint"]:
            os.environ['HF_ENDPOINT'] = mirror["hf_endpoint"]
        elif 'HF_ENDPOINT' in os.environ:
            del os.environ['HF_ENDPOINT']
        
        print(f"ğŸŒ ä½¿ç”¨é•œåƒ: {mirror['name']} - {mirror['description']}")
        return mirror["name"]
    else:
        print(f"âŒ æœªçŸ¥é•œåƒé€‰æ‹©: {mirror_choice}")
        return "Unknown"

# è®¾ç½®ç½‘ç»œè¶…æ—¶å’Œé‡è¯•
os.environ['HF_HUB_DOWNLOAD_TIMEOUT'] = '300'
os.environ['TRANSFORMERS_CACHE'] = '/root/.cache/huggingface'

# é»˜è®¤ä½¿ç”¨å›½å†…é•œåƒ
current_mirror = setup_mirror("hf_mirror")

print("ğŸŒ Network configuration:")
print(f"Current Mirror: {current_mirror}")
print(f"HF_ENDPOINT: {os.environ.get('HF_ENDPOINT', 'Official HuggingFace')}")
print(f"Cache dir: {os.environ.get('TRANSFORMERS_CACHE', 'Default')}")

# =============================================================================

def load_nemotron_model():
    """åŠ è½½çœŸæ­£çš„Nemotron-3-8B-SteerLMæ¨¡å‹"""
    print("ğŸ¤– Loading Nemotron-3-8B-SteerLM model...")
    
    try:
        # æ–¹æ³•1ï¼šå°è¯•ä½¿ç”¨NeMoæ¡†æ¶åŠ è½½
        from nemo.collections.nlp.models.language_modeling.megatron_gpt_model import MegatronGPTModel
        from lightning.pytorch import Trainer
        
        # æ¨¡å‹è·¯å¾„
        model_path = "/root/.cache/huggingface/models--nvidia--nemotron-3-8b-chat-4k-steerlm/snapshots/3c8811184fff2ccf55350ff819a786188987bc7f/Nemotron-3-8B-Chat-4k-SteerLM.nemo"
        
        print(f"ğŸ“¥ Loading from: {model_path}")
        
        # åˆ›å»ºtrainer
        trainer = Trainer(
            accelerator='gpu' if torch.cuda.is_available() else 'cpu',
            devices=1,
            precision=16 if torch.cuda.is_available() else 32,
            logger=False,
            enable_checkpointing=False,
            enable_progress_bar=False,
            enable_model_summary=False,
        )
        
        # åŠ è½½æ¨¡å‹
        model = MegatronGPTModel.restore_from(
            restore_path=model_path,
            trainer=trainer,
            map_location="cuda" if torch.cuda.is_available() else "cpu"
        )
        
        model.eval()
        print("âœ… Nemotron model loaded successfully!")
        return model, None  # NeMoæ¨¡å‹ä¸éœ€è¦tokenizer
        
    except Exception as e:
        print(f"âŒ Failed to load Nemotron model with NeMo: {e}")
        print("ğŸ’¡ Falling back to demo model...")
        
        # æ–¹æ³•2ï¼šå›é€€åˆ°æ¼”ç¤ºæ¨¡å‹
        from transformers import AutoTokenizer, AutoModelForCausalLM
        
        model_name = "microsoft/DialoGPT-medium"
        print(f"ğŸ“¥ Loading demo model: {model_name}")
        
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForCausalLM.from_pretrained(model_name)
        
        if torch.cuda.is_available():
            model = model.cuda()
        
        print("âœ… Demo model loaded successfully!")
        return model, tokenizer

def build_official_nemotron_prompt(prompt, attributes=None):
    """æ„å»ºå®˜æ–¹Nemotron SteerLMæ ¼å¼çš„prompt"""
    if attributes is None:
        # ä½¿ç”¨å®˜æ–¹æ–‡æ¡£ä¸­çš„é»˜è®¤å±æ€§
        attributes = {
            "quality": 4,
            "understanding": 4,
            "correctness": 4,
            "coherence": 4,
            "complexity": 4,
            "verbosity": 4,
            "toxicity": 0,
            "humor": 0,
            "creativity": 0,
            "violence": 0,
            "helpfulness": 4,
            "not_appropriate": 0,
            "hate_speech": 0,
            "sexual_content": 0,
            "fails_task": 0,
            "political_content": 0,
            "moral_judgement": 0,
            "lang": "en"
        }
    
    # æŒ‰ç…§å®˜æ–¹æ–‡æ¡£çš„é¡ºåºæ„å»ºå±æ€§å­—ç¬¦ä¸²
    attr_string = f"quality:{attributes['quality']},understanding:{attributes['understanding']},correctness:{attributes['correctness']},coherence:{attributes['coherence']},complexity:{attributes['complexity']},verbosity:{attributes['verbosity']},toxicity:{attributes['toxicity']},humor:{attributes['humor']},creativity:{attributes['creativity']},violence:{attributes['violence']},helpfulness:{attributes['helpfulness']},not_appropriate:{attributes['not_appropriate']},hate_speech:{attributes['hate_speech']},sexual_content:{attributes['sexual_content']},fails_task:{attributes['fails_task']},political_content:{attributes['political_content']},moral_judgement:{attributes['moral_judgement']},lang:{attributes['lang']}"
    
    # å®˜æ–¹SteerLM promptæ ¼å¼ï¼ˆæ¥è‡ªå®˜æ–¹æ–‡æ¡£ï¼‰
    official_prompt = f"""<extra_id_0>System
A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.

<extra_id_1>User
{prompt}
<extra_id_1>Assistant
<extra_id_2>{attr_string}
"""
    
    return official_prompt, attr_string

def generate_with_nemotron_model(model, prompt_text, max_tokens=512, temperature=0.7, tokenizer=None):
    """ä½¿ç”¨Nemotronæ¨¡å‹ç”Ÿæˆæ–‡æœ¬"""
    try:
        # æ£€æŸ¥æ˜¯å¦æ˜¯NeMoæ¨¡å‹
        if hasattr(model, 'generate') and hasattr(model, 'cfg'):
            # NeMoæ¨¡å‹ç”Ÿæˆ
            length_params = {"max_length": max_tokens, "min_length": 1}
            sampling_params = {
                "use_greedy": False,
                "temperature": temperature,
                "top_k": 50,
                "top_p": 0.9,
                "repetition_penalty": 1.2,
            }
            
            response = model.generate([prompt_text], length_params, sampling_params)
            
            if response and len(response) > 0:
                # æ¸…ç†è¾“å‡º
                if response[0].startswith(prompt_text):
                    response = response[0][len(prompt_text):].strip()
                else:
                    response = response[0]
                
                response = response.split("<extra_id_1>")[0].strip()
                return response
            else:
                return "Empty response from Nemotron model"
                
        else:
            # Transformersæ¨¡å‹ç”Ÿæˆ
            from transformers import GenerationConfig
            
            # ä½¿ç”¨ä¼ å…¥çš„tokenizer
            if tokenizer is None:
                return "ERROR: No tokenizer available for Transformers model"
            
            inputs = tokenizer.encode(prompt_text, return_tensors="pt")
            if torch.cuda.is_available():
                inputs = inputs.cuda()
            
            generation_config = GenerationConfig(
                max_new_tokens=max_tokens,
                min_new_tokens=1,
                temperature=temperature,
                top_k=50,
                top_p=0.9,
                repetition_penalty=1.2,
                do_sample=temperature > 0.0,
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id,
            )
            
            with torch.no_grad():
                outputs = model.generate(
                    inputs,
                    generation_config=generation_config,
                    return_dict_in_generate=True,
                    output_scores=False,
                )
            
            generated_text = tokenizer.decode(outputs.sequences[0], skip_special_tokens=False)
            
            if generated_text.startswith(prompt_text):
                response = generated_text[len(prompt_text):].strip()
            else:
                response = generated_text
            
            return response
            
    except Exception as e:
        return f"ERROR: {str(e)}"

def main():
    """ä¸»å‡½æ•° - åŠ è½½çœŸæ­£çš„Nemotronæ¨¡å‹"""
    print("ğŸš€ Loading Nemotron-3-8B-SteerLM model!")
    print("ğŸ“– Based on official documentation: https://huggingface.co/nvidia/nemotron-3-8b-chat-4k-steerlm")
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # æ ¹æ®æ˜¾å­˜è®¾ç½®batch_size
    if torch.cuda.is_available():
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3  # GB
        print(f"ğŸš€ GPU Memory: {gpu_memory:.1f}GB")
    else:
        print("ğŸ’» Using CPU")
    
    # åŠ è½½Nemotronæ¨¡å‹
    model, tokenizer = load_nemotron_model()
    if model is None:
        print("âŒ Failed to load any model")
        return
    
    # æµ‹è¯•prompt
    test_prompt = "Write a poem about NVIDIA in the style of Shakespeare"
    
    print(f"\nğŸ§ª Testing with prompt: {test_prompt}")
    
    # æ„å»ºå®˜æ–¹SteerLMæ ¼å¼çš„prompt
    nemotron_prompt, attr_string = build_official_nemotron_prompt(test_prompt)
    
    print(f"\nğŸ“ Generated Nemotron SteerLM Prompt:")
    print(f"```\n{nemotron_prompt}\n```")
    
    print(f"\nğŸ“Š Attribute String:")
    print(f"```\n{attr_string}\n```")
    
    # ç”Ÿæˆå“åº”
    print("âš¡ Generating response...")
    start_time = time.time()
    
    response = generate_with_nemotron_model(
        model=model,
        prompt_text=nemotron_prompt,
        max_tokens=512,
        temperature=0.7,
        tokenizer=tokenizer
    )
    
    end_time = time.time()
    
    print(f"\nğŸ¯ Generated Response (took {end_time - start_time:.2f}s):")
    print(f"```\n{response}\n```")
    
    print(f"\nâœ… Nemotron model test completed!")
    print(f"ğŸ“Š Attribute string used: {attr_string}")
    print(f"ğŸ”§ Model: nvidia/nemotron-3-8b-chat-4k-steerlm")
    print(f"ğŸ“š Framework: NVIDIA NeMo")
    print(f"ğŸ“– Documentation: https://huggingface.co/nvidia/nemotron-3-8b-chat-4k-steerlm")

if __name__ == "__main__":
    main() 