# load_nemotron_official.py - ä½¿ç”¨å®˜æ–¹NeMoæ¡†æ¶åŠ è½½Nemotron-3-8B-SteerLM
import os
import torch
import time
import json

# =============================================================================
# ğŸŒ ç½‘ç»œé…ç½® - æ”¯æŒå›½å†…é•œåƒ
# =============================================================================

# è®¾ç½®ç½‘ç»œè¶…æ—¶å’Œé‡è¯•
os.environ['HF_HUB_DOWNLOAD_TIMEOUT'] = '300'
os.environ['TRANSFORMERS_CACHE'] = '/root/.cache/huggingface'
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

print("ğŸŒ Network configuration:")
print(f"HF_ENDPOINT: {os.environ.get('HF_ENDPOINT', 'Official HuggingFace')}")
print(f"Cache dir: {os.environ.get('TRANSFORMERS_CACHE', 'Default')}")

# =============================================================================

def download_nemotron_model():
    """ä¸‹è½½Nemotron-3-8B-SteerLMæ¨¡å‹"""
    from huggingface_hub import hf_hub_download
    
    print("ğŸ“¥ Downloading Nemotron-3-8B-SteerLM (.nemo format)...")
    
    try:
        model_path = hf_hub_download(
            repo_id="nvidia/nemotron-3-8b-chat-4k-steerlm",
            filename="Nemotron-3-8B-Chat-4k-SteerLM.nemo",
            cache_dir="/root/.cache/huggingface",
            resume_download=True
        )
        print(f"âœ… Model downloaded to: {model_path}")
        return model_path
    except Exception as e:
        print(f"âŒ Download failed: {e}")
        print("ğŸ’¡ ç¡®ä¿å·²ç™»å½•HuggingFaceå¹¶æ¥å—æ¨¡å‹è®¸å¯è¯")
        return None

def build_official_prompt(prompt, attributes=None):
    """æ„å»ºå®˜æ–¹SteerLMæ ¼å¼çš„prompt"""
    if attributes is None:
        # ä½¿ç”¨å®˜æ–¹æ–‡æ¡£ä¸­çš„é»˜è®¤å±æ€§
        attributes = {
            "quality": 4,
            "understanding": 4,
            "correctness": 4,
            "coherence": 4,
            "complexity": 4,
            "verbosity": 4,
            "toxicity": 0,
            "humor": 0,
            "creativity": 0,
            "violence": 0,
            "helpfulness": 4,
            "not_appropriate": 0,
            "hate_speech": 0,
            "sexual_content": 0,
            "fails_task": 0,
            "political_content": 0,
            "moral_judgement": 0,
            "lang": "en"
        }
    
    # æŒ‰ç…§å®˜æ–¹æ–‡æ¡£çš„é¡ºåºæ„å»ºå±æ€§å­—ç¬¦ä¸²
    attr_string = f"quality:{attributes['quality']},understanding:{attributes['understanding']},correctness:{attributes['correctness']},coherence:{attributes['coherence']},complexity:{attributes['complexity']},verbosity:{attributes['verbosity']},toxicity:{attributes['toxicity']},humor:{attributes['humor']},creativity:{attributes['creativity']},violence:{attributes['violence']},helpfulness:{attributes['helpfulness']},not_appropriate:{attributes['not_appropriate']},hate_speech:{attributes['hate_speech']},sexual_content:{attributes['sexual_content']},fails_task:{attributes['fails_task']},political_content:{attributes['political_content']},moral_judgement:{attributes['moral_judgement']},lang:{attributes['lang']}"
    
    # å®˜æ–¹SteerLM promptæ ¼å¼ï¼ˆæ¥è‡ªå®˜æ–¹æ–‡æ¡£ï¼‰
    official_prompt = f"""<extra_id_0>System
A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.

<extra_id_1>User
{prompt}
<extra_id_1>Assistant
<extra_id_2>{attr_string}
"""
    
    return official_prompt, attr_string

def test_with_nemo_query():
    """ä½¿ç”¨å®˜æ–¹NemoQuery APIæµ‹è¯•"""
    print("ğŸš€ Testing with official NemoQuery API...")
    
    try:
        from nemo.deploy import NemoQuery
        
        # è¿æ¥åˆ°NeMoæ¨ç†æœåŠ¡å™¨
        nq = NemoQuery(url="localhost:8000", model_name="Nemotron-3-8B-Chat-4K-SteerLM")
        
        # æµ‹è¯•prompt
        test_prompt = "Write a poem about NVIDIA in the style of Shakespeare"
        prompt, attr_string = build_official_prompt(test_prompt)
        
        print(f"ğŸ“ Test prompt: {test_prompt}")
        print(f"ğŸ“ Generated prompt:")
        print(f"```\n{prompt}\n```")
        
        # ä½¿ç”¨å®˜æ–¹APIç”Ÿæˆå“åº”
        output = nq.query_llm(
            prompts=[prompt], 
            max_output_token=200, 
            top_k=1, 
            top_p=0.0, 
            temperature=0.1
        )
        
        # åå¤„ç†è¾“å‡ºï¼ˆå®˜æ–¹æ–‡æ¡£è¦æ±‚ï¼‰
        output = [[s.split("<extra_id_1>", 1)[0].strip() for s in out] for out in output]
        
        print(f"\nğŸ¯ Generated Response:")
        print(f"```\n{output[0][0]}\n```")
        
        print("âœ… NemoQuery API test completed!")
        return True
        
    except ImportError:
        print("âŒ NemoQuery not available. Trying alternative approach...")
        return False
    except Exception as e:
        print(f"âŒ NemoQuery failed: {e}")
        return False

def test_with_direct_nemo():
    """ç›´æ¥ä½¿ç”¨NeMoæ¡†æ¶æµ‹è¯•"""
    print("ğŸš€ Testing with direct NeMo framework...")
    
    try:
        from nemo.collections.nlp.models.language_modeling.megatron_gpt_model import MegatronGPTModel
        from lightning.pytorch import Trainer
        
        # ä¸‹è½½æ¨¡å‹
        model_path = download_nemotron_model()
        if model_path is None:
            return False
        
        print(f"ğŸ¤– Loading model from: {model_path}")
        
        # åˆ›å»ºtrainer
        trainer = Trainer(
            accelerator='gpu' if torch.cuda.is_available() else 'cpu',
            devices=1,
            precision=16 if torch.cuda.is_available() else 32,
            logger=False,
            enable_checkpointing=False,
            enable_progress_bar=False,
            enable_model_summary=False,
        )
        
        # åŠ è½½æ¨¡å‹
        model = MegatronGPTModel.restore_from(
            restore_path=model_path,
            trainer=trainer,
            map_location="cuda" if torch.cuda.is_available() else "cpu"
        )
        
        model.eval()
        print("âœ… Model loaded successfully!")
        
        # æµ‹è¯•prompt
        test_prompt = "Write a poem about NVIDIA in the style of Shakespeare"
        prompt, attr_string = build_official_prompt(test_prompt)
        
        print(f"\nğŸ“ Test prompt: {test_prompt}")
        print(f"ğŸ“ Generated prompt:")
        print(f"```\n{prompt}\n```")
        
        # ç”Ÿæˆå“åº”
        print("\nâš¡ Generating response...")
        start_time = time.time()
        
        length_params = {"max_length": 512, "min_length": 1}
        sampling_params = {
            "use_greedy": False,
            "temperature": 0.7,
            "top_k": 50,
            "top_p": 0.9,
            "repetition_penalty": 1.2,
        }
        
        response = model.generate([prompt], length_params, sampling_params)
        
        end_time = time.time()
        
        if response and len(response) > 0:
            # æ¸…ç†è¾“å‡º
            if response[0].startswith(prompt):
                response = response[0][len(prompt):].strip()
            else:
                response = response[0]
            
            response = response.split("<extra_id_1>")[0].strip()
            
            print(f"\nğŸ¯ Generated Response (took {end_time - start_time:.2f}s):")
            print(f"```\n{response}\n```")
            
            print(f"\nğŸ“Š Attribute string used: {attr_string}")
            print("âœ… Direct NeMo test completed!")
            return True
        else:
            print("âŒ Empty response from model")
            return False
            
    except Exception as e:
        print(f"âŒ Direct NeMo failed: {e}")
        return False

def create_docker_solution():
    """åˆ›å»ºDockerè§£å†³æ–¹æ¡ˆ"""
    print("ğŸ³ Creating Docker solution...")
    
    docker_script = '''#!/bin/bash

# å¯åŠ¨NeMoæ¨ç†æœåŠ¡å™¨
echo "ğŸš€ Starting NeMo inference server..."
docker run -d --name nemotron-server \\
    --gpus all \\
    --shm-size=16g \\
    --ulimit memlock=-1 \\
    --ulimit stack=67108864 \\
    -p 8000:8000 \\
    nvcr.io/nvidia/nemo:25.02 \\
    python -m nemo.deploy.inference.server \\
    --model-path /workspace/Nemotron-3-8B-Chat-4k-SteerLM.nemo \\
    --port 8000

# ç­‰å¾…æœåŠ¡å™¨å¯åŠ¨
echo "â³ Waiting for server to start..."
sleep 30

# è¿è¡Œæµ‹è¯•
echo "ğŸ§ª Running test..."
python3 load_nemotron_official.py

# æ¸…ç†
echo "ğŸ§¹ Cleaning up..."
docker stop nemotron-server
docker rm nemotron-server

echo "âœ… Docker solution completed!"
'''
    
    with open('run_nemotron_docker_official.sh', 'w') as f:
        f.write(docker_script)
    
    os.chmod('run_nemotron_docker_official.sh', 0o755)
    print("âœ… Docker script created: run_nemotron_docker_official.sh")

def main():
    """ä¸»å‡½æ•° - å°è¯•å¤šç§æ–¹æ³•åŠ è½½Nemotronæ¨¡å‹"""
    print("ğŸš€ Starting Nemotron-3-8B-SteerLM loading test!")
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    if torch.cuda.is_available():
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3  # GB
        print(f"ğŸš€ GPU Memory: {gpu_memory:.1f}GB")
    else:
        print("ğŸ’» Using CPU")
    
    # æ–¹æ³•1ï¼šå°è¯•å®˜æ–¹NemoQuery API
    print("\n" + "="*60)
    print("ğŸ§ª Method 1: Official NemoQuery API")
    print("="*60)
    
    if test_with_nemo_query():
        print("âœ… Success with NemoQuery API!")
        return
    
    # æ–¹æ³•2ï¼šå°è¯•ç›´æ¥NeMoæ¡†æ¶
    print("\n" + "="*60)
    print("ğŸ§ª Method 2: Direct NeMo Framework")
    print("="*60)
    
    if test_with_direct_nemo():
        print("âœ… Success with direct NeMo!")
        return
    
    # æ–¹æ³•3ï¼šåˆ›å»ºDockerè§£å†³æ–¹æ¡ˆ
    print("\n" + "="*60)
    print("ğŸ§ª Method 3: Docker Solution")
    print("="*60)
    
    create_docker_solution()
    print("ğŸ’¡ Docker solution created. Run: bash run_nemotron_docker_official.sh")
    
    print("\n" + "="*60)
    print("ğŸ“‹ Summary")
    print("="*60)
    print("ğŸ”§ Model: nvidia/nemotron-3-8b-chat-4k-steerlm")
    print("ğŸ“š Framework: NVIDIA NeMo")
    print("ğŸ“– Documentation: https://huggingface.co/nvidia/nemotron-3-8b-chat-4k-steerlm")
    print("ğŸ’¡ Best approach: Use Docker container with official NeMo framework")

if __name__ == "__main__":
    main() 