# -*- coding: utf-8 -*-
"""generate_response_v10.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14eo_Bbiacc5s8rz-OQWyqXPIESD7Yqig
"""

# ğŸš€ Step 0: é˜²æ­¢ Colab ç©ºé—²æ–­çº¿
from IPython.display import Javascript
Javascript('''
function ClickConnect(){
  console.log("ğŸ”„ ä¿æŒè¿æ¥ä¸­ - ClickConnect è¢«è°ƒç”¨äº†");
  document.querySelector("colab-connect-button").click();
}
setInterval(ClickConnect, 60000);
''')

# âœ… Step 1: æŒ‚è½½ Google Drive
from google.colab import drive
drive.mount('/content/drive')

# âœ… Step 2: å®‰è£…ä¾èµ–
!pip install -q transformers datasets accelerate

# âœ… å…ˆè®¾ç½®ä½ æƒ³è·‘çš„ v index
v_indices = [6]

from google.colab import drive
drive.mount('/content/drive')

import os
import numpy as np
import pandas as pd
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM
from tqdm.auto import tqdm
import torch
import time
import random

# âœ… è®¾ç½®è®¾å¤‡
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# è®¾ç½®éšæœºç§å­
seed = 42
np.random.seed(seed)
random.seed(seed)
torch.manual_seed(seed)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(seed)

# è¾“å‡ºç›®å½•
result_dir = "/content/drive/MyDrive/llm_pre_project/dpa_outputs_v8"
os.makedirs(result_dir, exist_ok=True)

# åŠ è½½æ•°æ®é›†
print("ğŸ“¦ Loading prompts from UltraFeedback...")
ds = load_dataset("HuggingFaceH4/ultrafeedback_binarized", split="test_prefs")
prompts = ds["prompt"][:2000]

# âœ… åŠ è½½æ¨¡å‹ï¼ˆç»Ÿä¸€æ”¾åˆ° CUDA ä¸Šï¼‰
print("ğŸ¤– Loading DPA-v1 model...")
model = AutoModelForCausalLM.from_pretrained(
    "Haoxiang-Wang/DPA-v1-Mistral-7B",
    torch_dtype=torch.float16
).to(device)

tokenizer = AutoTokenizer.from_pretrained("Haoxiang-Wang/RewardModel-Mistral-7B-for-DPA-v1")
tokenizer.padding_side = "left"
if tokenizer.pad_token_id is None:
    tokenizer.pad_token = tokenizer.eos_token

# æ„é€  Prompt
def build_input(prompt, v1, v2):
    h = int(np.round(v1 * 100))
    v = int(np.round(v2 * 100))
    sys_instruction = f"You are a helpful assistant. Your response should maximize weighted rating = helpfulness*{h} + verbosity*{v}."
    return [{"role": "user", "content": f"{sys_instruction}\n\n{prompt}"}]

# âœ… ä¿®å¤åçš„ batch ç”Ÿæˆå‡½æ•°
def generate_response_batch(prompts_batch, prompt_ids_batch, v1, v2):
    input_ids_list = []
    for prompt in prompts_batch:
        messages = build_input(prompt, v1, v2)
        input_ids = tokenizer.apply_chat_template(
            messages, add_generation_prompt=True, return_tensors="pt"
        )[0].to(device)
        input_ids_list.append(input_ids)

    input_ids_padded = torch.nn.utils.rnn.pad_sequence(
        input_ids_list, batch_first=True, padding_value=tokenizer.pad_token_id
    ).to(device)

    attention_mask = (input_ids_padded != tokenizer.pad_token_id).to(device)

    with torch.no_grad():
        outputs = model.generate(
            input_ids=input_ids_padded,
            attention_mask=attention_mask,
            max_new_tokens=2048,
            temperature=0.7,
            do_sample=True,
            num_return_sequences=3,
            pad_token_id=tokenizer.eos_token_id
        )

    responses = []
    for i in range(len(prompts_batch)):
        prompt_responses = []
        for j in range(3):
            idx = i * 3 + j
            generated_tokens = outputs[idx][input_ids_padded.shape[1]:]
            decoded = tokenizer.decode(generated_tokens, skip_special_tokens=True)
            prompt_responses.append(decoded)
        responses.append({
            "prompt_id": prompt_ids_batch[i],
            "prompt": prompts_batch[i],
            "responses": prompt_responses
        })
    return responses

# ä¸»å¾ªç¯ï¼šæ–­ç‚¹ç»­è·‘ + å¢é‡ä¿å­˜
angles = np.linspace(0, np.pi / 4, 10)
batch_size = 8

for v_index in v_indices:
    v1, v2 = np.cos(angles[v_index - 1]), np.sin(angles[v_index - 1])
    output_file = os.path.join(result_dir, f"batch_v{v_index}.csv")
    print(f"\nğŸ§­ Running v{v_index}: v = ({v1:.4f}, {v2:.4f})")

    if os.path.exists(output_file):
        existing_df = pd.read_csv(output_file)
        done_prompt_ids = set(existing_df["prompt_id"].unique())
        results = existing_df.to_dict("records")
        print(f"ğŸ” Loaded {len(done_prompt_ids)} prompts from previous run.")
    else:
        done_prompt_ids = set()
        results = []

    start_time = time.time()

    for start in tqdm(range(0, len(prompts), batch_size), desc=f"Generating batch_v{v_index}"):
        end = min(start + batch_size, len(prompts))
        prompt_batch = prompts[start:end]
        prompt_ids = list(range(start, end))

        if all(pid in done_prompt_ids for pid in prompt_ids):
            continue

        try:
            batch_results = generate_response_batch(prompt_batch, prompt_ids, v1, v2)
            new_rows = []
            for item in batch_results:
                for k, resp in enumerate(item["responses"]):
                    row = {
                        "prompt_id": item["prompt_id"],
                        "v1": float(v1),
                        "v2": float(v2),
                        "prompt": item["prompt"],
                        "response_id": k + 1,
                        "response": resp
                    }
                    results.append(row)
                    new_rows.append(row)

            pd.DataFrame(new_rows).to_csv(output_file, mode='a', header=not os.path.exists(output_file), index=False)
        except Exception as e:
            print(f"âš ï¸ Error at batch {start}-{end}: {e}")

    print(f"âœ… Final saved {len(results)} responses to {output_file}")
    print(f"ğŸ•’ Time: {time.time() - start_time:.1f}s")

# âœ… å…ˆè®¾ç½®ä½ æƒ³è·‘çš„ v index
v_indices = [10,6]

from google.colab import drive
drive.mount('/content/drive')

import os
import numpy as np
import pandas as pd
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM
from tqdm.auto import tqdm
import torch
import time
import random

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# è®¾ç½®éšæœºç§å­
seed = 42
np.random.seed(seed)
random.seed(seed)
torch.manual_seed(seed)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(seed)

# è¾“å‡ºç›®å½•
result_dir = "/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9"
os.makedirs(result_dir, exist_ok=True)

# åŠ è½½æ•°æ®é›†
print("ğŸ“¦ Loading prompts from UltraFeedback...")
ds = load_dataset("HuggingFaceH4/ultrafeedback_binarized", split="test_prefs")
prompts = ds["prompt"][:2000]

# åŠ è½½æ¨¡å‹
print("ğŸ¤– Loading DPA-v1 model...")
model = AutoModelForCausalLM.from_pretrained(
    "Haoxiang-Wang/DPA-v1-Mistral-7B",
    torch_dtype=torch.float16
).to(device)

tokenizer = AutoTokenizer.from_pretrained("Haoxiang-Wang/RewardModel-Mistral-7B-for-DPA-v1")
tokenizer.padding_side = "left"
if tokenizer.pad_token_id is None:
    tokenizer.pad_token = tokenizer.eos_token

# æ„é€  Prompt
def build_input(prompt, v1, v2):
    h = int(np.round(v1 * 100))
    v = int(np.round(v2 * 100))
    sys_instruction = f"You are a helpful assistant. Your response should maximize weighted rating = helpfulness*{h} + verbosity*{v}."
    return [{"role": "user", "content": f"{sys_instruction}\n\n{prompt}"}]

# âœ… æ”¯æŒåŠ¨æ€ max_new_tokens
def generate_response_batch(prompts_batch, prompt_ids_batch, v1, v2):
    input_ids_list = []
    individual_input_ids = []

    for prompt in prompts_batch:
        messages = build_input(prompt, v1, v2)
        input_ids = tokenizer.apply_chat_template(
            messages, add_generation_prompt=True, return_tensors="pt"
        )[0]
        individual_input_ids.append(input_ids)
        input_ids_list.append(input_ids.to(device))

    input_ids_padded = torch.nn.utils.rnn.pad_sequence(
        input_ids_list, batch_first=True, padding_value=tokenizer.pad_token_id
    )
    input_ids_padded = input_ids_padded.to(device)

    attention_mask = (input_ids_padded != tokenizer.pad_token_id).to(device)

    max_input_len = input_ids_padded.shape[1]
    dynamic_max_new_tokens = min(2048, 4096 - max_input_len)

    with torch.no_grad():
        outputs = model.generate(
            input_ids=input_ids_padded,
            attention_mask=attention_mask,
            max_new_tokens=dynamic_max_new_tokens,
            temperature=0.7,
            do_sample=True,
            num_return_sequences=3,
            pad_token_id=tokenizer.eos_token_id
        )

    responses = []
    for i in range(len(prompts_batch)):
        prompt_responses = []
        for j in range(3):
            idx = i * 3 + j
            generated_tokens = outputs[idx][input_ids_padded.shape[1]:]
            decoded = tokenizer.decode(generated_tokens, skip_special_tokens=True)
            prompt_responses.append(decoded)
        responses.append({
            "prompt_id": prompt_ids_batch[i],
            "prompt": prompts_batch[i],
            "responses": prompt_responses
        })
    return responses

# ä¸»å¾ªç¯ï¼šæ–­ç‚¹ç»­è·‘ + å¢é‡ä¿å­˜
angles = np.linspace(0, np.pi / 4, 10)
batch_size = 8

for v_index in v_indices:
    v1, v2 = np.cos(angles[v_index - 1]), np.sin(angles[v_index - 1])
    output_file = os.path.join(result_dir, f"batch_v{v_index}.csv")
    print(f"\nğŸ§­ Running v{v_index}: v = ({v1:.4f}, {v2:.4f})")

    if os.path.exists(output_file):
        existing_df = pd.read_csv(output_file)
        done_prompt_ids = set(existing_df["prompt_id"].unique())
        results = existing_df.to_dict("records")
        print(f"ğŸ” Loaded {len(done_prompt_ids)} prompts from previous run.")
    else:
        done_prompt_ids = set()
        results = []

    start_time = time.time()

    for start in tqdm(range(0, len(prompts), batch_size), desc=f"Generating batch_v{v_index}"):
        end = min(start + batch_size, len(prompts))
        prompt_batch = prompts[start:end]
        prompt_ids = list(range(start, end))

        if all(pid in done_prompt_ids for pid in prompt_ids):
            continue

        try:
            batch_results = generate_response_batch(prompt_batch, prompt_ids, v1, v2)
            new_rows = []
            for item in batch_results:
                for k, resp in enumerate(item["responses"]):
                    row = {
                        "prompt_id": item["prompt_id"],
                        "v1": float(v1),
                        "v2": float(v2),
                        "prompt": item["prompt"],
                        "response_id": k + 1,
                        "response": resp
                    }
                    results.append(row)
                    new_rows.append(row)

            pd.DataFrame(new_rows).to_csv(output_file, mode='a', header=not os.path.exists(output_file), index=False)
        except Exception as e:
            print(f"âš ï¸ Error at batch {start}-{end}: {e}")

    print(f"âœ… Final saved {len(results)} responses to {output_file}")
    print(f"ğŸ•’ Time: {time.time() - start_time:.1f}s")

import os
import pandas as pd
import numpy as np
import torch
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForCausalLM

# === æ¨¡å‹é…ç½® ===
device = "cuda" if torch.cuda.is_available() else "cpu"

model = AutoModelForCausalLM.from_pretrained(
    "Haoxiang-Wang/DPA-v1-Mistral-7B",
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
    device_map="auto" if torch.cuda.is_available() else None
).to(device)

tokenizer = AutoTokenizer.from_pretrained("Haoxiang-Wang/DPA-v1-Mistral-7B")
tokenizer.padding_side = "left"
if tokenizer.pad_token_id is None:
    tokenizer.pad_token = tokenizer.eos_token

# === Prompt æ„é€ å‡½æ•° ===
def build_input(prompt, v1, v2):
    h = int(np.round(v1 * 100))
    v = int(np.round(v2 * 100))
    sys_instruction = f"You are a helpful assistant. Your response should maximize weighted rating = helpfulness*{h} + verbosity*{v}."
    return [{"role": "system", "content": sys_instruction}, {"role": "user", "content": prompt}]

# === ç”Ÿæˆå“åº” ===
def regenerate_response(prompt, v1, v2):
    try:
        messages = build_input(prompt, v1, v2)
        input_ids = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors="pt").to(device)

        if input_ids.ndim != 2 or input_ids.shape[1] == 0:
            print(f"âš ï¸ Invalid input_ids shape {input_ids.shape}, skipping: {prompt[:60]}")
            return ""

        max_input_len = input_ids.shape[1]
        max_new_tokens = min(2048, 4096 - max_input_len)

        outputs = model.generate(
            input_ids=input_ids,
            max_new_tokens=max_new_tokens,
            temperature=0.7,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
        generated = outputs[0][input_ids.shape[1]:]
        return tokenizer.decode(generated, skip_special_tokens=True).strip()

    except Exception as e:
        print(f"âŒ Error: {e} \nPrompt: {prompt[:60]}")
        return ""

# === æ‰¹é‡ä¿®å¤ CSV ===
def fix_csv_responses(csv_path, save_dir):
    print(f"\nğŸ“‚ æ£€æŸ¥ï¼š{csv_path}")
    df = pd.read_csv(csv_path)
    blank_mask = df["response"].isna() | (df["response"].str.strip() == "")
    num_blank = blank_mask.sum()

    if num_blank == 0:
        print("âœ… æ— ç©ºç™½ responseï¼Œè·³è¿‡ã€‚")
        return

    print(f"ğŸ”§ å‘ç°ç©ºç™½ response æ•°é‡ï¼š{num_blank}")
    for i in tqdm(df[blank_mask].index, desc="ä¿®å¤ä¸­"):
        row = df.loc[i]
        new_response = regenerate_response(row["prompt"], row["v1"], row["v2"])
        df.at[i, "response"] = new_response

    # è¾“å‡ºè·¯å¾„
    os.makedirs(save_dir, exist_ok=True)
    file_name = os.path.basename(csv_path).replace(".csv", "_fixed.csv")
    save_path = os.path.join(save_dir, file_name)
    df.to_csv(save_path, index=False)
    print(f"âœ… ä¿®å¤å®Œæˆ â†’ ä¿å­˜åˆ°: {save_path}")

# === ä¸»é€»è¾‘ï¼šéå†æ‰€æœ‰ csv æ–‡ä»¶ ===
input_dir = "/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9"
output_dir = os.path.join(input_dir, "fixed_batch")

for file in sorted(os.listdir(input_dir)):
    if file.endswith(".csv") and file.startswith("batch_v"):
        fix_csv_responses(os.path.join(input_dir, file), output_dir)

#ä¸‹é¢æ˜¯åªå¯¹v7åšä¸‹æµ‹è¯•

import os
import pandas as pd
import numpy as np
import torch
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForCausalLM
from accelerate import Accelerator

# === æ¨¡å‹é…ç½® ===
device = "cuda" if torch.cuda.is_available() else "cpu"
accelerator = Accelerator()

model = AutoModelForCausalLM.from_pretrained(
    "Haoxiang-Wang/DPA-v1-Mistral-7B",
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
    device_map="auto" if torch.cuda.is_available() else None
)

# ä½¿ç”¨ accelerator æ¥å¤„ç†è®¾å¤‡æ˜ å°„
model = accelerator.prepare(model)

tokenizer = AutoTokenizer.from_pretrained("Haoxiang-Wang/DPA-v1-Mistral-7B")
tokenizer.padding_side = "left"
if tokenizer.pad_token_id is None:
    tokenizer.pad_token = tokenizer.eos_token

# === åˆ¤æ–­ response æ˜¯å¦éœ€è¦é‡ç”Ÿæˆ ===
def is_invalid_response(resp):
    if pd.isna(resp):
        return True
    resp = str(resp).strip()
    if not resp:
        return True

    invalid_keywords = [
        "[A:]", "[/SOLUTION]", "[ASS]", "[A]:", "[/CONV]", "[/CODE]", "[/USER]",
        "[OUTPUT]", "[CC]", "[OUT]", "[/DATA]", "[/ASST]", "[/ME]", "[/REST]",
        "[RESP]", "<[user]>", "<|user|>", "[Response]", "[Assistant]", "[Answer]",
        "[End of Response]", "<human>", "<|endoftext|>", "<<", "[INST]", "[/INST]",
        "[]", "[USER]"
    ]

    if resp in invalid_keywords:
        return True

    tokens = resp.split()
    if all(token.strip() in invalid_keywords for token in tokens):
        return True

    tag_like_count = sum(1 for t in tokens if (t.startswith("[") and t.endswith("]")) or (t.startswith("<") and t.endswith(">")))
    if len(tokens) > 0 and tag_like_count / len(tokens) > 0.8:
        return True

    return False

# === æ„é€  Prompt ===
def build_input(prompt, v1, v2):
    h = int(np.round(v1 * 100))
    v = int(np.round(v2 * 100))
    sys_instruction = f"You are a helpful assistant. Your response should maximize weighted rating = helpfulness*{h} + verbosity*{v}."
    return [{"role": "system", "content": sys_instruction}, {"role": "user", "content": prompt}]

# === é‡ç”Ÿæˆå“åº” ===
def regenerate_response(prompt, v1, v2):
    try:
        messages = build_input(prompt, v1, v2)
        input_ids = tokenizer.apply_chat_template(
            messages, add_generation_prompt=True, return_tensors="pt"
        ).to(device)

        if input_ids.ndim != 2 or input_ids.shape[1] == 0:
            print(f"âš ï¸ Invalid input_ids shape {input_ids.shape}, skipping: {prompt[:60]}")
            return ""

        outputs = model.generate(
            input_ids=input_ids,
            temperature=0.7,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
        generated = outputs[0][input_ids.shape[1]:]
        return tokenizer.decode(generated, skip_special_tokens=True).strip()

    except Exception as e:
        print(f"âŒ Error: {e} \nPrompt: {prompt[:60]}")
        return ""

# === ä¿®å¤å•ä¸ªæ–‡ä»¶ ===
def fix_csv_responses(csv_path, save_dir):
    print(f"\nğŸ“‚ æ£€æŸ¥ï¼š{csv_path}")

    if not os.path.exists(csv_path):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {csv_path}")
        return

    df = pd.read_csv(csv_path)
    bad_mask = df["response"].apply(is_invalid_response)
    num_bad = bad_mask.sum()

    if num_bad == 0:
        print("âœ… æ— éœ€ä¿®å¤ï¼Œè·³è¿‡ã€‚")
        return

    print(f"ğŸ”§ æ£€æµ‹åˆ°æ— æ•ˆ response æ•°é‡ï¼š{num_bad}")
    for i in tqdm(bad_mask[bad_mask].index, total=num_bad, desc=os.path.basename(csv_path), dynamic_ncols=True):
        row = df.loc[i]
        new_response = regenerate_response(row["prompt"], row["v1"], row["v2"])
        df.at[i, "response"] = new_response

    os.makedirs(save_dir, exist_ok=True)
    file_name = os.path.basename(csv_path).replace(".csv", "_fixed.csv")
    save_path = os.path.join(save_dir, file_name)
    df.to_csv(save_path, index=False)
    print(f"âœ… ä¿®å¤å®Œæˆ â†’ ä¿å­˜åˆ°: {save_path}")

# === ä¸»é€»è¾‘ ===
input_dir = "drive/MyDrive/llm_pre_project/dpa_outputs_v9"
output_dir = os.path.join(input_dir, "fixed_batch")

# åªä¿®å¤ batch_v4_2.csv
input_file = "batch_v7_2.csv"

# ä¿®å¤è¯¥æ–‡ä»¶
fix_csv_responses(os.path.join(input_dir, input_file), output_dir)



# âœ… è®¾ç½® v_index ä¸º11ï¼Œè¡¨ç¤º v11 = (0.95, -0.31)
v_indices = [11]

from google.colab import drive
drive.mount('/content/drive')

import os
import numpy as np
import pandas as pd
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM
from tqdm.auto import tqdm
import torch
import time
import random

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# è®¾ç½®éšæœºç§å­
seed = 42
np.random.seed(seed)
random.seed(seed)
torch.manual_seed(seed)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(seed)

# è¾“å‡ºç›®å½•
result_dir = "/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9"
os.makedirs(result_dir, exist_ok=True)

# åŠ è½½æ•°æ®é›†
print("ğŸ“¦ Loading prompts from UltraFeedback...")
ds = load_dataset("HuggingFaceH4/ultrafeedback_binarized", split="test_prefs")
prompts = ds["prompt"][:2000]

# åŠ è½½æ¨¡å‹
print("ğŸ¤– Loading DPA-v1 model...")
model = AutoModelForCausalLM.from_pretrained(
    "Haoxiang-Wang/DPA-v1-Mistral-7B",
    torch_dtype=torch.float16
).to(device)

tokenizer = AutoTokenizer.from_pretrained("Haoxiang-Wang/RewardModel-Mistral-7B-for-DPA-v1")
tokenizer.padding_side = "left"
if tokenizer.pad_token_id is None:
    tokenizer.pad_token = tokenizer.eos_token

# æ„é€  Prompt
def build_input(prompt, v1, v2):
    h = int(np.round(v1 * 100))
    v = int(np.round(v2 * 100))
    sys_instruction = f"You are a helpful assistant. Your response should maximize weighted rating = helpfulness*{h} + verbosity*{v}."
    return [{"role": "user", "content": f"{sys_instruction}\n\n{prompt}"}]

# âœ… æ”¯æŒåŠ¨æ€ max_new_tokens
def generate_response_batch(prompts_batch, prompt_ids_batch, v1, v2):
    input_ids_list = []
    individual_input_ids = []

    for prompt in prompts_batch:
        messages = build_input(prompt, v1, v2)
        input_ids = tokenizer.apply_chat_template(
            messages, add_generation_prompt=True, return_tensors="pt"
        )[0]
        individual_input_ids.append(input_ids)
        input_ids_list.append(input_ids.to(device))

    input_ids_padded = torch.nn.utils.rnn.pad_sequence(
        input_ids_list, batch_first=True, padding_value=tokenizer.pad_token_id
    )
    input_ids_padded = input_ids_padded.to(device)

    attention_mask = (input_ids_padded != tokenizer.pad_token_id).to(device)

    max_input_len = input_ids_padded.shape[1]
    dynamic_max_new_tokens = min(2048, 4096 - max_input_len)

    with torch.no_grad():
        outputs = model.generate(
            input_ids=input_ids_padded,
            attention_mask=attention_mask,
            max_new_tokens=dynamic_max_new_tokens,
            temperature=0.7,
            do_sample=True,
            num_return_sequences=3,
            pad_token_id=tokenizer.eos_token_id
        )

    responses = []
    for i in range(len(prompts_batch)):
        prompt_responses = []
        for j in range(3):
            idx = i * 3 + j
            generated_tokens = outputs[idx][input_ids_padded.shape[1]:]
            decoded = tokenizer.decode(generated_tokens, skip_special_tokens=True)
            prompt_responses.append(decoded)
        responses.append({
            "prompt_id": prompt_ids_batch[i],
            "prompt": prompts_batch[i],
            "responses": prompt_responses
        })
    return responses

# ä¸»å¾ªç¯ï¼šæ–­ç‚¹ç»­è·‘ + å¢é‡ä¿å­˜
angles = np.linspace(0, np.pi / 4, 10)
batch_size = 8

# åªè·‘v11 = (0.95, -0.31)
for v_index in v_indices:
    v1, v2 = 0.95, -0.31  # ç›´æ¥è®¾ç½®v11
    output_file = os.path.join(result_dir, f"batch_v{v_index}.csv")
    print(f"\nğŸ§­ Running v{v_index}: v = ({v1:.4f}, {v2:.4f})")

    if os.path.exists(output_file):
        existing_df = pd.read_csv(output_file)
        done_prompt_ids = set(existing_df["prompt_id"].unique())
        results = existing_df.to_dict("records")
        print(f"ğŸ” Loaded {len(done_prompt_ids)} prompts from previous run.")
    else:
        done_prompt_ids = set()
        results = []

    start_time = time.time()

    for start in tqdm(range(0, len(prompts), batch_size), desc=f"Generating batch_v{v_index}"):
        end = min(start + batch_size, len(prompts))
        prompt_batch = prompts[start:end]
        prompt_ids = list(range(start, end))

        if all(pid in done_prompt_ids for pid in prompt_ids):
            continue

        try:
            batch_results = generate_response_batch(prompt_batch, prompt_ids, v1, v2)
            new_rows = []
            for item in batch_results:
                for k, resp in enumerate(item["responses"]):
                    row = {
                        "prompt_id": item["prompt_id"],
                        "v1": float(v1),
                        "v2": float(v2),
                        "prompt": item["prompt"],
                        "response_id": k + 1,
                        "response": resp
                    }
                    results.append(row)
                    new_rows.append(row)

            pd.DataFrame(new_rows).to_csv(output_file, mode='a', header=not os.path.exists(output_file), index=False)
        except Exception as e:
            print(f"âš ï¸ Error at batch {start}-{end}: {e}")

    print(f"âœ… Final saved {len(results)} responses to {output_file}")
    print(f"ğŸ•’ Time: {time.time() - start_time:.1f}s")

#v11é‡æ–°è·‘çš„ç‰ˆæœ¬

from google.colab import drive
drive.mount('/content/drive')

from google.colab import drive
drive.mount('/content/drive')

import os
import numpy as np
import pandas as pd
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM
from tqdm.auto import tqdm
import torch
import time
import random

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# è®¾ç½®éšæœºç§å­
seed = 42
np.random.seed(seed)
random.seed(seed)
torch.manual_seed(seed)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(seed)

# è¾“å‡ºç›®å½•
result_dir = "/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9"
os.makedirs(result_dir, exist_ok=True)

# åŠ è½½æ•°æ®é›†
print("ğŸ“¦ Loading prompts from UltraFeedback...")
ds = load_dataset("HuggingFaceH4/ultrafeedback_binarized", split="test_prefs")
prompts = ds["prompt"][:2000]

# torch.cuda.empty_cache()
# torch.cuda.ipc_collect()

# åŠ è½½æ¨¡å‹
print("ğŸ¤– Loading DPA-v1 model...")
model = AutoModelForCausalLM.from_pretrained(
    "Haoxiang-Wang/DPA-v1-Mistral-7B",
    torch_dtype=torch.float16,
    device_map="auto"
).to(device)

tokenizer = AutoTokenizer.from_pretrained("Haoxiang-Wang/RewardModel-Mistral-7B-for-DPA-v1")
tokenizer.padding_side = "left"
if tokenizer.pad_token_id is None:
    tokenizer.pad_token = tokenizer.eos_token

# æ„é€  Prompt
def build_input(pro mpt, v1, v2):
    h = int(np.round(v1 * 100))
    v = int(np.round(v2 * 100))
    sys_instruction = f"You are a helpful assistant. Your response should maximize weighted rating = helpfulness*{h} + verbosity*{v}."
    return [{"role": "user", "content": f"{sys_instruction}\n\n{prompt}"}]

# âœ… æ”¯æŒåŠ¨æ€ max_new_tokens
def generate_response_batch(prompts_batch, prompt_ids_batch, v1, v2):
    input_ids_list = []
    individual_input_ids = []

    for prompt in prompts_batch:
        messages = build_input(prompt, v1, v2)
        input_ids = tokenizer.apply_chat_template(
            messages, add_generation_prompt=True, return_tensors="pt"
        )[0]
        individual_input_ids.append(input_ids)
        input_ids_list.append(input_ids.to(device))

    input_ids_padded = torch.nn.utils.rnn.pad_sequence(
        input_ids_list, batch_first=True, padding_value=tokenizer.pad_token_id
    )
    input_ids_padded = input_ids_padded.to(device)

    attention_mask = (input_ids_padded != tokenizer.pad_token_id).to(device)

    max_input_len = input_ids_padded.shape[1]
    dynamic_max_new_tokens = min(2048, 4096 - max_input_len)

    with torch.no_grad():
        outputs = model.generate(
            input_ids=input_ids_padded,
            attention_mask=attention_mask,
            max_new_tokens=dynamic_max_new_tokens,
            temperature=0.7,
            do_sample=True,
            num_return_sequences=3,
            pad_token_id=tokenizer.eos_token_id
        )

    responses = []
    for i in range(len(prompts_batch)):
        prompt_responses = []
        for j in range(3):
            idx = i * 3 + j
            generated_tokens = outputs[idx][input_ids_padded.shape[1]:]
            decoded = tokenizer.decode(generated_tokens, skip_special_tokens=True)
            prompt_responses.append(decoded)
        responses.append({
            "prompt_id": prompt_ids_batch[i],
            "prompt": prompts_batch[i],
            "responses": prompt_responses
        })
    return responses

# ä¸»å¾ªç¯ï¼šæ–­ç‚¹ç»­è·‘ + å¢é‡ä¿å­˜
angles = np.linspace(0, np.pi / 4, 10)
batch_size = 8
v_indices = [11]  # åªè·‘ v11
v1, v2 = 0.95, -0.31

for v_index in v_indices:
    output_file = os.path.join(result_dir, f"batch_v{v_index}.csv")
    print(f"\nğŸ§­ Running v{v_index}: v = ({v1:.4f}, {v2:.4f})")

    if os.path.exists(output_file):
        existing_df = pd.read_csv(output_file)
        done_prompt_ids = set(existing_df["prompt_id"].unique())
        results = existing_df.to_dict("records")
        print(f"ğŸ” Loaded {len(done_prompt_ids)} prompts from previous run.")
    else:
        done_prompt_ids = set()
        results = []

    start_time = time.time()

    for start in tqdm(range(0, len(prompts), batch_size), desc=f"Generating batch_v{v_index}"):
        end = min(start + batch_size, len(prompts))
        prompt_batch_all = prompts[start:end]
        prompt_ids_all = list(range(start, end))

        # ğŸ”§ ä¿®æ”¹ç‚¹ï¼šåªå¤„ç†æœªå®Œæˆçš„ prompts
        unprocessed_indices = [i for i, pid in enumerate(prompt_ids_all) if pid not in done_prompt_ids]
        if not unprocessed_indices:
            continue

        prompt_batch = [prompt_batch_all[i] for i in unprocessed_indices]
        prompt_ids = [prompt_ids_all[i] for i in unprocessed_indices]

        try:
            batch_results = generate_response_batch(prompt_batch, prompt_ids, v1, v2)
            new_rows = []
            for item in batch_results:
                for k, resp in enumerate(item["responses"]):
                    row = {
                        "prompt_id": item["prompt_id"],
                        "v1": float(v1),
                        "v2": float(v2),
                        "prompt": item["prompt"],
                        "response_id": k + 1,
                        "response": resp
                    }
                    results.append(row)
                    new_rows.append(row)

            pd.DataFrame(new_rows).to_csv(output_file, mode='a', header=not os.path.exists(output_file), index=False)
        except Exception as e:
            print(f"âš ï¸ Error at batch {start}-{end}: {e}")

    print(f"âœ… Final saved {len(results)} responses to {output_file}")
    print(f"ğŸ•’ Time: {time.time() - start_time:.1f}s")

import os
import pandas as pd
import numpy as np
import torch
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForCausalLM
from accelerate import Accelerator

# === æ¨¡å‹é…ç½® ===
device = "cuda" if torch.cuda.is_available() else "cpu"
accelerator = Accelerator()

model = AutoModelForCausalLM.from_pretrained(
    "Haoxiang-Wang/DPA-v1-Mistral-7B",
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
    device_map="auto" if torch.cuda.is_available() else None
)

# ä½¿ç”¨ accelerator æ¥å¤„ç†è®¾å¤‡æ˜ å°„
model = accelerator.prepare(model)

tokenizer = AutoTokenizer.from_pretrained("Haoxiang-Wang/DPA-v1-Mistral-7B")
tokenizer.padding_side = "left"
if tokenizer.pad_token_id is None:
    tokenizer.pad_token = tokenizer.eos_token

# === åˆ¤æ–­ response æ˜¯å¦éœ€è¦é‡ç”Ÿæˆ ===
def is_invalid_response(resp):
    if pd.isna(resp):
        return True
    resp = str(resp).strip()
    if not resp:
        return True

    invalid_keywords = [
        "[A:]", "[/SOLUTION]", "[ASS]", "[A]:", "[/CONV]", "[/CODE]", "[/USER]",
        "[OUTPUT]", "[CC]", "[OUT]", "[/DATA]", "[/ASST]", "[/ME]", "[/REST]",
        "[RESP]", "<[user]>", "<|user|>", "[Response]", "[Assistant]", "[Answer]",
        "[End of Response]", "<human>", "<|endoftext|>", "<<", "[INST]", "[/INST]",
        "[]", "[USER]"
    ]

    if resp in invalid_keywords:
        return True

    tokens = resp.split()
    if all(token.strip() in invalid_keywords for token in tokens):
        return True

    tag_like_count = sum(1 for t in tokens if (t.startswith("[") and t.endswith("]")) or (t.startswith("<") and t.endswith(">")))
    if len(tokens) > 0 and tag_like_count / len(tokens) > 0.8:
        return True

    return False

# === æ„é€  Prompt ===
def build_input(prompt, v1, v2):
    h = int(np.round(v1 * 100))
    v = int(np.round(v2 * 100))
    sys_instruction = f"You are a helpful assistant. Your response should maximize weighted rating = helpfulness*{h} + verbosity*{v}."
    return [{"role": "system", "content": sys_instruction}, {"role": "user", "content": prompt}]

# === é‡ç”Ÿæˆå“åº” ===
def regenerate_response(prompt, v1, v2):
    try:
        messages = build_input(prompt, v1, v2)
        input_ids = tokenizer.apply_chat_template(
            messages, add_generation_prompt=True, return_tensors="pt"
        ).to(device)

        if input_ids.ndim != 2 or input_ids.shape[1] == 0:
            print(f"âš ï¸ Invalid input_ids shape {input_ids.shape}, skipping: {prompt[:60]}")
            return ""

        outputs = model.generate(
            input_ids=input_ids,
            temperature=0.7,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
        generated = outputs[0][input_ids.shape[1]:]
        return tokenizer.decode(generated, skip_special_tokens=True).strip()

    except Exception as e:
        print(f"âŒ Error: {e} \nPrompt: {prompt[:60]}")
        return ""

# === ä¿®å¤å•ä¸ªæ–‡ä»¶ ===
def fix_csv_responses(csv_path, save_dir):
    print(f"\nğŸ“‚ æ£€æŸ¥ï¼š{csv_path}")

    if not os.path.exists(csv_path):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {csv_path}")
        return

    df = pd.read_csv(csv_path)
    bad_mask = df["response"].apply(is_invalid_response)
    num_bad = bad_mask.sum()

    if num_bad == 0:
        print("âœ… æ— éœ€ä¿®å¤ï¼Œè·³è¿‡ã€‚")
        return

    print(f"ğŸ”§ æ£€æµ‹åˆ°æ— æ•ˆ response æ•°é‡ï¼š{num_bad}")
    for i in tqdm(bad_mask[bad_mask].index, total=num_bad, desc=os.path.basename(csv_path), dynamic_ncols=True):
        row = df.loc[i]
        new_response = regenerate_response(row["prompt"], row["v1"], row["v2"])
        df.at[i, "response"] = new_response

    os.makedirs(save_dir, exist_ok=True)
    file_name = os.path.basename(csv_path).replace(".csv", "_fixed.csv")
    save_path = os.path.join(save_dir, file_name)
    df.to_csv(save_path, index=False)
    print(f"âœ… ä¿®å¤å®Œæˆ â†’ ä¿å­˜åˆ°: {save_path}")

# === ä¸»é€»è¾‘ ===
input_dir = "drive/MyDrive/llm_pre_project/dpa_outputs_v9"
output_dir = os.path.join(input_dir, "fixed_batch")

# åªä¿®å¤ batch_v4_2.csv
input_file = "batch_v11.csv"

# ä¿®å¤è¯¥æ–‡ä»¶
fix_csv_responses(os.path.join(input_dir, input_file), output_dir)

#æ‰“åˆ†

# === å¯¼å…¥åº“ ===
import os
import time
import pandas as pd
import numpy as np
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
from tqdm.auto import tqdm
from google.colab import drive

# === æŒ‚è½½ Google Drive ===
drive.mount('/content/drive')

# === è®¾ç½®è¾“å…¥è¾“å‡ºè·¯å¾„ ===
input_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/fixed_batch/batch_v11_fixed.csv"
output_dir = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch"
os.makedirs(output_dir, exist_ok=True)

# === åŠ è½½æ•°æ® ===
print(f"ğŸ“¦ Loading file: {input_path}")
df = pd.read_csv(input_path)

# === åŠ è½½ Reward Model ===
print("ğŸ¤– Loading Reward Model...")
device = "cuda" if torch.cuda.is_available() else "cpu"
rm = AutoModelForSequenceClassification.from_pretrained(
    "Haoxiang-Wang/RewardModel-Mistral-7B-for-DPA-v1", trust_remote_code=True
).to(device)
tokenizer = AutoTokenizer.from_pretrained("Haoxiang-Wang/RewardModel-Mistral-7B-for-DPA-v1")

def score_response(prompt, response):
    template = "[INST] You must read the following conversation carefully and rate the assistant's response from score 0-100 in these aspects: helpfulness, correctness, coherence, honesty, complexity, verbosity\n\nUser: {prompt}\n\nAssistant: {response} [/INST]"
    inputs = tokenizer(template.format(prompt=prompt, response=response), return_tensors="pt").to(device)
    with torch.no_grad():
        logits = rm(**inputs).logits.squeeze().cpu().numpy()
    return logits[9], logits[4]  # helpfulness, verbosity

# === å¼€å§‹æ‰“åˆ† ===
print("ğŸ§  Scoring responses...")
start_time = time.time()
scores = []

for i, row in tqdm(df.iterrows(), total=len(df), desc="ğŸ” Scoring"):
    try:
        h, v = score_response(row["prompt"], row["response"])
        scores.append({"v1": row["v1"], "v2": row["v2"], "helpfulness": h, "verbosity": v})
    except Exception as e:
        print(f"âš ï¸ Error on row {i}: {e}")
        scores.append({"v1": row["v1"], "v2": row["v2"], "helpfulness": None, "verbosity": None})

df_scores = pd.DataFrame(scores)
df_all = pd.concat([df, df_scores], axis=1)

# === ä¿å­˜ç»“æœ ===
output_file = os.path.join(output_dir, os.path.basename(input_path).replace(".csv", "_scored.csv"))
df_all.to_csv(output_file, index=False)

end_time = time.time()
print(f"\nâœ… Scoring complete! Total scored: {len(df_all)}")
print(f"ğŸ•’ Time elapsed: {end_time - start_time:.1f} seconds")
print(f"ğŸ“ Saved to: {output_file}")

#åŠ å¹²æ‰°

import pandas as pd
import numpy as np
import os
from tqdm import tqdm
from google.colab import drive

# === æŒ‚è½½ Google Drive ===
drive.mount('/content/drive')

# === è®¾ç½®è¾“å…¥è¾“å‡ºè·¯å¾„ ===
input_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch/batch_v11_fixed_scored.csv"
output_folder = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/generate_perturbe"
os.makedirs(output_folder, exist_ok=True)

# === åŠ è½½æ•°æ® ===
data = pd.read_csv(input_path)

# === å®šä¹‰æ—‹è½¬æ‰°åŠ¨å‡½æ•°ï¼ˆè§’åº¦èŒƒå›´ Â±30Â°ï¼‰===
def apply_random_perturbation(v1, v2, max_angle_deg=45):
    theta = np.radians(np.random.uniform(-max_angle_deg, max_angle_deg))  # è§’åº¦èŒƒå›´ [-15Â°, 15Â°]
    cos_t, sin_t = np.cos(theta), np.sin(theta)
    # æ—‹è½¬çŸ©é˜µä¹˜ä»¥å‘é‡ (v1, v2)
    new_v1 = v1 * cos_t - v2 * sin_t
    new_v2 = v1 * sin_t + v2 * cos_t
    # å•ä½å‘é‡å½’ä¸€åŒ–ï¼ˆå¯é€‰ï¼‰
    norm = np.sqrt(new_v1**2 + new_v2**2)
    return new_v1 / norm, new_v2 / norm

# === ç”Ÿæˆæ‰°åŠ¨æ•°æ® ===
all_perturbed_data = []

for _, row in tqdm(data.iterrows(), total=len(data), desc="ğŸ”„ Generating Perturbations"):
    v1, v2 = row['v1'], row['v2']

    for _ in range(10):  # ç”Ÿæˆ10ä¸ªæ‰°åŠ¨æ–¹å‘
        perturbed_v1, perturbed_v2 = apply_random_perturbation(v1, v2)

        perturbed_row = row.copy()
        perturbed_row['perturbed_v1'] = perturbed_v1
        perturbed_row['perturbed_v2'] = perturbed_v2
        all_perturbed_data.append(perturbed_row)

# === ä¿å­˜ç»“æœ ===
perturbed_df = pd.DataFrame(all_perturbed_data)
output_path = os.path.join(output_folder, "batch_v11_30.csv")
perturbed_df.to_csv(output_path, index=False)

print(f"\nâœ… æ‰°åŠ¨å¤„ç†å®Œæˆï¼Œä¿å­˜ä¸ºï¼š{output_path}")

#åŠ å¹²æ‰°åæ‰“åˆ†

# === å®‰è£…ä¾èµ–ï¼ˆå¦‚éœ€è¦ï¼‰===
!pip install -q pandas numpy

# === å¯¼å…¥åº“ ===
import pandas as pd
import numpy as np
import os
from google.colab import drive

# === æŒ‚è½½ Google Drive ===
drive.mount('/content/drive')

# === è®¾ç½®è¾“å…¥è¾“å‡ºè·¯å¾„ ===
input_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/generate_perturbe/batch_v11_30.csv"
output_folder = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/generate_perturbe/pertube_score"
os.makedirs(output_folder, exist_ok=True)

# === è¯»å–æ•°æ® ===
print(f"ğŸ“„ æ­£åœ¨è¯»å–æ•°æ®æ–‡ä»¶: {input_path}")
perturbed_df = pd.read_csv(input_path)

# === å®šä¹‰å¾—åˆ†å‡½æ•° ===
def compute_score(v1, v2, helpfulness, verbosity):
    return v1 * helpfulness + v2 * verbosity

# === è®¡ç®— baseline å’Œæ‰°åŠ¨å¾—åˆ† ===
perturbed_df['baseline_score'] = perturbed_df.apply(
    lambda row: compute_score(row['v1'], row['v2'], row['helpfulness'], row['verbosity']),
    axis=1
)

perturbed_df['perturbed_score'] = perturbed_df.apply(
    lambda row: compute_score(row['perturbed_v1'], row['perturbed_v2'], row['helpfulness'], row['verbosity']),
    axis=1
)

# === ä¿å­˜ç»“æœ ===
output_path = os.path.join(output_folder, "pertube_score_v11_with_baseline_30.csv")
perturbed_df.to_csv(output_path, index=False)

print(f"\nâœ… æ‰“åˆ†å®Œæˆï¼Œç»“æœå·²ä¿å­˜åˆ°: {output_path}")

#é€‰å‡ºbaslineå’Œrpsçš„best response

import os
import pandas as pd
import numpy as np
from google.colab import drive

# === æŒ‚è½½ Google Drive ===
drive.mount('/content/drive')

# === è·¯å¾„è®¾ç½® ===
input_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/generate_perturbe/pertube_score/pertube_score_v11_with_baseline_30.csv"
intermediate_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/generate_perturbe/intermediate_with_min_score.csv"
output_folder = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/generate_perturbe/best_response"
os.makedirs(output_folder, exist_ok=True)
output_path = os.path.join(output_folder, "final_best_responses_with_baseline_and_perturbed_v11_30.csv")

# === åŠ è½½æ•°æ® ===
df = pd.read_csv(input_path)
print("âœ… å·²åŠ è½½æ•°æ®ï¼Œåˆ—å¦‚ä¸‹ï¼š")
print(df.columns)

# === æ·»åŠ æ¯ç»„ response çš„æœ€å°æ‰°åŠ¨å¾—åˆ† ===
df['min_perturbed_score'] = df.groupby(['prompt_id', 'response_id'])['perturbed_score'].transform('min')

# === ä¿å­˜ä¸­é—´æ•°æ®ï¼ˆå¯é€‰è°ƒè¯•ç”¨ï¼‰===
df.to_csv(intermediate_path, index=False)

# === é‡æ–°è¯»å–ä¸­é—´æ–‡ä»¶ï¼Œç¡®ä¿å®Œæ•´åˆ—ä¿ç•™ ===
df = pd.read_csv(intermediate_path)

# === æ£€æŸ¥åˆ— ===
if 'min_perturbed_score' not in df.columns:
    raise ValueError("âš ï¸ min_perturbed_score åˆ—æœªæˆåŠŸåˆ›å»ºï¼")
else:
    print("âœ… min_perturbed_score åˆ—å­˜åœ¨")

# === é€‰æ‹© baseline æœ€ä¼˜å“åº”ï¼ˆæ¯ä¸ª prompt_id ä¸‹ baseline_score æœ€å¤§ï¼‰===
baseline_best_responses = df.loc[df.groupby('prompt_id')['baseline_score'].idxmax()].copy()

# === é€‰æ‹©æ‰°åŠ¨æœ€ç¨³å¥å“åº”ï¼ˆmin_perturbed_score æœ€å¤§ï¼‰===
perturbed_best_responses = df.loc[df.groupby('prompt_id')['min_perturbed_score'].idxmax()].copy()

# === åˆå¹¶ç»“æœ ===
final_best_responses = baseline_best_responses[['prompt_id', 'v1', 'v2', 'prompt', 'response_id',
                                                'baseline_score']].copy()

final_best_responses['perturbed_score'] = perturbed_best_responses['min_perturbed_score'].values
final_best_responses['perturbed_best_response'] = perturbed_best_responses['response'].values
final_best_responses['baseline_best_response'] = baseline_best_responses['response'].values
final_best_responses['response_id_perturbed'] = perturbed_best_responses['response_id'].values
final_best_responses['perturbed_v1'] = perturbed_best_responses['perturbed_v1'].values
final_best_responses['perturbed_v2'] = perturbed_best_responses['perturbed_v2'].values

final_best_responses['best_baseline_response_id'] = baseline_best_responses['response_id'].values
final_best_responses['best_perturbed_response_id'] = perturbed_best_responses['response_id'].values

# === ä¿å­˜æœ€ç»ˆç»“æœ ===
final_best_responses.to_csv(output_path, index=False)
print(f"âœ… æœ€ç»ˆç»“æœå·²ä¿å­˜è‡³ï¼š{output_path}")

#check best_response.csv

import pandas as pd

# è·¯å¾„
csv_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/generate_perturbe/best_response/final_best_responses_with_baseline_and_perturbed_v11_30.csv"

# è¯»å– CSV
df = pd.read_csv(csv_path)

# æŸ¥çœ‹åˆ—å
print("ğŸ“‹ åˆ—åå¦‚ä¸‹ï¼š")
print(df.columns.tolist())

#æ”¹æˆgpt4oå½¢å¼

import pandas as pd
import json
import os

# è¾“å…¥è·¯å¾„
input_csv = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/generate_perturbe/best_response/final_best_responses_with_baseline_and_perturbed_v11_15.csv"
output_jsonl = input_csv.replace(".csv", "_gpt4o_eval.jsonl")

# è¯»å–æ•°æ®
df = pd.read_csv(input_csv)

# æ„é€  pairwise æ ¼å¼
pairs = []
tie_count = 0

for _, row in df.iterrows():
    prompt = row['prompt']
    baseline_resp = row['baseline_best_response']
    rps_resp = row['perturbed_best_response']
    baseline_id = row['baseline_best_response_id']
    rps_id = row['perturbed_best_response_id']

    # å¦‚æœ response id ç›¸åŒï¼Œç›´æ¥æ ‡è®°ä¸º Tie
    if baseline_id == rps_id:
        pairs.append({
            "prompt": prompt,
            "response_a": baseline_resp,
            "response_b": rps_resp,
            "auto_result": "Tie"
        })
        tie_count += 1
        continue

    # å¦åˆ™ç”Ÿæˆ GPT æ¯”è¾ƒ prompt
    formatted_prompt = f"""For the following query to a chatbot, which response is more helpful?

Query: {prompt}


Response A: {baseline_resp}

Response B: {rps_resp}

FIRST provide a one-sentence comparison of the two responses and explain which you feel is more helpful.
SECOND, on a new line, state only 'A' or 'B' to indicate which response is more helpful.

Your response should use the format:

Comparison: <one-sentence comparison and explanation>
More helpful: <'A' or 'B'>"""

    pairs.append({
        "prompt": formatted_prompt,
        "auto_result": None
    })

# ä¿å­˜ä¸º JSONL æ ¼å¼
with open(output_jsonl, 'w', encoding='utf-8') as f:
    for item in pairs:
        f.write(json.dumps(item, ensure_ascii=False) + "\n")

print(f"âœ… å·²ç”Ÿæˆ GPT è¯„ä¼°æ ¼å¼æ–‡ä»¶: {output_jsonl}")
print(f"ğŸ¤ å·²è·³è¿‡ {tie_count} æ¡å®Œå…¨ç›¸åŒ responseï¼Œæ ‡è®°ä¸º Tie")

import openai
import os

# è®¾ç½® API Key å’Œä»£ç†åœ°å€ï¼ˆå¦‚æœä½¿ç”¨ä»£ç†ï¼‰
os.environ["OPENAI_API_KEY"] = "sk-XGGe5y0ZvLcQVFp6XnRizs7q47gsVnAbZx0Xr2mfcVlbr99f"
openai.api_key = os.environ["OPENAI_API_KEY"]
openai.api_base = "https://api2.aigcbest.top/v1"  # å¦‚æœç”¨ä»£ç†å°±æ”¹è¿™é‡Œ

print("âœ… OpenAI è®¾ç½®å®Œæˆ")

import os

output_path = "gpt4o_comparisons.jsonl"

if os.path.exists(output_path):
    os.remove(output_path)
    print("âœ… å·²åˆ é™¤æ—§çš„è¯„ä¼°ç»“æœæ–‡ä»¶ï¼Œå‡†å¤‡é‡æ–°å¼€å§‹è¯„ä¼°")
else:
    print("â„¹ï¸ æ²¡æœ‰æ‰¾åˆ°å·²æœ‰è¯„ä¼°æ–‡ä»¶ï¼Œç›´æ¥å¼€å§‹è¯„ä¼°")

import json
import time
import os

jsonl_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/generate_perturbe/best_response/final_best_responses_with_baseline_and_perturbed_v11_15_gpt4o_eval.jsonl"
output_path = "gpt4o_comparisons.jsonl"

# âœ… åŠ è½½è¾“å…¥æ•°æ®
with open(jsonl_path, 'r', encoding='utf-8') as f:
    data = [json.loads(line) for line in f]

print(f"âœ… è¯»å– {len(data)} æ¡ prompt")

# âœ… åŠ è½½å·²æœ‰ç»“æœï¼ˆæ”¯æŒæ–­ç‚¹ç»­è·‘ï¼‰
seen_prompts = set()
results = []

if os.path.exists(output_path):
    with open(output_path, 'r', encoding='utf-8') as f:
        for line in f:
            item = json.loads(line)
            results.append(item)
            seen_prompts.add(item['prompt'])
    print(f"ğŸ” å·²åŠ è½½ {len(seen_prompts)} æ¡å·²å®Œæˆç»“æœï¼Œç»§ç»­è¯„ä¼°æœªå®Œæˆéƒ¨åˆ†")

# âœ… GPT è¯„ä¼°å‡½æ•°
def gpt4o_judge(prompt):
    try:
        res = openai.ChatCompletion.create(
            model="gpt-4o-mini",  # ç¡®ä¿ä½ ç”¨çš„æ˜¯æ­£ç¡®æ¨¡å‹ ID
            messages=[{"role": "user", "content": prompt}],
            temperature=0.0,
            max_tokens=200
        )
        return res['choices'][0]['message']['content']
    except Exception as e:
        print("âŒ GPT è°ƒç”¨å¤±è´¥ï¼š", e)  # â† åŠ è¿™è¡Œï¼
        return f"[ERROR] {e}"


# âœ… æ­£å¼å¼€å§‹è¯„ä¼°
error_count = 0
start_time = time.time()

with open(output_path, "a", encoding="utf-8") as f_out:
    for idx, item in enumerate(data):
        prompt = item["prompt"]

        # â© è·³è¿‡å·²å¤„ç†çš„æ ·æœ¬
        if prompt in seen_prompts:
            continue

        # âœ… åˆ¤æ–­æ˜¯å¦ä¸ºè‡ªåŠ¨ Tie æ ·æœ¬
        if item.get("auto_result") == "Tie":
            result = {
                "prompt": prompt,
                "choice": "Tie",
                "reason": "Responses are identical, skipped GPT evaluation.",
                "response_raw": "More helpful: Tie"
            }
        else:
            reply = gpt4o_judge(prompt)
            lines = reply.strip().splitlines()
            explanation = ""
            choice = ""

            for line in lines:
                if line.lower().startswith("comparison:"):
                    explanation = line[len("comparison:"):].strip()
                elif line.lower().startswith("more helpful:"):
                    choice = line[len("more helpful:"):].strip().upper()

            if not choice:
                choice = "Unknown"

            result = {
                "prompt": prompt,
                "choice": choice,
                "reason": explanation,
                "response_raw": reply
            }

            if "[ERROR]" in reply:
                error_count += 1

        # ğŸ’¾ å†™å…¥ç»“æœ
        f_out.write(json.dumps(result, ensure_ascii=False) + "\n")
        f_out.flush()
        results.append(result)

        # ğŸ”„ æ‰“å°è¿›åº¦
        i = len(results)
        if i % 10 == 0:
            print(f"âœ… å·²è¯„ä¼° {i}/{len(data)} æ¡")

        if i % 50 == 0:
            elapsed = time.time() - start_time
            speed = elapsed / i
            eta = (len(data) - i) * speed
            print(f"â±ï¸ é¢„è®¡å‰©ä½™æ—¶é—´ï¼š{eta/60:.1f} åˆ†é’Ÿï¼Œå¹³å‡æ¯æ¡ {speed:.2f}s")

# âœ… ç»“æŸä¿¡æ¯
duration = time.time() - start_time
print(f"\nğŸ è¯„ä¼°å®Œæˆï¼Œæ€»è€—æ—¶ {duration/60:.1f} åˆ†é’Ÿï¼Œç»“æœä¿å­˜è‡³ {output_path}")
print(f"ğŸ“‰ é”™è¯¯æ¡æ•°ï¼š{error_count}")

import json
from collections import Counter

output_path = "gpt4o_comparisons.jsonl"

# è¯»å–è¯„ä¼°ç»“æœ
results = []
with open(output_path, 'r', encoding='utf-8') as f:
    for line in f:
        results.append(json.loads(line))

# ç»Ÿè®¡ç»“æœåˆ†å¸ƒ
choices = [r.get("choice", "Unknown") for r in results]
counter = Counter(choices)

total = len(results)
a_win = counter["A"]
b_win = counter["B"]
tie = counter["Tie"]
unknown = counter["Unknown"]

# æ‰“å°ç»Ÿè®¡ç»“æœ
print(f"ğŸ” æ€»å…±è¯„ä¼°æ ·æœ¬æ•°ï¼š{total}")
print(f"ğŸ¥‡ A èƒœå‡ºï¼š{a_win} ({a_win/total:.1%})")
print(f"ğŸ¥ˆ B èƒœå‡ºï¼š{b_win} ({b_win/total:.1%})")
print(f"ğŸ¤ Tie å¹³å±€ï¼š{tie} ({tie/total:.1%})")
print(f"â“ Unknownï¼ˆæ ¼å¼é”™è¯¯ç­‰ï¼‰ï¼š{unknown} ({unknown/total:.1%})")

import pandas as pd

df = pd.DataFrame(results)
df.to_csv("gpt4o_eval_result.csv", index=False)
print("âœ… å·²å¯¼å‡ºä¸º gpt4o_eval_result.csv")

#äº¤æ¢ä¸‹ä½ç½®

import pandas as pd
import json
import os

# è¾“å…¥è·¯å¾„
input_csv = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/generate_perturbe/best_response/final_best_responses_with_baseline_and_perturbed_v11_15.csv"
output_jsonl = input_csv.replace(".csv", "_gpt4o_eval_swap.jsonl")

# è¯»å–æ•°æ®
df = pd.read_csv(input_csv)

# æ„é€  pairwise æ ¼å¼
pairs = []
tie_count = 0

for _, row in df.iterrows():
    prompt = row['prompt']
    baseline_resp = row['baseline_best_response']
    rps_resp = row['perturbed_best_response']
    baseline_id = row['baseline_best_response_id']
    rps_id = row['perturbed_best_response_id']

    # å¦‚æœ response id ç›¸åŒï¼Œç›´æ¥æ ‡è®°ä¸º Tie
    if baseline_id == rps_id:
        pairs.append({
            "prompt": prompt,
            "response_a": baseline_resp,
            "response_b": rps_resp,
            "auto_result": "Tie"
        })
        tie_count += 1
        continue

    # å¦åˆ™ç”Ÿæˆ GPT æ¯”è¾ƒ prompt
    formatted_prompt = f"""For the following query to a chatbot, which response is more helpful?

Query: {prompt}

Response A: {rps_resp}

Response B: {baseline_resp}


FIRST provide a one-sentence comparison of the two responses and explain which you feel is more helpful.
SECOND, on a new line, state only 'A' or 'B' to indicate which response is more helpful.

Your response should use the format:

Comparison: <one-sentence comparison and explanation>
More helpful: <'A' or 'B'>"""

    pairs.append({
        "prompt": formatted_prompt,
        "auto_result": None
    })

# ä¿å­˜ä¸º JSONL æ ¼å¼
with open(output_jsonl, 'w', encoding='utf-8') as f:
    for item in pairs:
        f.write(json.dumps(item, ensure_ascii=False) + "\n")

print(f"âœ… å·²ç”Ÿæˆ GPT è¯„ä¼°æ ¼å¼æ–‡ä»¶: {output_jsonl}")
print(f"ğŸ¤ å·²è·³è¿‡ {tie_count} æ¡å®Œå…¨ç›¸åŒ responseï¼Œæ ‡è®°ä¸º Tie")

import json
import time
import os

jsonl_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/generate_perturbe/best_response/final_best_responses_with_baseline_and_perturbed_v11_15_gpt4o_eval_swap.jsonl"
output_path = "gpt4o_comparisons_swap.jsonl"

# âœ… åŠ è½½è¾“å…¥æ•°æ®
with open(jsonl_path, 'r', encoding='utf-8') as f:
    data = [json.loads(line) for line in f]

print(f"âœ… è¯»å– {len(data)} æ¡ prompt")

# âœ… åŠ è½½å·²æœ‰ç»“æœï¼ˆæ”¯æŒæ–­ç‚¹ç»­è·‘ï¼‰
seen_prompts = set()
results = []

if os.path.exists(output_path):
    with open(output_path, 'r', encoding='utf-8') as f:
        for line in f:
            item = json.loads(line)
            results.append(item)
            seen_prompts.add(item['prompt'])
    print(f"ğŸ” å·²åŠ è½½ {len(seen_prompts)} æ¡å·²å®Œæˆç»“æœï¼Œç»§ç»­è¯„ä¼°æœªå®Œæˆéƒ¨åˆ†")

# âœ… GPT è¯„ä¼°å‡½æ•°
def gpt4o_judge(prompt):
    try:
        res = openai.ChatCompletion.create(
            model="gpt-4o-mini",  # ç¡®ä¿ä½ ç”¨çš„æ˜¯æ­£ç¡®æ¨¡å‹ ID
            messages=[{"role": "user", "content": prompt}],
            temperature=0.0,
            max_tokens=200
        )
        return res['choices'][0]['message']['content']
    except Exception as e:
        print("âŒ GPT è°ƒç”¨å¤±è´¥ï¼š", e)  # â† åŠ è¿™è¡Œï¼
        return f"[ERROR] {e}"


# âœ… æ­£å¼å¼€å§‹è¯„ä¼°
error_count = 0
start_time = time.time()

with open(output_path, "a", encoding="utf-8") as f_out:
    for idx, item in enumerate(data):
        prompt = item["prompt"]

        # â© è·³è¿‡å·²å¤„ç†çš„æ ·æœ¬
        if prompt in seen_prompts:
            continue

        # âœ… åˆ¤æ–­æ˜¯å¦ä¸ºè‡ªåŠ¨ Tie æ ·æœ¬
        if item.get("auto_result") == "Tie":
            result = {
                "prompt": prompt,
                "choice": "Tie",
                "reason": "Responses are identical, skipped GPT evaluation.",
                "response_raw": "More helpful: Tie"
            }
        else:
            reply = gpt4o_judge(prompt)
            lines = reply.strip().splitlines()
            explanation = ""
            choice = ""

            for line in lines:
                if line.lower().startswith("comparison:"):
                    explanation = line[len("comparison:"):].strip()
                elif line.lower().startswith("more helpful:"):
                    choice = line[len("more helpful:"):].strip().upper()

            if not choice:
                choice = "Unknown"

            result = {
                "prompt": prompt,
                "choice": choice,
                "reason": explanation,
                "response_raw": reply
            }

            if "[ERROR]" in reply:
                error_count += 1

        # ğŸ’¾ å†™å…¥ç»“æœ
        f_out.write(json.dumps(result, ensure_ascii=False) + "\n")
        f_out.flush()
        results.append(result)

        # ğŸ”„ æ‰“å°è¿›åº¦
        i = len(results)
        if i % 10 == 0:
            print(f"âœ… å·²è¯„ä¼° {i}/{len(data)} æ¡")

        if i % 50 == 0:
            elapsed = time.time() - start_time
            speed = elapsed / i
            eta = (len(data) - i) * speed
            print(f"â±ï¸ é¢„è®¡å‰©ä½™æ—¶é—´ï¼š{eta/60:.1f} åˆ†é’Ÿï¼Œå¹³å‡æ¯æ¡ {speed:.2f}s")

# âœ… ç»“æŸä¿¡æ¯
duration = time.time() - start_time
print(f"\nğŸ è¯„ä¼°å®Œæˆï¼Œæ€»è€—æ—¶ {duration/60:.1f} åˆ†é’Ÿï¼Œç»“æœä¿å­˜è‡³ {output_path}")
print(f"ğŸ“‰ é”™è¯¯æ¡æ•°ï¼š{error_count}")

import json
from collections import Counter

output_path = "gpt4o_comparisons_swap.jsonl"

# è¯»å–è¯„ä¼°ç»“æœ
results = []
with open(output_path, 'r', encoding='utf-8') as f:
    for line in f:
        results.append(json.loads(line))

# ç»Ÿè®¡ç»“æœåˆ†å¸ƒ
choices = [r.get("choice", "Unknown") for r in results]
counter = Counter(choices)

total = len(results)
a_win = counter["A"]
b_win = counter["B"]
tie = counter["Tie"]
unknown = counter["Unknown"]

# æ‰“å°ç»Ÿè®¡ç»“æœ
print(f"ğŸ” æ€»å…±è¯„ä¼°æ ·æœ¬æ•°ï¼š{total}")
print(f"ğŸ¥‡ A èƒœå‡ºï¼š{a_win} ({a_win/total:.1%})")
print(f"ğŸ¥ˆ B èƒœå‡ºï¼š{b_win} ({b_win/total:.1%})")
print(f"ğŸ¤ Tie å¹³å±€ï¼š{tie} ({tie/total:.1%})")
print(f"â“ Unknownï¼ˆæ ¼å¼é”™è¯¯ç­‰ï¼‰ï¼š{unknown} ({unknown/total:.1%})")

#æ¶ˆé™¤position biasç‰ˆæœ¬+åˆç†prompt

import openai
import os

# è®¾ç½® API Key å’Œä»£ç†åœ°å€ï¼ˆå¦‚æœä½¿ç”¨ä»£ç†ï¼‰
os.environ["OPENAI_API_KEY"] = "sk-XGGe5y0ZvLcQVFp6XnRizs7q47gsVnAbZx0Xr2mfcVlbr99f"
openai.api_key = os.environ["OPENAI_API_KEY"]
openai.api_base = "https://api2.aigcbest.top/v1"  # å¦‚æœç”¨ä»£ç†å°±æ”¹è¿™é‡Œ

print("âœ… OpenAI è®¾ç½®å®Œæˆ")

import pandas as pd
import json
import os

# è¾“å…¥ CSV è·¯å¾„
input_csv = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/generate_perturbe/best_response/final_best_responses_with_baseline_and_perturbed_v11_30.csv"
output_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/generate_perturbe/best_response/gpt4o_compare/pairwise_prompts_30.jsonl"

df = pd.read_csv(input_csv)
pairs = []
tie_count = 0

for idx, row in df.iterrows():
    prompt = row["prompt"]
    baseline = row["baseline_best_response"]
    rps = row["perturbed_best_response"]
    v1 = row["v1"]
    v2 = row["v2"]
    baseline_id = row["best_baseline_response_id"]
    rps_id = row["best_perturbed_response_id"]

    # è‡ªåŠ¨ Tieï¼ˆID ç›¸åŒï¼‰
    if baseline_id == rps_id:
        for order, base_text, rps_text in [("original", baseline, rps), ("flipped", rps, baseline)]:
            pairs.append({
                "pair_id": idx,
                "order": order,
                "prompt": prompt,
                "baseline_response": base_text,
                "rps_response": rps_text,
                "auto_result": "Tie",
                "v1": v1,
                "v2": v2
            })
        tie_count += 1
        continue

    # åŒå‘æ„é€  prompt
    for order, base_text, rps_text in [("original", baseline, rps), ("flipped", rps, baseline)]:
        full_prompt = f"""You are an assistant evaluating two chatbot responses based on user preferences.

The user prefers responses that score higher under this formula:
score = helpfulness Ã— {v1:.2f} + verbosity Ã— {v2:.2f}

Note: The order of responses is randomized. Do not assume A is better than B.

Query: {prompt}

Response A: {base_text}

Response B: {rps_text}

FIRST, write one sentence comparing both responses.
SECOND, on a new line, state only 'A' or 'B'.

Format:
Comparison: ...
More aligned: A/B"""

        pairs.append({
            "pair_id": idx,
            "order": order,
            "prompt": full_prompt,
            "auto_result": None,
            "v1": v1,
            "v2": v2
        })

# ä¿å­˜ JSONL
with open(output_path, "w", encoding="utf-8") as f:
    for item in pairs:
        f.write(json.dumps(item, ensure_ascii=False) + "\n")

print(f"âœ… æ„é€ å®Œæˆï¼š{len(pairs)} æ¡ï¼ˆåŒå‘ + Tieï¼‰")
print(f"ğŸ¤ è‡ªåŠ¨ Tie å¯¹æ•°ï¼š{tie_count}")
print(f"ğŸ“ æ–‡ä»¶å·²ä¿å­˜è‡³ï¼š{output_path}")

!pip install openai==0.28 --upgrade

!rm "drive/MyDrive/llm_pre_project/dpa_outputs_v9/generate_perturbe/best_response/gpt4o_compare/results/gpt4o_scored_30.jsonl"

import openai
import json
import time
import os

# === å¯é…ç½®éƒ¨åˆ† ===
input_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/generate_perturbe/best_response/gpt4o_compare/pairwise_prompts_30.jsonl"
output_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/generate_perturbe/best_response/gpt4o_compare/results/gpt4o_scored_30.jsonl"
FORCE_RERUN = False  # â¬…ï¸ æ˜¯å¦å¼ºåˆ¶é‡æ–°æ‰“åˆ†ï¼ˆTrue åˆ™å¿½ç•¥å†å²ï¼‰

# === åˆ›å»ºè¾“å‡ºç›®å½• ===
os.makedirs(os.path.dirname(output_path), exist_ok=True)

# === åŠ è½½è¾“å…¥æ•°æ® ===
with open(input_path, "r", encoding="utf-8") as f:
    data = [json.loads(line) for line in f]

# === åŠ è½½å·²å¤„ç†æ ·æœ¬ IDï¼ˆpair_id, orderï¼‰ ===
seen_ids = set()
if os.path.exists(output_path) and not FORCE_RERUN:
    with open(output_path, "r", encoding="utf-8") as f:
        for line in f:
            obj = json.loads(line)
            seen_ids.add((obj["pair_id"], obj["order"]))
    print(f"ğŸ” å·²åŠ è½½ {len(seen_ids)} æ¡å·²è¯„ä¼°æ ·æœ¬")

# === GPT æ‰“åˆ†å‡½æ•° ===
def gpt_score(prompt):
    try:
        res = openai.ChatCompletion.create(
            model="gpt-4o",  # â† æ”¹ä¸ºä½ è‡ªå·±çš„æ¨¡å‹åæˆ–ä»£ç†æ¨¡å‹å
            messages=[{"role": "user", "content": prompt}],
            temperature=0,
            max_tokens=200
        )
        return res['choices'][0]['message']['content']
    except Exception as e:
        return f"[ERROR] {e}"

# === å¼€å§‹æ‰“åˆ† ===
start_time = time.time()
error_count, processed = 0, 0
total_to_process = len(data) if FORCE_RERUN else len([d for d in data if (d["pair_id"], d["order"]) not in seen_ids])

print(f"ğŸ“¦ æ€»æ ·æœ¬æ•°ï¼š{len(data)}ï¼Œå¾…è¯„ä¼°ï¼š{total_to_process}")

with open(output_path, "a", encoding="utf-8") as f_out:
    for item in data:
        pair_id = item["pair_id"]
        order = item["order"]
        prompt = item["prompt"]
        auto = item.get("auto_result")

        # è·³è¿‡å·²å¤„ç†
        if (pair_id, order) in seen_ids:
            continue

        # è‡ªåŠ¨ Tie çš„ç›´æ¥å†™å…¥
        if auto == "Tie":
            result = {
                "prompt": prompt,
                "choice": "Tie",
                "reason": "Identical response IDs, skipped GPT.",
                "pair_id": pair_id,
                "order": order
            }
        else:
            reply = gpt_score(prompt)
            lines = reply.strip().splitlines()
            explanation, choice = "", ""

            for line in lines:
                if line.lower().startswith("comparison:"):
                    explanation = line[len("comparison:"):].strip()
                elif line.lower().startswith("more aligned:"):
                    choice = line[len("more aligned:"):].strip().upper()

            if choice not in ["A", "B"]:
                choice = "Unknown"

            result = {
                "prompt": prompt,
                "response_raw": reply,
                "choice": choice,
                "reason": explanation,
                "pair_id": pair_id,
                "order": order
            }

            if "[ERROR]" in reply:
                error_count += 1

        # å†™å…¥ç»“æœ
        f_out.write(json.dumps(result, ensure_ascii=False) + "\n")
        f_out.flush()
        processed += 1

        # æ‰“å°è¿›åº¦
        if processed % 10 == 0 or processed == total_to_process:
            print(f"âœ… å·²è¯„ä¼° {processed}/{total_to_process}")

        if processed % 50 == 0:
            elapsed = time.time() - start_time
            avg = elapsed / processed
            eta = (total_to_process - processed) * avg
            print(f"â±ï¸ å¹³å‡ {avg:.2f}s/æ¡ï¼Œé¢„è®¡å‰©ä½™ {eta/60:.1f} åˆ†é’Ÿ")

# === å®Œæˆä¿¡æ¯ ===
print(f"\nğŸ æ‰“åˆ†å®Œæˆï¼Œæ€»å¤„ç† {processed} æ¡")
print(f"ğŸ“‰ é”™è¯¯æ•°ï¼š{error_count}")
print(f"ğŸ“ ä¿å­˜äºï¼š{output_path}")

from collections import defaultdict
import json

input_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/generate_perturbe/best_response/gpt4o_compare/results/gpt4o_scored_30.jsonl"

results = defaultdict(dict)
with open(input_path, 'r', encoding='utf-8') as f:
    for line in f:
        item = json.loads(line)
        pid = str(item.get("pair_id"))
        order = item.get("order")
        choice = item.get("choice", "Unknown")
        results[pid][order] = choice

baseline_win = 0
rps_win = 0
tie = 0
unknown = 0

for pid, pair in results.items():
    orig = pair.get("original")
    flip = pair.get("flipped")

    if orig == "A" and flip == "B":
        baseline_win += 1
    elif orig == "B" and flip == "A":
        rps_win += 1
    elif orig == "Tie" and flip == "Tie":
        tie += 1
    else:
        unknown += 1

total = baseline_win + rps_win + tie + unknown

print("\nğŸ“Š åŒå‘å½’å¹¶ç»Ÿè®¡")
print(f"æ€»æ ·æœ¬å¯¹ï¼š{total}")
print(f"ğŸ¥‡ Baseline æ›´å¥½ï¼š{baseline_win} ({baseline_win/total:.1%})")
print(f"ğŸ¥ˆ RPS æ›´å¥½ï¼š{rps_win} ({rps_win/total:.1%})")
print(f"ğŸ¤ å®Œå…¨ Tieï¼š{tie} ({tie/total:.1%})")
print(f"â“ Unknown / ä¸ä¸€è‡´ï¼š{unknown} ({unknown/total:.1%})")

import json
from collections import defaultdict

input_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/generate_perturbe/best_response/gpt4o_compare/results/gpt4o_scored.jsonl"

# åŠ è½½æ‰€æœ‰æ‰“åˆ†ç»“æœ
results = defaultdict(dict)
with open(input_path, "r", encoding="utf-8") as f:
    for line in f:
        item = json.loads(line)
        pair_id = str(item.get("pair_id"))
        order = item.get("order")
        choice = item.get("choice", "Unknown")
        results[pair_id][order] = choice

# ç»Ÿè®¡å„ç§å¼‚å¸¸æƒ…å†µ
missing_sides = 0
format_errors = 0
conflicts = 0
conflict_examples = []

for pid, pair in results.items():
    orig = pair.get("original")
    flip = pair.get("flipped")

    if orig is None or flip is None:
        missing_sides += 1
    elif orig == "A" and flip == "A":
        conflicts += 1
        conflict_examples.append((pid, "original: A", "flipped: A"))
    elif orig == "B" and flip == "B":
        conflicts += 1
        conflict_examples.append((pid, "original: B", "flipped: B"))
    elif "Unknown" in [orig, flip]:
        format_errors += 1

# è¾“å‡ºæ€»ç»“
total_unknown = missing_sides + format_errors + conflicts
print("\nğŸ“Š Unknown æ ·æœ¬åˆ†ææŠ¥å‘Šï¼šå…±", total_unknown, "æ¡\n")
print(f"â— ç¼ºå¤± original/flippedï¼š{missing_sides}")
print(f"â— GPT æ ¼å¼é”™è¯¯ï¼ˆè¾“å‡ºç¼ºå°‘ More alignedï¼‰ï¼š{format_errors}")
print(f"â— å†²çªé€‰æ‹©ï¼ˆæ¨¡å‹ä¸¤æ¬¡éƒ½é€‰ A æˆ– Bï¼‰ï¼š{conflicts}")

# æ˜¾ç¤ºå‰ 10 ä¸ªå†²çªæ ·æœ¬
print("\nğŸ§¾ ç¤ºä¾‹ï¼ˆå‰ 10 æ¡ï¼‰ï¼š")
for ex in conflict_examples[:10]:
    print(f"â€¢ pair_id: {ex[0]} â†’ å†²çªé€‰æ‹© â†’ {ex[1]}, {ex[2]}")

#å†æ”¹prompt

import pandas as pd
import json
import os

# è¾“å…¥ CSV è·¯å¾„
input_csv = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/generate_perturbe/best_response/final_best_responses_with_baseline_and_perturbed_v11_30.csv"
output_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/generate_perturbe/best_response/gpt4o_compare/pairwise_prompts_30.jsonl"

df = pd.read_csv(input_csv)
pairs = []
tie_count = 0

for idx, row in df.iterrows():
    prompt = row["prompt"]
    baseline = row["baseline_best_response"]
    rps = row["perturbed_best_response"]
    v1 = row["v1"]
    v2 = row["v2"]
    baseline_id = row["best_baseline_response_id"]
    rps_id = row["best_perturbed_response_id"]

    # è‡ªåŠ¨ Tieï¼ˆID ç›¸åŒï¼‰
    if baseline_id == rps_id:
        for order, base_text, rps_text in [("original", baseline, rps), ("flipped", rps, baseline)]:
            pairs.append({
                "pair_id": idx,
                "order": order,
                "prompt": prompt,
                "baseline_response": base_text,
                "rps_response": rps_text,
                "auto_result": "Tie",
                "v1": v1,
                "v2": v2
            })
        tie_count += 1
        continue

    # åŒå‘æ„é€  prompt
    for order, base_text, rps_text in [("original", baseline, rps), ("flipped", rps, baseline)]:
        full_prompt = f"""You are an assistant evaluating two chatbot responses.

The user generally prefers responses that score higher under this formula:
score = helpfulness Ã— {v1:.2f} + verbosity Ã— {v2:.2f}

However, in real-world settings, users often express their preferences imprecisely, and their expectations may vary slightly depending on context.

Please consider which response is more robust and likely to remain aligned with the user's intent, even if their preferences shift a little.

The order of responses is randomized. Do not assume A is better than B.

Query: {prompt}

Response A: {base_text}

Response B: {rps_text}

First, briefly compare the two responses.

Then, on a new line, write only 'A' or 'B' to indicate which is more robust and aligned overall.

Format:
Comparison: ...
More aligned: A/B"""

        pairs.append({
            "pair_id": idx,
            "order": order,
            "prompt": full_prompt,
            "auto_result": None,
            "v1": v1,
            "v2": v2
        })

# ä¿å­˜ JSONL
with open(output_path, "w", encoding="utf-8") as f:
    for item in pairs:
        f.write(json.dumps(item, ensure_ascii=False) + "\n")

print(f"âœ… æ„é€ å®Œæˆï¼š{len(pairs)} æ¡ï¼ˆåŒå‘ + Tieï¼‰")
print(f"ğŸ¤ è‡ªåŠ¨ Tie å¯¹æ•°ï¼š{tie_count}")
print(f"ğŸ“ æ–‡ä»¶å·²ä¿å­˜è‡³ï¼š{output_path}")

!rm "drive/MyDrive/llm_pre_project/dpa_outputs_v9/generate_perturbe/best_response/gpt4o_compare/results/gpt4o_scored_30.jsonl"

import openai
import json
import time
import os

# === å¯é…ç½®éƒ¨åˆ† ===
input_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/generate_perturbe/best_response/gpt4o_compare/pairwise_prompts_30.jsonl"
output_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/generate_perturbe/best_response/gpt4o_compare/results/gpt4o_scored_30.jsonl"
FORCE_RERUN = False  # â¬…ï¸ æ˜¯å¦å¼ºåˆ¶é‡æ–°æ‰“åˆ†ï¼ˆTrue åˆ™å¿½ç•¥å†å²ï¼‰

# === åˆ›å»ºè¾“å‡ºç›®å½• ===
os.makedirs(os.path.dirname(output_path), exist_ok=True)

# === åŠ è½½è¾“å…¥æ•°æ® ===
with open(input_path, "r", encoding="utf-8") as f:
    data = [json.loads(line) for line in f]

# === åŠ è½½å·²å¤„ç†æ ·æœ¬ IDï¼ˆpair_id, orderï¼‰ ===
seen_ids = set()
if os.path.exists(output_path) and not FORCE_RERUN:
    with open(output_path, "r", encoding="utf-8") as f:
        for line in f:
            obj = json.loads(line)
            seen_ids.add((obj["pair_id"], obj["order"]))
    print(f"ğŸ” å·²åŠ è½½ {len(seen_ids)} æ¡å·²è¯„ä¼°æ ·æœ¬")

# === GPT æ‰“åˆ†å‡½æ•° ===
def gpt_score(prompt):
    try:
        res = openai.ChatCompletion.create(
            model="gpt-4o",  # â† æ”¹ä¸ºä½ è‡ªå·±çš„æ¨¡å‹åæˆ–ä»£ç†æ¨¡å‹å
            messages=[{"role": "user", "content": prompt}],
            temperature=0,
            max_tokens=200
        )
        return res['choices'][0]['message']['content']
    except Exception as e:
        return f"[ERROR] {e}"

# === å¼€å§‹æ‰“åˆ† ===
start_time = time.time()
error_count, processed = 0, 0
total_to_process = len(data) if FORCE_RERUN else len([d for d in data if (d["pair_id"], d["order"]) not in seen_ids])

print(f"ğŸ“¦ æ€»æ ·æœ¬æ•°ï¼š{len(data)}ï¼Œå¾…è¯„ä¼°ï¼š{total_to_process}")

with open(output_path, "a", encoding="utf-8") as f_out:
    for item in data:
        pair_id = item["pair_id"]
        order = item["order"]
        prompt = item["prompt"]
        auto = item.get("auto_result")

        # è·³è¿‡å·²å¤„ç†
        if (pair_id, order) in seen_ids:
            continue

        # è‡ªåŠ¨ Tie çš„ç›´æ¥å†™å…¥
        if auto == "Tie":
            result = {
                "prompt": prompt,
                "choice": "Tie",
                "reason": "Identical response IDs, skipped GPT.",
                "pair_id": pair_id,
                "order": order
            }
        else:
            reply = gpt_score(prompt)
            lines = reply.strip().splitlines()
            explanation, choice = "", ""

            for line in lines:
                if line.lower().startswith("comparison:"):
                    explanation = line[len("comparison:"):].strip()
                elif line.lower().startswith("more aligned:"):
                    choice = line[len("more aligned:"):].strip().upper()

            if choice not in ["A", "B"]:
                choice = "Unknown"

            result = {
                "prompt": prompt,
                "response_raw": reply,
                "choice": choice,
                "reason": explanation,
                "pair_id": pair_id,
                "order": order
            }

            if "[ERROR]" in reply:
                error_count += 1

        # å†™å…¥ç»“æœ
        f_out.write(json.dumps(result, ensure_ascii=False) + "\n")
        f_out.flush()
        processed += 1

        # æ‰“å°è¿›åº¦
        if processed % 10 == 0 or processed == total_to_process:
            print(f"âœ… å·²è¯„ä¼° {processed}/{total_to_process}")

        if processed % 50 == 0:
            elapsed = time.time() - start_time
            avg = elapsed / processed
            eta = (total_to_process - processed) * avg
            print(f"â±ï¸ å¹³å‡ {avg:.2f}s/æ¡ï¼Œé¢„è®¡å‰©ä½™ {eta/60:.1f} åˆ†é’Ÿ")

# === å®Œæˆä¿¡æ¯ ===
print(f"\nğŸ æ‰“åˆ†å®Œæˆï¼Œæ€»å¤„ç† {processed} æ¡")
print(f"ğŸ“‰ é”™è¯¯æ•°ï¼š{error_count}")
print(f"ğŸ“ ä¿å­˜äºï¼š{output_path}")

from collections import defaultdict
import json

input_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/generate_perturbe/best_response/gpt4o_compare/results/gpt4o_scored_30.jsonl"

results = defaultdict(dict)
with open(input_path, 'r', encoding='utf-8') as f:
    for line in f:
        item = json.loads(line)
        pid = str(item.get("pair_id"))
        order = item.get("order")
        choice = item.get("choice", "Unknown")
        results[pid][order] = choice

baseline_win = 0
rps_win = 0
tie = 0
unknown = 0

for pid, pair in results.items():
    orig = pair.get("original")
    flip = pair.get("flipped")

    if orig == "A" and flip == "B":
        baseline_win += 1
    elif orig == "B" and flip == "A":
        rps_win += 1
    elif orig == "Tie" and flip == "Tie":
        tie += 1
    else:
        unknown += 1

total = baseline_win + rps_win + tie + unknown

print("\nğŸ“Š åŒå‘å½’å¹¶ç»Ÿè®¡")
print(f"æ€»æ ·æœ¬å¯¹ï¼š{total}")
print(f"ğŸ¥‡ Baseline æ›´å¥½ï¼š{baseline_win} ({baseline_win/total:.1%})")
print(f"ğŸ¥ˆ RPS æ›´å¥½ï¼š{rps_win} ({rps_win/total:.1%})")
print(f"ğŸ¤ å®Œå…¨ Tieï¼š{tie} ({tie/total:.1%})")
print(f"â“ Unknown / ä¸ä¸€è‡´ï¼š{unknown} ({unknown/total:.1%})")











#F-DIVæ–¹æ³•

import pandas as pd

df = pd.read_csv("/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/batch_v11.csv")
sample = df[df["prompt_id"] == 42]  # ä»»é€‰ä¸€è¡Œ
print(sample["prompt"].iloc[0])
print(sample["response"].tolist())  # ä¸‰ä¸ª response



#f-div

#V5:  v = (0.9397, 0.3420)  ä¸ºä¸»æ–¹å‘æµ‹è¯•

import numpy as np

v_main = np.array([0.9397, 0.3420])  # ä¸»æ–¹å‘å‘é‡ vâ‚… â‰ˆ (cos20Â°, sin20Â°)

angle_offsets = [-60,-45, -30, -15, 15, 30, 45,60]
perturbed_vs = []
perturbed_angles = []

for offset in angle_offsets:
    angle_deg = 20 + offset
    angle_rad = np.radians(angle_deg)
    v = np.array([np.cos(angle_rad), np.sin(angle_rad)])
    perturbed_vs.append(v)
    perturbed_angles.append(angle_deg)

print(perturbed_angles)

#KL divergence

def kl_divergence(p, q):
    p = np.abs(p) / np.sum(np.abs(p))  # å½’ä¸€åŒ–æˆä¼ªæ¦‚ç‡åˆ†å¸ƒ
    q = np.abs(q) / np.sum(np.abs(q))
    return np.sum(p * np.log(p / (q + 1e-12)))  # é˜²æ­¢é™¤é›¶

delta = 0.1  # KL æ•£åº¦å®¹å·®
kl_values = []

for v in perturbed_vs:
    kl = kl_divergence(v, v_main)
    kl_values.append(kl)

#ç­›é€‰æ–¹å‘ï¼ŒKL â‰¤ Î´
valid_vs = []
valid_angles = []

for v, angle, kl in zip(perturbed_vs, perturbed_angles, kl_values):
    if kl <= delta:
        valid_vs.append(v)
        valid_angles.append(angle)

print(valid_vs)
print(valid_angles)

import matplotlib.pyplot as plt

fig, ax = plt.subplots(figsize=(6, 6))
ax.add_artist(plt.Circle((0, 0), 1, color='lightgray', fill=False))
ax.quiver(0, 0, v_main[0], v_main[1], angles='xy', scale_units='xy', scale=1, color='blue', label='Main v (20Â°)')

for v, angle, kl in zip(perturbed_vs, perturbed_angles, kl_values):
    color = 'orange' if kl <= delta else 'gray'
    ax.quiver(0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1, color=color, alpha=0.8)
    ax.text(v[0]*1.1, v[1]*1.1, f"{angle}Â°", fontsize=8, ha='center', color=color)

ax.set_xlim(-1.2, 1.2)
ax.set_ylim(-1.2, 1.2)
ax.set_aspect('equal')
ax.set_title("f-div Ball (KL â‰¤ Î´) around v5 (20Â°)")
ax.legend()
plt.grid(True)
plt.show()

#ç”Ÿæˆå›ç­”
#v5çš„å‰500
#ç¬¬äºŒæ¬¡è·‘å°±æ˜¯å®Œæ•´çš„2000äº†

from google.colab import drive
drive.mount('/content/drive')

import os
import numpy as np
import pandas as pd
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM
from tqdm.auto import tqdm
import torch
import time
import random

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
seed = 42
np.random.seed(seed)
random.seed(seed)
torch.manual_seed(seed)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(seed)

# âœ… STEP 2: ä¿å­˜è·¯å¾„
result_dir = "/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v5"
os.makedirs(result_dir, exist_ok=True)


# âœ… STEP 3: åŠ è½½å‰ 2000 æ¡ UltraFeedback prompt
ds = load_dataset("HuggingFaceH4/ultrafeedback_binarized", split="test_prefs")
prompts = ds["prompt"][:2000]
prompt_ids = list(range(2000))

# âœ… STEP 4: åŠ è½½ DPA æ¨¡å‹å’Œ tokenizer
model = AutoModelForCausalLM.from_pretrained(
    "Haoxiang-Wang/DPA-v1-Mistral-7B",
    torch_dtype=torch.float16,
    device_map="auto"
).to(device)

tokenizer = AutoTokenizer.from_pretrained("Haoxiang-Wang/RewardModel-Mistral-7B-for-DPA-v1")
tokenizer.padding_side = "left"
if tokenizer.pad_token_id is None:
    tokenizer.pad_token = tokenizer.eos_token

# âœ… STEP 5: æ„é€  DPA Prompt
def build_input(prompt, v1, v2):
    h = int(np.round(v1 * 100))
    v = int(np.round(v2 * 100))
    sys_instruction = f"You are a helpful assistant. Your response should maximize weighted rating = helpfulness*{h} + verbosity*{v}."
    return [{"role": "user", "content": f"{sys_instruction}\n\n{prompt}"}]

# âœ… STEP 6: æ‰¹é‡ç”Ÿæˆ response
def generate_response_batch(prompts_batch, prompt_ids_batch, v1, v2):
    input_ids_list = []

    for prompt in prompts_batch:
        messages = build_input(prompt, v1, v2)
        input_ids = tokenizer.apply_chat_template(
            messages, add_generation_prompt=True, return_tensors="pt"
        )[0]
        input_ids_list.append(input_ids.to(device))

    input_ids_padded = torch.nn.utils.rnn.pad_sequence(
        input_ids_list, batch_first=True, padding_value=tokenizer.pad_token_id
    ).to(device)

    attention_mask = (input_ids_padded != tokenizer.pad_token_id).to(device)
    max_input_len = input_ids_padded.shape[1]
    max_new_tokens = min(2048, 4096 - max_input_len)

    with torch.no_grad():
        outputs = model.generate(
            input_ids=input_ids_padded,
            attention_mask=attention_mask,
            max_new_tokens=max_new_tokens,
            temperature=0.7,
            do_sample=True,
            num_return_sequences=1,
            pad_token_id=tokenizer.eos_token_id
        )

    responses = []
    for i, input_ids in enumerate(input_ids_list):
        generated_tokens = outputs[i][input_ids.shape[0]:]
        decoded = tokenizer.decode(generated_tokens, skip_special_tokens=True)
        responses.append({
            "prompt_id": prompt_ids_batch[i],
            "prompt": prompts_batch[i],
            "response": decoded
        })
    return responses

# âœ… STEP 7: ä½ è‡ªå·±çš„ valid_vs å’Œ valid_anglesï¼ˆè¯·æ›¿æ¢ä¸ºä½ çš„çœŸå®å€¼ï¼‰
valid_vs = [
    np.array([0.76604444, -0.64278761]),
    np.array([0.90630779, -0.42261826]),
    np.array([0.98480775, -0.17364818]),
    np.array([0.81915204, 0.57357644])
]
valid_angles = [-40, -25, -10, 35]  # ä¸ valid_vs é¡ºåºä¸€è‡´

# âœ… ä¸»æ–¹å‘ï¼ˆv5ï¼‰
main_v1 = 0.9397
main_v2 = 0.3420

# âœ… STEP 8: ä¸»ç”Ÿæˆå¾ªç¯
batch_size = 8

for i, (v_vec, angle_deg) in enumerate(zip(valid_vs, valid_angles)):
    v1, v2 = v_vec[0], v_vec[1]
    output_file = os.path.join(result_dir, f"fdiv_v5_dir{i}.csv")
    print(f"\nğŸš€ Generating for direction {i}: angle â‰ˆ {angle_deg}Â°, v = ({v1:.4f}, {v2:.4f})")

    if os.path.exists(output_file):
        existing_df = pd.read_csv(output_file)
        done_prompt_ids = set(existing_df["prompt_id"].unique())
        results = existing_df.to_dict("records")
        print(f"ğŸ” Resuming from previous run: {len(done_prompt_ids)} prompts already completed.")
    else:
        done_prompt_ids = set()
        results = []

    for start in tqdm(range(0, len(prompts), batch_size), desc=f"Generating fdiv_dir{i}"):
        end = min(start + batch_size, len(prompts))
        batch_prompts_all = prompts[start:end]
        batch_ids_all = prompt_ids[start:end]

        unprocessed_indices = [j for j, pid in enumerate(batch_ids_all) if pid not in done_prompt_ids]
        if not unprocessed_indices:
            continue

        batch_prompts = [batch_prompts_all[j] for j in unprocessed_indices]
        batch_ids = [batch_ids_all[j] for j in unprocessed_indices]

        try:
            batch_outputs = generate_response_batch(batch_prompts, batch_ids, v1, v2)
            for item in batch_outputs:
                item.update({
                    "v1_p": round(v1, 4),
                    "v2_p": round(v2, 4),
                    "direction_index": i,
                    "valid_angle": round(angle_deg, 1),
                    "main_v1": round(main_v1, 4),
                    "main_v2": round(main_v2, 4)
                })
                results.append(item)

            pd.DataFrame(batch_outputs).to_csv(
                output_file, mode='a', index=False,
                header=not os.path.exists(output_file)
            )

        except Exception as e:
            print(f"âš ï¸ Error at batch {start}-{end}: {e}")

    print(f"âœ… Final saved {len(results)} responses to {output_file}")

#æ£€æŸ¥ç©ºå’Œä¿®å¤v5

import os
import pandas as pd
import numpy as np
import torch
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForCausalLM
from accelerate import Accelerator

# === æ¨¡å‹å‡†å¤‡ ===
device = "cuda" if torch.cuda.is_available() else "cpu"
accelerator = Accelerator()

model = AutoModelForCausalLM.from_pretrained(
    "Haoxiang-Wang/DPA-v1-Mistral-7B",
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
    device_map="auto" if torch.cuda.is_available() else None
)
model = accelerator.prepare(model)

tokenizer = AutoTokenizer.from_pretrained("Haoxiang-Wang/DPA-v1-Mistral-7B")
tokenizer.padding_side = "left"
if tokenizer.pad_token_id is None:
    tokenizer.pad_token = tokenizer.eos_token

# === åˆ¤æ–­æ˜¯å¦æ— æ•ˆ response ===
def is_invalid_response(resp):
    if pd.isna(resp):
        return True
    resp = str(resp).strip()
    if not resp:
        return True

    invalid_keywords = [
        "[A:]", "[/SOLUTION]", "[ASS]", "[A]:", "[/CONV]", "[/CODE]", "[/USER]",
        "[OUTPUT]", "[CC]", "[OUT]", "[/DATA]", "[/ASST]", "[/ME]", "[/REST]",
        "[RESP]", "<[user]>", "<|user|>", "[Response]", "[Assistant]", "[Answer]",
        "[End of Response]", "<human>", "<|endoftext|>", "<<", "[INST]", "[/INST]",
        "[]", "[USER]"
    ]
    tokens = resp.split()
    if all(token.strip() in invalid_keywords for token in tokens):
        return True
    tag_like_count = sum(1 for t in tokens if (t.startswith("[") and t.endswith("]")) or (t.startswith("<") and t.endswith(">")))
    if len(tokens) > 0 and tag_like_count / len(tokens) > 0.8:
        return True
    return False

# === æ„é€ è¾“å…¥ prompt ===
def build_input(prompt, v1, v2):
    h = int(np.round(v1 * 100))
    v = int(np.round(v2 * 100))
    sys_instruction = f"You are a helpful assistant. Your response should maximize weighted rating = helpfulness*{h} + verbosity*{v}."
    return [{"role": "system", "content": sys_instruction}, {"role": "user", "content": prompt}]

# === ç”¨æ¨¡å‹é‡æ–°ç”Ÿæˆ response ===
def regenerate_response(prompt, v1, v2):
    try:
        messages = build_input(prompt, v1, v2)
        input_ids = tokenizer.apply_chat_template(
            messages, add_generation_prompt=True, return_tensors="pt"
        ).to(device)

        if input_ids.ndim != 2 or input_ids.shape[1] == 0:
            print(f"âš ï¸ Invalid input_ids shape {input_ids.shape}, skipping: {prompt[:60]}")
            return ""

        outputs = model.generate(
            input_ids=input_ids,
            temperature=0.7,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
        generated = outputs[0][input_ids.shape[1]:]
        return tokenizer.decode(generated, skip_special_tokens=True).strip()

    except Exception as e:
        print(f"âŒ Error: {e} \nPrompt: {prompt[:60]}")
        return ""

# === ä¿®å¤å•ä¸ª CSV æ–‡ä»¶ ===
def fix_csv_responses(csv_path, save_dir):
    print(f"\nğŸ“‚ æ­£åœ¨æ£€æŸ¥: {csv_path}")

    if not os.path.exists(csv_path):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {csv_path}")
        return

    df = pd.read_csv(csv_path)
    if "v1_p" not in df.columns or "v2_p" not in df.columns:
        print(f"âŒ ç¼ºå°‘ v1_p/v2_p åˆ—ï¼Œè·³è¿‡: {csv_path}")
        return

    bad_mask = df["response"].apply(is_invalid_response)
    num_bad = bad_mask.sum()

    if num_bad == 0:
        print("âœ… æ— éœ€ä¿®å¤ï¼Œè·³è¿‡ã€‚")
        return

    print(f"ğŸ”§ å‘ç° {num_bad} æ¡æ— æ•ˆ responseï¼Œå¼€å§‹é‡ç”Ÿæˆ...")
    for i in tqdm(bad_mask[bad_mask].index, total=num_bad, desc=os.path.basename(csv_path), dynamic_ncols=True):
        row = df.loc[i]
        new_response = regenerate_response(row["prompt"], row["v1_p"], row["v2_p"])
        df.at[i, "response"] = new_response

    os.makedirs(save_dir, exist_ok=True)
    file_name = os.path.basename(csv_path).replace(".csv", "_fixed.csv")
    save_path = os.path.join(save_dir, file_name)
    df.to_csv(save_path, index=False)
    print(f"âœ… ä¿®å¤å®Œæˆ â†’ å·²ä¿å­˜: {save_path}")

# === ä¸»é€»è¾‘ï¼šä¿®å¤æ‰€æœ‰ CSV ===
input_dir = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v5"
output_dir = os.path.join(input_dir, "fixed")

for file in os.listdir(input_dir):
    if file.endswith(".csv") and not file.endswith("_fixed.csv"):
        fix_csv_responses(os.path.join(input_dir, file), output_dir)

#v5 f div æ‰“åˆ†

# === å¯¼å…¥åº“ ===
import os
import time
import pandas as pd
import numpy as np
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
from tqdm.auto import tqdm
from google.colab import drive

# === æŒ‚è½½ Google Drive ===
drive.mount('/content/drive')

# === è®¾ç½®è·¯å¾„ ===
input_dir = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v5/fixed"
output_dir = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v5/scored"
os.makedirs(output_dir, exist_ok=True)

# === åŠ è½½ Reward Model ===
print("ğŸ¤– Loading Reward Model...")
device = "cuda" if torch.cuda.is_available() else "cpu"
rm = AutoModelForSequenceClassification.from_pretrained(
    "Haoxiang-Wang/RewardModel-Mistral-7B-for-DPA-v1", trust_remote_code=True
).to(device)
tokenizer = AutoTokenizer.from_pretrained("Haoxiang-Wang/RewardModel-Mistral-7B-for-DPA-v1")

# === æ„é€ è¾“å…¥æ¨¡æ¿å¹¶æ‰“åˆ† ===
def score_response(prompt, response):
    template = "[INST] You must read the following conversation carefully and rate the assistant's response from score 0-100 in these aspects: helpfulness, correctness, coherence, honesty, complexity, verbosity\n\nUser: {prompt}\n\nAssistant: {response} [/INST]"
    inputs = tokenizer(template.format(prompt=prompt, response=response), return_tensors="pt").to(device)
    with torch.no_grad():
        logits = rm(**inputs).logits.squeeze().cpu().numpy()
    return logits[9], logits[4]  # helpfulness, verbosity

# === æ‰“åˆ†æ•´ä¸ª CSV æ–‡ä»¶ ===
def score_file(file_path):
    print(f"\nğŸ“‚ Scoring file: {file_path}")
    df = pd.read_csv(file_path)
    scores = []

    for i, row in tqdm(df.iterrows(), total=len(df), desc=os.path.basename(file_path), dynamic_ncols=True):
        try:
            h, v = score_response(row["prompt"], row["response"])
        except Exception as e:
            print(f"âš ï¸ Error on row {i}: {e}")
            h, v = None, None
        scores.append({"helpfulness": h, "verbosity": v})

    df_scores = pd.DataFrame(scores)
    df_all = pd.concat([df, df_scores], axis=1)

    file_name = os.path.basename(file_path).replace(".csv", "_scored.csv")
    save_path = os.path.join(output_dir, file_name)
    df_all.to_csv(save_path, index=False)
    print(f"âœ… Scoring done â†’ Saved to: {save_path}")
    return df_all

# === æ‰¹é‡æ‰“åˆ†ç›®å½•ä¸­çš„æ‰€æœ‰ _fixed.csv æ–‡ä»¶ ===
for file in os.listdir(input_dir):
    if file.endswith("_fixed.csv"):
        score_file(os.path.join(input_dir, file))

#é€‰å‡ºf-div v5 best resposne

from google.colab import drive
drive.mount('/content/drive')

import os
import pandas as pd

# === è®¾ç½®è·¯å¾„ï¼ˆæ”¹ä¸º Colab ä¸‹çš„ç»å¯¹è·¯å¾„ï¼‰===
input_dir = "/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v5/scored"
output_file = os.path.join(input_dir, "fdiv_v5_all_best_response.csv")

# === ä¸»æ–¹å‘è®¾ç½®ï¼ˆv5ï¼‰
main_v1 = 0.9397
main_v2 = 0.3420

# === åˆå¹¶æ‰€æœ‰ scored æ–‡ä»¶
dfs = []
for file in os.listdir(input_dir):
    if file.endswith("_scored.csv"):
        df = pd.read_csv(os.path.join(input_dir, file))
        df["main_v1"] = main_v1
        df["main_v2"] = main_v2
        df["score_total"] = df["main_v1"] * df["helpfulness"] + df["main_v2"] * df["verbosity"]
        dfs.append(df)

df_all = pd.concat(dfs, ignore_index=True)
df_best = df_all.loc[df_all.groupby("prompt_id")["score_total"].idxmax()]
df_best = df_best.rename(columns={"response": "f_best_response"})

# === ä¿ç•™å­—æ®µ
cols_to_keep = [
    "prompt_id", "prompt", "f_best_response",
    "v1_p", "v2_p", "valid_angle",
    "main_v1", "main_v2",
    "helpfulness", "verbosity", "score_total"
]
df_best_clean = df_best[cols_to_keep]

# === ä¿å­˜ç»“æœ
df_best_clean.to_csv(output_file, index=False)
print(f"âœ… å·²ä¿å­˜åˆ°: {output_file}")



#ä¿®å¤v5 baseline

import os
import pandas as pd
import numpy as np
import torch
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForCausalLM
from accelerate import Accelerator

# === æ¨¡å‹é…ç½® ===
device = "cuda" if torch.cuda.is_available() else "cpu"
accelerator = Accelerator()

model = AutoModelForCausalLM.from_pretrained(
    "Haoxiang-Wang/DPA-v1-Mistral-7B",
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
    device_map="auto" if torch.cuda.is_available() else None
)

# ä½¿ç”¨ accelerator æ¥å¤„ç†è®¾å¤‡æ˜ å°„
model = accelerator.prepare(model)

tokenizer = AutoTokenizer.from_pretrained("Haoxiang-Wang/DPA-v1-Mistral-7B")
tokenizer.padding_side = "left"
if tokenizer.pad_token_id is None:
    tokenizer.pad_token = tokenizer.eos_token

# === åˆ¤æ–­ response æ˜¯å¦éœ€è¦é‡ç”Ÿæˆ ===
def is_invalid_response(resp):
    if pd.isna(resp):
        return True
    resp = str(resp).strip()
    if not resp:
        return True

    invalid_keywords = [
        "[A:]", "[/SOLUTION]", "[ASS]", "[A]:", "[/CONV]", "[/CODE]", "[/USER]",
        "[OUTPUT]", "[CC]", "[OUT]", "[/DATA]", "[/ASST]", "[/ME]", "[/REST]",
        "[RESP]", "<[user]>", "<|user|>", "[Response]", "[Assistant]", "[Answer]",
        "[End of Response]", "<human>", "<|endoftext|>", "<<", "[INST]", "[/INST]",
        "[]", "[USER]"
    ]

    if resp in invalid_keywords:
        return True

    tokens = resp.split()
    if all(token.strip() in invalid_keywords for token in tokens):
        return True

    tag_like_count = sum(1 for t in tokens if (t.startswith("[") and t.endswith("]")) or (t.startswith("<") and t.endswith(">")))
    if len(tokens) > 0 and tag_like_count / len(tokens) > 0.8:
        return True

    return False

# === æ„é€  Prompt ===
def build_input(prompt, v1, v2):
    h = int(np.round(v1 * 100))
    v = int(np.round(v2 * 100))
    sys_instruction = f"You are a helpful assistant. Your response should maximize weighted rating = helpfulness*{h} + verbosity*{v}."
    return [{"role": "system", "content": sys_instruction}, {"role": "user", "content": prompt}]

# === é‡ç”Ÿæˆå“åº” ===
def regenerate_response(prompt, v1, v2):
    try:
        messages = build_input(prompt, v1, v2)
        input_ids = tokenizer.apply_chat_template(
            messages, add_generation_prompt=True, return_tensors="pt"
        ).to(device)

        if input_ids.ndim != 2 or input_ids.shape[1] == 0:
            print(f"âš ï¸ Invalid input_ids shape {input_ids.shape}, skipping: {prompt[:60]}")
            return ""

        outputs = model.generate(
            input_ids=input_ids,
            temperature=0.7,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
        generated = outputs[0][input_ids.shape[1]:]
        return tokenizer.decode(generated, skip_special_tokens=True).strip()

    except Exception as e:
        print(f"âŒ Error: {e} \nPrompt: {prompt[:60]}")
        return ""

# === ä¿®å¤å•ä¸ªæ–‡ä»¶ ===
def fix_csv_responses(csv_path, save_dir):
    print(f"\nğŸ“‚ æ£€æŸ¥ï¼š{csv_path}")

    if not os.path.exists(csv_path):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {csv_path}")
        return

    df = pd.read_csv(csv_path)
    bad_mask = df["response"].apply(is_invalid_response)
    num_bad = bad_mask.sum()

    if num_bad == 0:
        print("âœ… æ— éœ€ä¿®å¤ï¼Œè·³è¿‡ã€‚")
        return

    print(f"ğŸ”§ æ£€æµ‹åˆ°æ— æ•ˆ response æ•°é‡ï¼š{num_bad}")
    for i in tqdm(bad_mask[bad_mask].index, total=num_bad, desc=os.path.basename(csv_path), dynamic_ncols=True):
        row = df.loc[i]
        new_response = regenerate_response(row["prompt"], row["v1"], row["v2"])
        df.at[i, "response"] = new_response

    os.makedirs(save_dir, exist_ok=True)
    file_name = os.path.basename(csv_path).replace(".csv", "_fixed.csv")
    save_path = os.path.join(save_dir, file_name)
    df.to_csv(save_path, index=False)
    print(f"âœ… ä¿®å¤å®Œæˆ â†’ ä¿å­˜åˆ°: {save_path}")

# === ä¸»é€»è¾‘ ===
input_dir = "drive/MyDrive/llm_pre_project/dpa_outputs_v9"
output_dir = os.path.join(input_dir, "fixed_batch")

# åªä¿®å¤ batch_v5.csv
input_file = "batch_v5.csv"

# ä¿®å¤è¯¥æ–‡ä»¶
fix_csv_responses(os.path.join(input_dir, input_file), output_dir)

#baslinev5æ‰“åˆ†ï¼ˆverbosityå’Œhelpfulness)

# === å¯¼å…¥åº“ ===
import os
import time
import pandas as pd
import numpy as np
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
from tqdm.auto import tqdm
from google.colab import drive

# === æŒ‚è½½ Google Drive ===
drive.mount('/content/drive')

# === è®¾ç½®è¾“å…¥è¾“å‡ºè·¯å¾„ ===
input_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/fixed_batch/batch_v5_fixed.csv"
output_dir = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch"
os.makedirs(output_dir, exist_ok=True)

# === åŠ è½½æ•°æ® ===
print(f"ğŸ“¦ Loading file: {input_path}")
df = pd.read_csv(input_path)

# === åŠ è½½ Reward Model ===
print("ğŸ¤– Loading Reward Model...")
device = "cuda" if torch.cuda.is_available() else "cpu"
rm = AutoModelForSequenceClassification.from_pretrained(
    "Haoxiang-Wang/RewardModel-Mistral-7B-for-DPA-v1", trust_remote_code=True
).to(device)
tokenizer = AutoTokenizer.from_pretrained("Haoxiang-Wang/RewardModel-Mistral-7B-for-DPA-v1")

def score_response(prompt, response):
    template = "[INST] You must read the following conversation carefully and rate the assistant's response from score 0-100 in these aspects: helpfulness, correctness, coherence, honesty, complexity, verbosity\n\nUser: {prompt}\n\nAssistant: {response} [/INST]"
    inputs = tokenizer(template.format(prompt=prompt, response=response), return_tensors="pt").to(device)
    with torch.no_grad():
        logits = rm(**inputs).logits.squeeze().cpu().numpy()
    return logits[9], logits[4]  # helpfulness, verbosity

# === å¼€å§‹æ‰“åˆ† ===
print("ğŸ§  Scoring responses...")
start_time = time.time()
scores = []

for i, row in tqdm(df.iterrows(), total=len(df), desc="ğŸ” Scoring"):
    try:
        h, v = score_response(row["prompt"], row["response"])
        scores.append({"v1": row["v1"], "v2": row["v2"], "helpfulness": h, "verbosity": v})
    except Exception as e:
        print(f"âš ï¸ Error on row {i}: {e}")
        scores.append({"v1": row["v1"], "v2": row["v2"], "helpfulness": None, "verbosity": None})

df_scores = pd.DataFrame(scores)
df_all = pd.concat([df, df_scores], axis=1)

# === ä¿å­˜ç»“æœ ===
output_file = os.path.join(output_dir, os.path.basename(input_path).replace(".csv", "_scored.csv"))
df_all.to_csv(output_file, index=False)

end_time = time.time()
print(f"\nâœ… Scoring complete! Total scored: {len(df_all)}")
print(f"ğŸ•’ Time elapsed: {end_time - start_time:.1f} seconds")
print(f"ğŸ“ Saved to: {output_file}")

import pandas as pd
import os

# === è®¾ç½®è¾“å…¥æ–‡ä»¶è·¯å¾„ ===
input_file = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch/batch_v5_fixed_scored.csv"

# === è®¾ç½®è¾“å‡ºè·¯å¾„ ===
output_dir = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch"
output_file_with_total = os.path.join(output_dir, "batch_v5_fixed_scored_with_total.csv")

# best response è¾“å‡ºç›®å½•å’Œæ–‡ä»¶å
best_response_dir = os.path.join(output_dir, "best_response")
os.makedirs(best_response_dir, exist_ok=True)
output_best_file = os.path.join(best_response_dir, "best_response_v5_baseline.csv")

# === åŠ è½½å·²æœ‰è¯„åˆ†æ–‡ä»¶ ===
df = pd.read_csv(input_file)

# === è®¡ç®—æ€»åˆ† ===
df["total_score"] = df["v1"] * df["helpfulness"] + df["v2"] * df["verbosity"]

# === ä¿å­˜å«æ€»åˆ†çš„æ–°æ–‡ä»¶ ===
df.to_csv(output_file_with_total, index=False)
print(f"âœ… Saved file with total_score: {output_file_with_total}")

# === é€‰å‡ºæ¯ä¸ª prompt_id ä¸‹ total_score æœ€é«˜çš„ response ===
df_best = df.loc[df.groupby("prompt_id")["total_score"].idxmax()].copy()

# å¯é€‰ï¼šå°† response åˆ—é‡å‘½åä¸º best_response
df_best = df_best.rename(columns={"response": "best_response"})

# === ä¿å­˜æ¯ä¸ª prompt_id çš„æœ€ä½³å›ç­”åˆ°æŒ‡å®šè·¯å¾„ ===
df_best.to_csv(output_best_file, index=False)
print(f"ğŸ† Best responses per prompt saved to: {output_best_file}")

#æ¯”è¾ƒå‰500 baselineå’Œfdiv

#step2:æŒ‰prompt_idå¯¹é½ä¸¤ç»„æ•°æ®

import pandas as pd

# === è·¯å¾„è®¾ç½® ===
baseline_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch/best_response/best_response_v5_baseline.csv"
fdiv_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v5/scored/fdiv_v5_all_best_response.csv"

# === åŠ è½½æ•°æ® ===
df_baseline = pd.read_csv(baseline_path)
df_fdiv = pd.read_csv(fdiv_path)

# === åˆå¹¶æ•°æ®ï¼ˆæŒ‰ prompt_idï¼‰
df_merged = pd.merge(
    df_baseline,
    df_fdiv,
    on="prompt_id",
    suffixes=("_baseline", "_fdiv")
)

# === å­—æ®µé‡å‘½å ===
df_merged = df_merged.rename(columns={
    "prompt_baseline": "prompt",
    "best_response": "baseline_best_response",
    "score_total": "f_score_total",                # æ¥è‡ª f-div
    "total_score": "baseline_score_total"          # æ¥è‡ª baseline
})

# === ä¿ç•™å­—æ®µï¼ŒåŒ…æ‹¬ baseline çš„ response_id
cols_to_show = [
    "prompt_id", "prompt",
    "baseline_best_response", "f_best_response",
    "baseline_score_total", "f_score_total",
    "response_id",                       # âœ… åŠ ä¸Š response_id
    "v1_p", "v2_p", "valid_angle",
    "main_v1", "main_v2"
]
df_clean = df_merged[cols_to_show]

# === ä¿å­˜ç»“æœ ===
output_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/v5_final_merged_for_judge.csv"
df_clean.to_csv(output_path, index=False)

print(f"âœ… åˆå¹¶å®Œæˆï¼Œæ–‡ä»¶ä¿å­˜è‡³ï¼š{output_path}")

#step3 æ„é€ åŒå‘

import pandas as pd
import json
import os

# === è·¯å¾„è®¾ç½® ===
input_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/v5_final_merged_for_judge.csv"
output_dir = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/gpt4_judge_inputs/"
os.makedirs(output_dir, exist_ok=True)

output_run1 = os.path.join(output_dir, "pairwise_run1_baselineA.jsonl")
output_run2 = os.path.join(output_dir, "pairwise_run2_fdivA.jsonl")

# === è¯»å–åˆå¹¶æ•°æ® ===
df = pd.read_csv(input_path)

# === æ„é€ ä¸¤ä¸ªç‰ˆæœ¬çš„ JSONL ===
run1, run2 = [], []
tie_count = 0

for idx, row in df.iterrows():
    prompt = row["prompt"]
    baseline = row["baseline_best_response"]
    fdiv = row["f_best_response"]
    v1 = row["main_v1"]
    v2 = row["main_v2"]
    pid = row["prompt_id"]
    baseline_id = row["response_id"]

    # æ ‡å‡†åŒ–æ„é€ å‡½æ•°
    def build_prompt(a_text, b_text):
        return f"""[HH-RLHF]: For the following query to a chatbot, which response is more helpful?

Query: {prompt}

Response A: {a_text}

Response B: {b_text}

FIRST provide a one-sentence comparison of the two responses and explain which you feel is more helpful.
SECOND, on a new line, state only 'A' or 'B' to indicate which response is more helpful.

Format:
Comparison: ...
More helpful: A/B"""

    # è‡ªåŠ¨ Tie æƒ…å†µ
    if baseline.strip() == fdiv.strip():
        for group, a_text, b_text, tag in [
            (run1, baseline, fdiv, "baseline_A"),
            (run2, fdiv, baseline, "fdiv_A")
        ]:
            group.append({
                "pair_id": idx,
                "prompt_id": pid,
                "order": tag,
                "auto_result": "Tie",
                "baseline_id": baseline_id,
                "v1": v1,
                "v2": v2,
                "formatted_prompt": build_prompt(a_text, b_text)
            })
        tie_count += 1
        continue

    # Run1: baseline A
    run1.append({
        "pair_id": idx,
        "prompt_id": pid,
        "order": "baseline_A",
        "auto_result": None,
        "baseline_id": baseline_id,
        "v1": v1,
        "v2": v2,
        "formatted_prompt": build_prompt(baseline, fdiv)
    })

    # Run2: f-div A
    run2.append({
        "pair_id": idx,
        "prompt_id": pid,
        "order": "fdiv_A",
        "auto_result": None,
        "baseline_id": baseline_id,
        "v1": v1,
        "v2": v2,
        "formatted_prompt": build_prompt(fdiv, baseline)
    })

# === ä¿å­˜ä¸º JSONLï¼ˆASCII å®‰å…¨ï¼‰===
with open(output_run1, "w", encoding="utf-8") as f:
    for item in run1:
        f.write(json.dumps(item, ensure_ascii=True) + "\n")

with open(output_run2, "w", encoding="utf-8") as f:
    for item in run2:
        f.write(json.dumps(item, ensure_ascii=True) + "\n")

print(f"âœ… æ„é€ å®Œæˆï¼šRun1 æ¡æ•° = {len(run1)}, Run2 æ¡æ•° = {len(run2)}, è‡ªåŠ¨ Tie = {tie_count}")
print(f"ğŸ“ ä¿å­˜è‡³ï¼š\n- {output_run1}\n- {output_run2}")

#gpt4oå¼€å§‹æ¯”è¾ƒ

import openai
import os

# è®¾ç½® API Key å’Œä»£ç†åœ°å€ï¼ˆå¦‚æœä½¿ç”¨ä»£ç†ï¼‰
os.environ["OPENAI_API_KEY"] = "sk-XGGe5y0ZvLcQVFp6XnRizs7q47gsVnAbZx0Xr2mfcVlbr99f"
openai.api_key = os.environ["OPENAI_API_KEY"]
openai.api_base = "https://api2.aigcbest.top/v1"  # å¦‚æœç”¨ä»£ç†å°±æ”¹è¿™é‡Œ

print("âœ… OpenAI è®¾ç½®å®Œæˆ")

import openai
import json
import time
import os
from tqdm import tqdm

# === é…ç½®å‚æ•° ===
input_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/gpt4_judge_inputs/pairwise_run1_baselineA.jsonl"
output_path = input_path.replace(".jsonl", "_results.jsonl")
model = "gpt-4o"
sleep_time = 1.0
max_retries = 3

# === åŠ è½½è¾“å…¥æ•°æ® ===
with open(input_path, "r", encoding="utf-8") as f:
    all_prompts = [json.loads(line) for line in f]

# === æ–­ç‚¹ç»­è·‘ï¼ˆè¯»å–å·²æœ‰ç»“æœï¼‰
completed_ids = set()
results = []
if os.path.exists(output_path):
    with open(output_path, "r", encoding="utf-8") as f:
        for line in f:
            item = json.loads(line)
            results.append(item)
            completed_ids.add(item["pair_id"])
    print(f"ğŸ” æ£€æµ‹åˆ°å·²æœ‰ç»“æœï¼Œå·²è·³è¿‡ {len(completed_ids)} æ¡")

# === å¼€å§‹è¯„ä¼° ===
start_time = time.time()

for item in tqdm(all_prompts, desc="ğŸ§  GPT-4o è¯„ä¼°ä¸­"):
    pid = item["pair_id"]
    if pid in completed_ids:
        continue

    if item.get("auto_result") == "Tie":
        item["gpt_judgment"] = "Tie"
        print(f"ğŸ¤ pair_id={pid} â†’ Auto-Tie")
        results.append(item)
        continue

    prompt = item["formatted_prompt"]
    for attempt in range(max_retries):
        try:
            response = openai.ChatCompletion.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.0,
            )
            reply = response.choices[0].message["content"].strip()
            item["gpt_raw_response"] = reply

            last_line = reply.strip().splitlines()[-1].strip().upper()
            if "MORE HELPFUL: A" in last_line or last_line == "A":
                item["gpt_judgment"] = "A"
            elif "MORE HELPFUL: B" in last_line or last_line == "B":
                item["gpt_judgment"] = "B"
            else:
                item["gpt_judgment"] = "Unclear"

            print(f"âœ… pair_id={pid} â†’ {item['gpt_judgment']}")
            break
        except Exception as e:
            item["error"] = str(e)
            print(f"âŒ pair_id={pid} â†’ Error: {str(e)}")
            time.sleep(sleep_time)
    else:
        item["gpt_judgment"] = "Error"
        print(f"âŒ pair_id={pid} â†’ Failed after {max_retries} attempts")

    results.append(item)
    time.sleep(sleep_time)

# === ä¿å­˜æ‰€æœ‰ç»“æœ ===
with open(output_path, "w", encoding="utf-8") as f:
    for r in results:
        f.write(json.dumps(r, ensure_ascii=False) + "\n")

end_time = time.time()
print(f"\nâœ… è¯„ä¼°å®Œæˆï¼Œå…±è®¡è¯„ä¼° {len(results)} æ¡")
print(f"ğŸ•’ æ€»è€—æ—¶ï¼š{end_time - start_time:.1f} ç§’")
print(f"ğŸ“ ä¿å­˜ç»“æœåˆ°ï¼š{output_path}")

#äº¤æ¢ä½ç½®

import openai
import json
import time
import os
from tqdm import tqdm

# === é…ç½®å‚æ•° ===
input_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/gpt4_judge_inputs/pairwise_run2_fdivA.jsonl"
output_path = input_path.replace(".jsonl", "_results.jsonl")
model = "gpt-4o"
sleep_time = 1.0
max_retries = 3

# === åŠ è½½è¾“å…¥æ•°æ® ===
with open(input_path, "r", encoding="utf-8") as f:
    all_prompts = [json.loads(line) for line in f]

# === æ–­ç‚¹ç»­è·‘ï¼ˆè¯»å–å·²æœ‰ç»“æœï¼‰
completed_ids = set()
results = []
if os.path.exists(output_path):
    with open(output_path, "r", encoding="utf-8") as f:
        for line in f:
            item = json.loads(line)
            results.append(item)
            completed_ids.add(item["pair_id"])
    print(f"ğŸ” æ£€æµ‹åˆ°å·²æœ‰ç»“æœï¼Œå·²è·³è¿‡ {len(completed_ids)} æ¡")

# === å¼€å§‹è¯„ä¼° ===
start_time = time.time()

for item in tqdm(all_prompts, desc="ğŸ§  GPT-4o è¯„ä¼°ä¸­"):
    pid = item["pair_id"]
    if pid in completed_ids:
        continue

    if item.get("auto_result") == "Tie":
        item["gpt_judgment"] = "Tie"
        print(f"ğŸ¤ pair_id={pid} â†’ Auto-Tie")
        results.append(item)
        continue

    prompt = item["formatted_prompt"]
    for attempt in range(max_retries):
        try:
            response = openai.ChatCompletion.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.0,
            )
            reply = response.choices[0].message["content"].strip()
            item["gpt_raw_response"] = reply

            last_line = reply.strip().splitlines()[-1].strip().upper()
            if "MORE HELPFUL: A" in last_line or last_line == "A":
                item["gpt_judgment"] = "A"
            elif "MORE HELPFUL: B" in last_line or last_line == "B":
                item["gpt_judgment"] = "B"
            else:
                item["gpt_judgment"] = "Unclear"

            print(f"âœ… pair_id={pid} â†’ {item['gpt_judgment']}")
            break
        except Exception as e:
            item["error"] = str(e)
            print(f"âŒ pair_id={pid} â†’ Error: {str(e)}")
            time.sleep(sleep_time)
    else:
        item["gpt_judgment"] = "Error"
        print(f"âŒ pair_id={pid} â†’ Failed after {max_retries} attempts")

    results.append(item)
    time.sleep(sleep_time)

# === ä¿å­˜æ‰€æœ‰ç»“æœ ===
with open(output_path, "w", encoding="utf-8") as f:
    for r in results:
        f.write(json.dumps(r, ensure_ascii=False) + "\n")

end_time = time.time()
print(f"\nâœ… è¯„ä¼°å®Œæˆï¼Œå…±è®¡è¯„ä¼° {len(results)} æ¡")
print(f"ğŸ•’ æ€»è€—æ—¶ï¼š{end_time - start_time:.1f} ç§’")
print(f"ğŸ“ ä¿å­˜ç»“æœåˆ°ï¼š{output_path}")

#èƒœç‡ç»Ÿè®¡

import json
import pandas as pd
from collections import Counter

# âœ… è·¯å¾„é…ç½®
run1_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/gpt4_judge_inputs/pairwise_run1_baselineA_results.jsonl"
run2_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/gpt4_judge_inputs/pairwise_run2_fdivA_results.jsonl"

# âœ… åŠ è½½ JSONL æ–‡ä»¶
def load_jsonl(path):
    with open(path, "r", encoding="utf-8") as f:
        return [json.loads(line) for line in f]

run1 = load_jsonl(run1_path)
run2 = load_jsonl(run2_path)

# âœ… æ ¸å¿ƒåˆ¤æ–­é€»è¾‘ï¼ˆè°èµ¢äº†ï¼Ÿï¼‰
def resolve_winner(entry):
    judge = entry.get("gpt_judgment", "").strip().upper()
    order = entry.get("order")

    if judge == "TIE":
        return "Tie"
    elif judge == "ERROR" or judge == "UNCLEAR":
        return "Invalid"

    # æ ¹æ® A/B åˆ¤æ–­è°èµ¢
    if order == "baseline_A":
        return "Baseline" if judge == "A" else "f-div"
    elif order == "fdiv_A":
        return "f-div" if judge == "A" else "Baseline"
    else:
        return "Invalid"

# âœ… æ±‡æ€»ç»Ÿè®¡
def count_results(entries, run_name):
    counter = Counter()
    for e in entries:
        winner = resolve_winner(e)
        counter[winner] += 1
    total = sum(counter.values())
    win_rate = {
        "Run": run_name,
        "Total": total,
        "f-div wins": counter["f-div"],
        "baseline wins": counter["Baseline"],
        "Ties": counter["Tie"],
        "Invalid": counter["Invalid"],
        "f-div win rate (%)": round(counter["f-div"] / total * 100, 2) if total else 0,
        "baseline win rate (%)": round(counter["Baseline"] / total * 100, 2) if total else 0,
        "Tie rate (%)": round(counter["Tie"] / total * 100, 2) if total else 0
    }
    return win_rate

# âœ… è¾“å‡ºä¸º DataFrame
df_stats = pd.DataFrame([
    count_results(run1, "Run1 (baseline in A)"),
    count_results(run2, "Run2 (f-div in A)")
])

# âœ… æ˜¾ç¤ºç»Ÿè®¡ç»“æœ
from IPython.display import display
display(df_stats)

#éšæœºæ‰“ä¹±é¡ºåºè¯„ä¼°

import pandas as pd
import json
import os
import random

# === è®¾ç½®è·¯å¾„ ===
input_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/v5_final_merged_for_judge.csv"
output_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/gpt4_judge_inputs/pairwise_randomized.jsonl"

# === è¯»å–åˆå¹¶æ•°æ® ===
df = pd.read_csv(input_path)

# === è®¾ç½®éšæœºç§å­ï¼ˆå¯é€‰ï¼‰===
random.seed(42)

# === æ„é€ å•è½®éšæœºåŒ– JSONL ===
output = []
tie_count = 0

for idx, row in df.iterrows():
    prompt = row["prompt"]
    baseline = row["baseline_best_response"]
    fdiv = row["f_best_response"]
    pid = row["prompt_id"]
    baseline_id = row["response_id"]

    # å¦‚æœä¸¤å“åº”å®Œå…¨ä¸€æ ·ï¼Œè‡ªåŠ¨ Tie
    if baseline.strip() == fdiv.strip():
        output.append({
            "pair_id": idx,
            "prompt_id": pid,
            "auto_result": "Tie",
            "baseline_id": baseline_id,
            "a_origin": "baseline",
            "b_origin": "f-div",
            "formatted_prompt": "[HH-RLHF]: (Responses identical, auto tie)"
        })
        tie_count += 1
        continue

    # éšæœºæ‰“ä¹±é¡ºåº
    if random.random() < 0.5:
        response_a, response_b = baseline, fdiv
        a_origin, b_origin = "baseline", "f-div"
    else:
        response_a, response_b = fdiv, baseline
        a_origin, b_origin = "f-div", "baseline"

    full_prompt = f"""[HH-RLHF]: For the following query to a chatbot, which response is more helpful?

Query: {prompt}

Response A: {response_a}

Response B: {response_b}

FIRST provide a one-sentence comparison of the two responses and explain which you feel is more helpful.
SECOND, on a new line, state only 'A' or 'B' to indicate which response is more helpful.

Format:
Comparison: ...
More helpful: A/B"""

    output.append({
        "pair_id": idx,
        "prompt_id": pid,
        "auto_result": None,
        "baseline_id": baseline_id,
        "a_origin": a_origin,
        "b_origin": b_origin,
        "formatted_prompt": full_prompt
    })

# === ä¿å­˜ä¸º JSONL ===
with open(output_path, "w", encoding="utf-8") as f:
    for item in output:
        f.write(json.dumps(item, ensure_ascii=False) + "\n")

print(f"âœ… å·²æ„é€  JSONLï¼Œå…± {len(output)} æ¡ï¼Œè‡ªåŠ¨ Tie: {tie_count}")
print(f"ğŸ“ ä¿å­˜è‡³ï¼š{output_path}")

#gpt4oå¼€å§‹è¯„ä¼°

import openai
import os

# è®¾ç½® API Key å’Œä»£ç†åœ°å€ï¼ˆå¦‚æœä½¿ç”¨ä»£ç†ï¼‰
os.environ["OPENAI_API_KEY"] = "sk-XGGe5y0ZvLcQVFp6XnRizs7q47gsVnAbZx0Xr2mfcVlbr99f"
openai.api_key = os.environ["OPENAI_API_KEY"]
openai.api_base = "https://api2.aigcbest.top/v1"  # å¦‚æœç”¨ä»£ç†å°±æ”¹è¿™é‡Œ

print("âœ… OpenAI è®¾ç½®å®Œæˆ")

import openai
import json
import time
import os
from tqdm import tqdm

# === å‚æ•°é…ç½® ===
input_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/gpt4_judge_inputs/pairwise_randomized.jsonl"
output_path = input_path.replace(".jsonl", "_results.jsonl")
model = "gpt-4o"
sleep_time = 1.0
max_retries = 3

# === åŠ è½½è¾“å…¥æ•°æ® ===
with open(input_path, "r", encoding="utf-8") as f:
    all_prompts = [json.loads(line) for line in f]

# === è¯»å–å·²å®Œæˆé¡¹ï¼ˆæ”¯æŒæ–­ç‚¹ç»­è·‘ï¼‰===
completed_ids = set()
results = []
if os.path.exists(output_path):
    with open(output_path, "r", encoding="utf-8") as f:
        for line in f:
            item = json.loads(line)
            results.append(item)
            completed_ids.add(item["pair_id"])
    print(f"ğŸ” å·²åŠ è½½ {len(completed_ids)} æ¡å†å²ç»“æœï¼Œè·³è¿‡")

# === å¼€å§‹è¯„ä¼° ===
start_time = time.time()

for item in tqdm(all_prompts, desc="ğŸ§  GPT-4o è¯„ä¼°ä¸­"):
    pid = item["pair_id"]
    if pid in completed_ids:
        continue

    if item.get("auto_result") == "Tie":
        item["gpt_judgment"] = "Tie"
        print(f"ğŸ¤ pair_id={pid} â†’ Auto-Tie")
        results.append(item)
        continue

    prompt = item["formatted_prompt"]
    for attempt in range(max_retries):
        try:
            response = openai.ChatCompletion.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.0,
            )
            reply = response.choices[0].message["content"].strip()
            item["gpt_raw_response"] = reply

            last_line = reply.strip().splitlines()[-1].strip().upper()
            if "MORE HELPFUL: A" in last_line or last_line == "A":
                item["gpt_judgment"] = "A"
            elif "MORE HELPFUL: B" in last_line or last_line == "B":
                item["gpt_judgment"] = "B"
            else:
                item["gpt_judgment"] = "Unclear"

            print(f"âœ… pair_id={pid} â†’ {item['gpt_judgment']}")
            break
        except Exception as e:
            item["error"] = str(e)
            print(f"âŒ pair_id={pid} â†’ Error: {str(e)}")
            time.sleep(sleep_time)
    else:
        item["gpt_judgment"] = "Error"
        print(f"âŒ pair_id={pid} â†’ Failed after {max_retries} attempts")

    results.append(item)
    time.sleep(sleep_time)

# === ä¿å­˜è¯„ä¼°ç»“æœ ===
with open(output_path, "w", encoding="utf-8") as f:
    for r in results:
        f.write(json.dumps(r, ensure_ascii=False) + "\n")

end_time = time.time()
print(f"\nâœ… è¯„ä¼°å®Œæˆï¼Œå…±è®¡ {len(results)} æ¡")
print(f"ğŸ•’ è€—æ—¶ï¼š{end_time - start_time:.1f} ç§’")
print(f"ğŸ“ è¾“å‡ºç»“æœè·¯å¾„ï¼š{output_path}")

#ç»Ÿè®¡èƒœç‡

import json
from collections import Counter
import pandas as pd
import matplotlib.pyplot as plt

# === æ›¿æ¢ä¸ºä½ çš„ç»“æœæ–‡ä»¶è·¯å¾„ ===
results_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/gpt4_judge_inputs/pairwise_randomized_results.jsonl"

# === åŠ è½½æ•°æ® ===
with open(results_path, "r", encoding="utf-8") as f:
    data = [json.loads(line) for line in f]

# === ç»Ÿè®¡æ ¸å¿ƒå‡½æ•° ===
def judge_winner(item):
    judgment = item.get("gpt_judgment", "")
    a_origin = item.get("a_origin")
    b_origin = item.get("b_origin")

    if judgment == "Tie":
        return "Tie"
    elif judgment == "A":
        return a_origin  # è°åœ¨Aä½èƒœ
    elif judgment == "B":
        return b_origin  # è°åœ¨Bä½èƒœ
    elif judgment == "Unclear":
        return "Unclear"
    elif judgment == "Error":
        return "Error"
    else:
        return "Invalid"

# === åº”ç”¨ç»Ÿè®¡ ===
outcomes = [judge_winner(item) for item in data]
counter = Counter(outcomes)

# === æ•´ç†ä¸º DataFrame å±•ç¤º ===
total = sum(counter.values())
win_fdiv = counter.get("f-div", 0)
win_baseline = counter.get("baseline", 0)
tie = counter.get("Tie", 0)
unclear = counter.get("Unclear", 0)
error = counter.get("Error", 0)

summary = pd.DataFrame([{
    "Total": total,
    "f-div wins": win_fdiv,
    "baseline wins": win_baseline,
    "Ties": tie,
    "Unclear": unclear,
    "Error": error,
    "f-div win rate (%)": round(win_fdiv / total * 100, 2),
    "baseline win rate (%)": round(win_baseline / total * 100, 2),
    "Tie rate (%)": round(tie / total * 100, 2),
    "Unclear/Error rate (%)": round((unclear + error) / total * 100, 2)
}])

# === æ‰“å°ç»“æœ ===
print(summary)

# === å¯è§†åŒ–èƒœç‡æ¡å½¢å›¾ ===
summary_plot = {
    "f-div": win_fdiv,
    "baseline": win_baseline,
    "Tie": tie,
    "Unclear": unclear,
    "Error": error
}

# === å¸¦ç™¾åˆ†æ¯”æ ‡æ³¨çš„å¯è§†åŒ– ===
plt.figure(figsize=(8, 4))
bars = plt.bar(summary_plot.keys(), summary_plot.values(), color=["#2ca02c", "#1f77b4", "#ff7f0e", "#999999", "#d62728"])

plt.title("GPT-4o Judged Outcome (Randomized A/B)", fontsize=14)
plt.ylabel("Count")
plt.xticks(fontsize=10)
plt.grid(axis="y", linestyle="--", alpha=0.5)

# âœ… åœ¨æ¯ä¸ªæŸ±å­ä¸Šæ–¹æ ‡æ³¨ç™¾åˆ†æ¯”
for bar, label in zip(bars, summary_plot.keys()):
    count = bar.get_height()
    pct = count / total * 100
    plt.text(bar.get_x() + bar.get_width()/2, count + 2, f"{pct:.1f}%", ha="center", va="bottom", fontsize=10)

plt.tight_layout()
plt.show()



#v10 2000æ¡

import numpy as np

v_main = np.array([0.7071, 0.7071])  # ä¸»æ–¹å‘ vâ‚â‚€ = 45Â°
angle_offsets = [-60, -45, -30, -15, 15, 30, 45, 60]

perturbed_vs = []
perturbed_angles = []

for offset in angle_offsets:
    angle_deg = 45 + offset  # å›´ç»• 45Â° åšæ‰°åŠ¨
    angle_rad = np.radians(angle_deg)
    v = np.array([np.cos(angle_rad), np.sin(angle_rad)])
    perturbed_vs.append(v)
    perturbed_angles.append(angle_deg)

def kl_divergence(p, q):
    p = np.abs(p) / np.sum(np.abs(p))  # å½’ä¸€åŒ–æˆä¼ªæ¦‚ç‡åˆ†å¸ƒ
    q = np.abs(q) / np.sum(np.abs(q))
    return np.sum(p * np.log(p / (q + 1e-12)))  # é˜²æ­¢é™¤é›¶

delta = 0.1 # KL æ•£åº¦å®¹å·®
kl_values = []

for v in perturbed_vs:
    kl = kl_divergence(v, v_main)
    kl_values.append(kl)

# âœ… ç­›é€‰æ–¹å‘ï¼ŒKL â‰¤ Î´
valid_vs = []
valid_angles = []

for v, angle, kl in zip(perturbed_vs, perturbed_angles, kl_values):
    if kl <= delta:
        valid_vs.append(v)
        valid_angles.append(angle)

print(valid_vs)
print(valid_angles)

import matplotlib.pyplot as plt

fig, ax = plt.subplots(figsize=(6, 6))
ax.add_artist(plt.Circle((0, 0), 1, color='lightgray', fill=False))
ax.quiver(0, 0, v_main[0], v_main[1], angles='xy', scale_units='xy', scale=1, color='blue', label='Main v (45Â°)')

for v, angle, kl in zip(perturbed_vs, perturbed_angles, kl_values):
    color = 'orange' if kl <= delta else 'gray'
    ax.quiver(0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1, color=color, alpha=0.8)
    ax.text(v[0]*1.1, v[1]*1.1, f"{angle}Â°", fontsize=8, ha='center', color=color)

ax.set_xlim(-1.2, 1.2)
ax.set_ylim(-1.2, 1.2)
ax.set_aspect('equal')
ax.set_title("f-div Ball (KL â‰¤ Î´) around v5 (45Â°)")
ax.legend()
plt.grid(True)
plt.show()

#ç”Ÿæˆv10 2000

from google.colab import drive
drive.mount('/content/drive')

import os
import numpy as np
import pandas as pd
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM
from tqdm.auto import tqdm
import torch
import time
import random
Â·
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
seed = 42
np.random.seed(seed)
random.seed(seed)
torch.manual_seed(seed)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(seed)

# âœ… STEP 2: ä¿å­˜è·¯å¾„
result_dir = "/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v10"
os.makedirs(result_dir, exist_ok=True)


# âœ… STEP 3: åŠ è½½å‰ 500 æ¡ UltraFeedback prompt
ds = load_dataset("HuggingFaceH4/ultrafeedback_binarized", split="test_prefs")
prompts = ds["prompt"][:2000]
prompt_ids = list(range(2000))

# âœ… STEP 4: åŠ è½½ DPA æ¨¡å‹å’Œ tokenizer
model = AutoModelForCausalLM.from_pretrained(
    "Haoxiang-Wang/DPA-v1-Mistral-7B",
    torch_dtype=torch.float16,
    device_map="auto"
).to(device)

tokenizer = AutoTokenizer.from_pretrained("Haoxiang-Wang/RewardModel-Mistral-7B-for-DPA-v1")
tokenizer.padding_side = "left"
if tokenizer.pad_token_id is None:
    tokenizer.pad_token = tokenizer.eos_token

# âœ… STEP 5: æ„é€  DPA Prompt
def build_input(prompt, v1, v2):
    h = int(np.round(v1 * 100))
    v = int(np.round(v2 * 100))
    sys_instruction = f"You are a helpful assistant. Your response should maximize weighted rating = helpfulness*{h} + verbosity*{v}."
    return [{"role": "user", "content": f"{sys_instruction}\n\n{prompt}"}]

# âœ… STEP 6: æ‰¹é‡ç”Ÿæˆ response
def generate_response_batch(prompts_batch, prompt_ids_batch, v1, v2):
    input_ids_list = []

    for prompt in prompts_batch:
        messages = build_input(prompt, v1, v2)
        input_ids = tokenizer.apply_chat_template(
            messages, add_generation_prompt=True, return_tensors="pt"
        )[0]
        input_ids_list.append(input_ids.to(device))

    input_ids_padded = torch.nn.utils.rnn.pad_sequence(
        input_ids_list, batch_first=True, padding_value=tokenizer.pad_token_id
    ).to(device)

    attention_mask = (input_ids_padded != tokenizer.pad_token_id).to(device)
    max_input_len = input_ids_padded.shape[1]
    max_new_tokens = min(2048, 4096 - max_input_len)

    with torch.no_grad():
        outputs = model.generate(
            input_ids=input_ids_padded,
            attention_mask=attention_mask,
            max_new_tokens=max_new_tokens,
            temperature=0.7,
            do_sample=True,
            num_return_sequences=1,
            pad_token_id=tokenizer.eos_token_id
        )

    responses = []
    for i, input_ids in enumerate(input_ids_list):
        generated_tokens = outputs[i][input_ids.shape[0]:]
        decoded = tokenizer.decode(generated_tokens, skip_special_tokens=True)
        responses.append({
            "prompt_id": prompt_ids_batch[i],
            "prompt": prompts_batch[i],
            "response": decoded
        })
    return responses

# âœ… STEP 7: ä½ è‡ªå·±çš„ valid_vs å’Œ valid_anglesï¼ˆè¯·æ›¿æ¢ä¸ºä½ çš„çœŸå®å€¼ï¼‰
valid_vs = [
    np.array([0.8660, 0.5000]),  # è§’åº¦ â‰ˆ 30Â°
    np.array([0.5000, 0.8660])   # è§’åº¦ â‰ˆ 60Â°
]

valid_angles = [30, 60]  # ä¸ valid_vs é¡ºåºä¸€è‡´

# âœ… ä¸»æ–¹å‘ï¼ˆv10ï¼‰
main_v1 = 0.7071
main_v2 = 0.7071

# âœ… STEP 8: ä¸»ç”Ÿæˆå¾ªç¯
batch_size = 8

for i, (v_vec, angle_deg) in enumerate(zip(valid_vs, valid_angles)):
    v1, v2 = v_vec[0], v_vec[1]
    output_file = os.path.join(result_dir, f"fdiv_v10_dir{i}.csv")
    print(f"\nğŸš€ Generating for direction {i}: angle â‰ˆ {angle_deg}Â°, v = ({v1:.4f}, {v2:.4f})")

    if os.path.exists(output_file):
        existing_df = pd.read_csv(output_file)
        done_prompt_ids = set(existing_df["prompt_id"].unique())
        results = existing_df.to_dict("records")
        print(f"ğŸ” Resuming from previous run: {len(done_prompt_ids)} prompts already completed.")
    else:
        done_prompt_ids = set()
        results = []

    for start in tqdm(range(0, len(prompts), batch_size), desc=f"Generating fdiv_dir{i}"):
        end = min(start + batch_size, len(prompts))
        batch_prompts_all = prompts[start:end]
        batch_ids_all = prompt_ids[start:end]

        unprocessed_indices = [j for j, pid in enumerate(batch_ids_all) if pid not in done_prompt_ids]
        if not unprocessed_indices:
            continue

        batch_prompts = [batch_prompts_all[j] for j in unprocessed_indices]
        batch_ids = [batch_ids_all[j] for j in unprocessed_indices]

        try:
            batch_outputs = generate_response_batch(batch_prompts, batch_ids, v1, v2)
            for item in batch_outputs:
                item.update({
                    "v1_p": round(v1, 4),
                    "v2_p": round(v2, 4),
                    "direction_index": i,
                    "valid_angle": round(angle_deg, 1),
                    "main_v1": round(main_v1, 4),
                    "main_v2": round(main_v2, 4)
                })
                results.append(item)

            pd.DataFrame(batch_outputs).to_csv(
                output_file, mode='a', index=False,
                header=not os.path.exists(output_file)
            )

        except Exception as e:
            print(f"âš ï¸ Error at batch {start}-{end}: {e}")

    print(f"âœ… Final saved {len(results)} responses to {output_file}")

#ä¿®å¤v10

import os
import pandas as pd
import numpy as np
import torch
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForCausalLM
from accelerate import Accelerator

# === æ¨¡å‹å‡†å¤‡ ===
device = "cuda" if torch.cuda.is_available() else "cpu"
accelerator = Accelerator()

model = AutoModelForCausalLM.from_pretrained(
    "Haoxiang-Wang/DPA-v1-Mistral-7B",
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
    device_map="auto" if torch.cuda.is_available() else None
)
model = accelerator.prepare(model)

tokenizer = AutoTokenizer.from_pretrained("Haoxiang-Wang/DPA-v1-Mistral-7B")
tokenizer.padding_side = "left"
if tokenizer.pad_token_id is None:
    tokenizer.pad_token = tokenizer.eos_token

# === åˆ¤æ–­æ˜¯å¦æ— æ•ˆ response ===
def is_invalid_response(resp):
    if pd.isna(resp):
        return True
    resp = str(resp).strip()
    if not resp:
        return True

    invalid_keywords = [
        "[A:]", "[/SOLUTION]", "[ASS]", "[A]:", "[/CONV]", "[/CODE]", "[/USER]",
        "[OUTPUT]", "[CC]", "[OUT]", "[/DATA]", "[/ASST]", "[/ME]", "[/REST]",
        "[RESP]", "<[user]>", "<|user|>", "[Response]", "[Assistant]", "[Answer]",
        "[End of Response]", "<human>", "<|endoftext|>", "<<", "[INST]", "[/INST]",
        "[]", "[USER]"
    ]
    tokens = resp.split()
    if all(token.strip() in invalid_keywords for token in tokens):
        return True
    tag_like_count = sum(1 for t in tokens if (t.startswith("[") and t.endswith("]")) or (t.startswith("<") and t.endswith(">")))
    if len(tokens) > 0 and tag_like_count / len(tokens) > 0.8:
        return True
    return False

# === æ„é€ è¾“å…¥ prompt ===
def build_input(prompt, v1, v2):
    h = int(np.round(v1 * 100))
    v = int(np.round(v2 * 100))
    sys_instruction = f"You are a helpful assistant. Your response should maximize weighted rating = helpfulness*{h} + verbosity*{v}."
    return [{"role": "system", "content": sys_instruction}, {"role": "user", "content": prompt}]

# === ç”¨æ¨¡å‹é‡æ–°ç”Ÿæˆ response ===
def regenerate_response(prompt, v1, v2):
    try:
        messages = build_input(prompt, v1, v2)
        input_ids = tokenizer.apply_chat_template(
            messages, add_generation_prompt=True, return_tensors="pt"
        ).to(device)

        if input_ids.ndim != 2 or input_ids.shape[1] == 0:
            print(f"âš ï¸ Invalid input_ids shape {input_ids.shape}, skipping: {prompt[:60]}")
            return ""

        outputs = model.generate(
            input_ids=input_ids,
            temperature=0.7,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
        generated = outputs[0][input_ids.shape[1]:]
        return tokenizer.decode(generated, skip_special_tokens=True).strip()

    except Exception as e:
        print(f"âŒ Error: {e} \nPrompt: {prompt[:60]}")
        return ""

# === ä¿®å¤å•ä¸ª CSV æ–‡ä»¶ ===
def fix_csv_responses(csv_path, save_dir):
    print(f"\nğŸ“‚ æ­£åœ¨æ£€æŸ¥: {csv_path}")

    if not os.path.exists(csv_path):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {csv_path}")
        return

    df = pd.read_csv(csv_path)
    if "v1_p" not in df.columns or "v2_p" not in df.columns:
        print(f"âŒ ç¼ºå°‘ v1_p/v2_p åˆ—ï¼Œè·³è¿‡: {csv_path}")
        return

    bad_mask = df["response"].apply(is_invalid_response)
    num_bad = bad_mask.sum()

    if num_bad == 0:
        print("âœ… æ— éœ€ä¿®å¤ï¼Œè·³è¿‡ã€‚")
        return

    print(f"ğŸ”§ å‘ç° {num_bad} æ¡æ— æ•ˆ responseï¼Œå¼€å§‹é‡ç”Ÿæˆ...")
    for i in tqdm(bad_mask[bad_mask].index, total=num_bad, desc=os.path.basename(csv_path), dynamic_ncols=True):
        row = df.loc[i]
        new_response = regenerate_response(row["prompt"], row["v1_p"], row["v2_p"])
        df.at[i, "response"] = new_response

    os.makedirs(save_dir, exist_ok=True)
    file_name = os.path.basename(csv_path).replace(".csv", "_fixed.csv")
    save_path = os.path.join(save_dir, file_name)
    df.to_csv(save_path, index=False)
    print(f"âœ… ä¿®å¤å®Œæˆ â†’ å·²ä¿å­˜: {save_path}")

# === ä¸»é€»è¾‘ï¼šä¿®å¤æ‰€æœ‰ CSV ===
input_dir = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v10"
output_dir = os.path.join(input_dir, "fixed")

for file in os.listdir(input_dir):
    if file.endswith(".csv") and not file.endswith("_fixed.csv"):
        fix_csv_responses(os.path.join(input_dir, file), output_dir)

#10 fdiv æ‰“åˆ†

# === å¯¼å…¥åº“ ===
import os
import time
import pandas as pd
import numpy as np
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
from tqdm.auto import tqdm
from google.colab import drive

# === æŒ‚è½½ Google Drive ===
drive.mount('/content/drive')

# === è®¾ç½®è·¯å¾„ ===
input_dir = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v10/fixed"
output_dir = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v10/scored"
os.makedirs(output_dir, exist_ok=True)

# === åŠ è½½ Reward Model ===
print("ğŸ¤– Loading Reward Model...")
device = "cuda" if torch.cuda.is_available() else "cpu"
rm = AutoModelForSequenceClassification.from_pretrained(
    "Haoxiang-Wang/RewardModel-Mistral-7B-for-DPA-v1", trust_remote_code=True
).to(device)
tokenizer = AutoTokenizer.from_pretrained("Haoxiang-Wang/RewardModel-Mistral-7B-for-DPA-v1")

# === æ„é€ è¾“å…¥æ¨¡æ¿å¹¶æ‰“åˆ† ===
def score_response(prompt, response):
    template = "[INST] You must read the following conversation carefully and rate the assistant's response from score 0-100 in these aspects: helpfulness, correctness, coherence, honesty, complexity, verbosity\n\nUser: {prompt}\n\nAssistant: {response} [/INST]"
    inputs = tokenizer(template.format(prompt=prompt, response=response), return_tensors="pt").to(device)
    with torch.no_grad():
        logits = rm(**inputs).logits.squeeze().cpu().numpy()
    return logits[9], logits[4]  # helpfulness, verbosity

# === æ‰“åˆ†æ•´ä¸ª CSV æ–‡ä»¶ ===
def score_file(file_path):
    print(f"\nğŸ“‚ Scoring file: {file_path}")
    df = pd.read_csv(file_path)
    scores = []

    for i, row in tqdm(df.iterrows(), total=len(df), desc=os.path.basename(file_path), dynamic_ncols=True):
        try:
            h, v = score_response(row["prompt"], row["response"])
        except Exception as e:
            print(f"âš ï¸ Error on row {i}: {e}")
            h, v = None, None
        scores.append({"helpfulness": h, "verbosity": v})

    df_scores = pd.DataFrame(scores)
    df_all = pd.concat([df, df_scores], axis=1)

    file_name = os.path.basename(file_path).replace(".csv", "_scored.csv")
    save_path = os.path.join(output_dir, file_name)
    df_all.to_csv(save_path, index=False)
    print(f"âœ… Scoring done â†’ Saved to: {save_path}")
    return df_all

# === æ‰¹é‡æ‰“åˆ†ç›®å½•ä¸­çš„æ‰€æœ‰ _fixed.csv æ–‡ä»¶ ===
for file in os.listdir(input_dir):
    if file.endswith("_fixed.csv"):
        score_file(os.path.join(input_dir, file))

#é€‰å‡ºf_div v10 best response

from google.colab import drive
drive.mount('/content/drive')

import os
import pandas as pd

# === è®¾ç½®è·¯å¾„ï¼ˆæ”¹ä¸º Colab ä¸‹çš„ç»å¯¹è·¯å¾„ï¼‰===
input_dir = "/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v10/scored"
output_file = os.path.join(input_dir, "fdiv_v10_all_best_response.csv")

# === ä¸»æ–¹å‘è®¾ç½®ï¼ˆv10ï¼‰
main_v1 = 0.7071
main_v2 = 0.7071

# === åˆå¹¶æ‰€æœ‰ scored æ–‡ä»¶
dfs = []
for file in os.listdir(input_dir):
    if file.endswith("_scored.csv"):
        df = pd.read_csv(os.path.join(input_dir, file))
        df["main_v1"] = main_v1
        df["main_v2"] = main_v2
        df["score_total"] = df["main_v1"] * df["helpfulness"] + df["main_v2"] * df["verbosity"]
        dfs.append(df)

df_all = pd.concat(dfs, ignore_index=True)
df_best = df_all.loc[df_all.groupby("prompt_id")["score_total"].idxmax()]
df_best = df_best.rename(columns={"response": "f_best_response"})

# === ä¿ç•™å­—æ®µ
cols_to_keep = [
    "prompt_id", "prompt", "f_best_response",
    "v1_p", "v2_p", "valid_angle",
    "main_v1", "main_v2",
    "helpfulness", "verbosity", "score_total"
]
df_best_clean = df_best[cols_to_keep]

# === ä¿å­˜ç»“æœ
df_best_clean.to_csv(output_file, index=False)
print(f"âœ… å·²ä¿å­˜åˆ°: {output_file}")

#baseline v10 æ‰“åˆ†ï¼ˆhelpfulness,verbosity)

# === å¯¼å…¥åº“ ===
import os
import time
import pandas as pd
import numpy as np
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
from tqdm.auto import tqdm
from google.colab import drive

# === æŒ‚è½½ Google Drive ===
drive.mount('/content/drive')

# === è®¾ç½®è¾“å…¥è¾“å‡ºè·¯å¾„ ===
input_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/fixed_batch/batch_v10_fixed.csv"
output_dir = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch"
os.makedirs(output_dir, exist_ok=True)

# === åŠ è½½æ•°æ® ===
print(f"ğŸ“¦ Loading file: {input_path}")
df = pd.read_csv(input_path)

# === åŠ è½½ Reward Model ===
print("ğŸ¤– Loading Reward Model...")
device = "cuda" if torch.cuda.is_available() else "cpu"
rm = AutoModelForSequenceClassification.from_pretrained(
    "Haoxiang-Wang/RewardModel-Mistral-7B-for-DPA-v1", trust_remote_code=True
).to(device)
tokenizer = AutoTokenizer.from_pretrained("Haoxiang-Wang/RewardModel-Mistral-7B-for-DPA-v1")

def score_response(prompt, response):
    template = "[INST] You must read the following conversation carefully and rate the assistant's response from score 0-100 in these aspects: helpfulness, correctness, coherence, honesty, complexity, verbosity\n\nUser: {prompt}\n\nAssistant: {response} [/INST]"
    inputs = tokenizer(template.format(prompt=prompt, response=response), return_tensors="pt").to(device)
    with torch.no_grad():
        logits = rm(**inputs).logits.squeeze().cpu().numpy()
    return logits[9], logits[4]  # helpfulness, verbosity

# === å¼€å§‹æ‰“åˆ† ===
print("ğŸ§  Scoring responses...")
start_time = time.time()
scores = []

for i, row in tqdm(df.iterrows(), total=len(df), desc="ğŸ” Scoring"):
    try:
        h, v = score_response(row["prompt"], row["response"])
        scores.append({"v1": row["v1"], "v2": row["v2"], "helpfulness": h, "verbosity": v})
    except Exception as e:
        print(f"âš ï¸ Error on row {i}: {e}")
        scores.append({"v1": row["v1"], "v2": row["v2"], "helpfulness": None, "verbosity": None})

df_scores = pd.DataFrame(scores)
df_all = pd.concat([df, df_scores], axis=1)

# === ä¿å­˜ç»“æœ ===
output_file = os.path.join(output_dir, os.path.basename(input_path).replace(".csv", "_scored.csv"))
df_all.to_csv(output_file, index=False)

end_time = time.time()
print(f"\nâœ… Scoring complete! Total scored: {len(df_all)}")
print(f"ğŸ•’ Time elapsed: {end_time - start_time:.1f} seconds")
print(f"ğŸ“ Saved to: {output_file}")

#é€‰å‡ºä¸¤ç»„æœ€å¥½çš„ç­”æ¡ˆ

import pandas as pd
import os

# === è®¾ç½®è¾“å…¥æ–‡ä»¶è·¯å¾„ ===
input_file = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch/batch_v10_fixed_scored.csv"

# === è®¾ç½®è¾“å‡ºè·¯å¾„ ===
output_dir = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch"
output_file_with_total = os.path.join(output_dir, "batch_v10_fixed_scored_with_total.csv")

# best response è¾“å‡ºç›®å½•å’Œæ–‡ä»¶å
best_response_dir = os.path.join(output_dir, "best_response")
os.makedirs(best_response_dir, exist_ok=True)
output_best_file = os.path.join(best_response_dir, "best_response_v10_baseline.csv")

# === åŠ è½½å·²æœ‰è¯„åˆ†æ–‡ä»¶ ===
df = pd.read_csv(input_file)

# === è®¡ç®—æ€»åˆ† ===
df["total_score"] = df["v1"] * df["helpfulness"] + df["v2"] * df["verbosity"]

# === ä¿å­˜å«æ€»åˆ†çš„æ–°æ–‡ä»¶ ===
df.to_csv(output_file_with_total, index=False)
print(f"âœ… Saved file with total_score: {output_file_with_total}")

# === é€‰å‡ºæ¯ä¸ª prompt_id ä¸‹ total_score æœ€é«˜çš„ response ===
df_best = df.loc[df.groupby("prompt_id")["total_score"].idxmax()].copy()

# å¯é€‰ï¼šå°† response åˆ—é‡å‘½åä¸º best_response
df_best = df_best.rename(columns={"response": "best_response"})

# === ä¿å­˜æ¯ä¸ª prompt_id çš„æœ€ä½³å›ç­”åˆ°æŒ‡å®šè·¯å¾„ ===
df_best.to_csv(output_best_file, index=False)
print(f"ğŸ† Best responses per prompt saved to: {output_best_file}")

import pandas as pd

# === è·¯å¾„è®¾ç½® ===
baseline_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/scored_batch/best_response/best_response_v10_baseline.csv"
fdiv_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/fdiv_v10/scored/fdiv_v10_all_best_response.csv"

# === åŠ è½½æ•°æ® ===
df_baseline = pd.read_csv(baseline_path)
df_fdiv = pd.read_csv(fdiv_path)

# === åˆå¹¶æ•°æ®ï¼ˆæŒ‰ prompt_idï¼‰
df_merged = pd.merge(
    df_baseline,
    df_fdiv,
    on="prompt_id",
    suffixes=("_baseline", "_fdiv")
)

# === å­—æ®µé‡å‘½å ===
df_merged = df_merged.rename(columns={
    "prompt_baseline": "prompt",
    "best_response": "baseline_best_response",
    "score_total": "f_score_total",                # æ¥è‡ª f-div
    "total_score": "baseline_score_total"          # æ¥è‡ª baseline
})

# === ä¿ç•™å­—æ®µï¼ŒåŒ…æ‹¬ baseline çš„ response_id
cols_to_show = [
    "prompt_id", "prompt",
    "baseline_best_response", "f_best_response",
    "baseline_score_total", "f_score_total",
    "response_id",                       # âœ… åŠ ä¸Š response_id
    "v1_p", "v2_p", "valid_angle",
    "main_v1", "main_v2"
]
df_clean = df_merged[cols_to_show]

# === ä¿å­˜ç»“æœ ===
output_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/v10_final_merged_for_judge.csv"
df_clean.to_csv(output_path, index=False)

print(f"âœ… åˆå¹¶å®Œæˆï¼Œæ–‡ä»¶ä¿å­˜è‡³ï¼š{output_path}")

#gpt4 å¯¹v10æ‰“åˆ†

import pandas as pd
import json
import os
import random

# === è®¾ç½®è·¯å¾„ ===
input_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/v10_final_merged_for_judge.csv"
output_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/gpt4_judge_inputs/v10_pairwise_randomized.jsonl"

# === è¯»å–åˆå¹¶æ•°æ® ===
df = pd.read_csv(input_path)

# === è®¾ç½®éšæœºç§å­ï¼ˆå¯é€‰ï¼‰===
random.seed(42)

# === æ„é€ å•è½®éšæœºåŒ– JSONL ===
output = []
tie_count = 0

for idx, row in df.iterrows():
    prompt = row["prompt"]
    baseline = row["baseline_best_response"]
    fdiv = row["f_best_response"]
    pid = row["prompt_id"]
    baseline_id = row["response_id"]

    # === ç¼ºå¤±å€¼æˆ–éå­—ç¬¦ä¸²å¤„ç† ===
    if not isinstance(baseline, str):
        baseline = str(baseline) if pd.notna(baseline) else ""
    if not isinstance(fdiv, str):
        fdiv = str(fdiv) if pd.notna(fdiv) else ""

    # å¦‚æœä¸¤å“åº”å®Œå…¨ä¸€æ ·ï¼Œè‡ªåŠ¨ Tie
    if baseline.strip() == fdiv.strip():
        output.append({
            "pair_id": idx,
            "prompt_id": pid,
            "auto_result": "Tie",
            "baseline_id": baseline_id,
            "a_origin": "baseline",
            "b_origin": "f-div",
            "formatted_prompt": "[HH-RLHF]: (Responses identical, auto tie)"
        })
        tie_count += 1
        continue

    # éšæœºæ‰“ä¹±é¡ºåº
    if random.random() < 0.5:
        response_a, response_b = baseline, fdiv
        a_origin, b_origin = "baseline", "f-div"
    else:
        response_a, response_b = fdiv, baseline
        a_origin, b_origin = "f-div", "baseline"

    full_prompt = f"""[HH-RLHF]: For the following query to a chatbot, which response is more helpful?

Query: {prompt}

Response A: {response_a}

Response B: {response_b}

FIRST provide a one-sentence comparison of the two responses and explain which you feel is more helpful.
SECOND, on a new line, state only 'A' or 'B' to indicate which response is more helpful.

Format:
Comparison: ...
More helpful: A/B"""

    output.append({
        "pair_id": idx,
        "prompt_id": pid,
        "auto_result": None,
        "baseline_id": baseline_id,
        "a_origin": a_origin,
        "b_origin": b_origin,
        "formatted_prompt": full_prompt
    })

# === ä¿å­˜ä¸º JSONL ===
with open(output_path, "w", encoding="utf-8") as f:
    for item in output:
        f.write(json.dumps(item, ensure_ascii=False) + "\n")

print(f"âœ… å·²æ„é€  JSONLï¼Œå…± {len(output)} æ¡ï¼Œè‡ªåŠ¨ Tie: {tie_count}")
print(f"ğŸ“ ä¿å­˜è‡³ï¼š{output_path}")

import openai
import os

# è®¾ç½® API Key å’Œä»£ç†åœ°å€ï¼ˆå¦‚æœä½¿ç”¨ä»£ç†ï¼‰
os.environ["OPENAI_API_KEY"] = "sk-XGGe5y0ZvLcQVFp6XnRizs7q47gsVnAbZx0Xr2mfcVlbr99f"
openai.api_key = os.environ["OPENAI_API_KEY"]
openai.api_base = "https://api2.aigcbest.top/v1"  # å¦‚æœç”¨ä»£ç†å°±æ”¹è¿™é‡Œ

print("âœ… OpenAI è®¾ç½®å®Œæˆ")

import openai
import json
import time
import os
from tqdm import tqdm

# === å‚æ•°é…ç½® ===
input_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/gpt4_judge_inputs/v10_pairwise_randomized.jsonl"
output_path = input_path.replace(".jsonl", "_results.jsonl")
model = "gpt-4o"
sleep_time = 1.0
max_retries = 3

# === åŠ è½½è¾“å…¥æ•°æ® ===
with open(input_path, "r", encoding="utf-8") as f:
    all_prompts = [json.loads(line) for line in f]

# === è¯»å–å·²å®Œæˆé¡¹ï¼ˆæ”¯æŒæ–­ç‚¹ç»­è·‘ï¼‰===
completed_ids = set()
results = []
if os.path.exists(output_path):
    with open(output_path, "r", encoding="utf-8") as f:
        for line in f:
            item = json.loads(line)
            results.append(item)
            completed_ids.add(item["pair_id"])
    print(f"ğŸ” å·²åŠ è½½ {len(completed_ids)} æ¡å†å²ç»“æœï¼Œè·³è¿‡")

# === å¼€å§‹è¯„ä¼° ===
start_time = time.time()

for item in tqdm(all_prompts, desc="ğŸ§  GPT-4o è¯„ä¼°ä¸­"):
    pid = item["pair_id"]
    if pid in completed_ids:
        continue

    if item.get("auto_result") == "Tie":
        item["gpt_judgment"] = "Tie"
        print(f"ğŸ¤ pair_id={pid} â†’ Auto-Tie")
        results.append(item)
        continue

    prompt = item["formatted_prompt"]
    for attempt in range(max_retries):
        try:
            response = openai.ChatCompletion.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.0,
            )
            reply = response.choices[0].message["content"].strip()
            item["gpt_raw_response"] = reply

            last_line = reply.strip().splitlines()[-1].strip().upper()
            if "MORE HELPFUL: A" in last_line or last_line == "A":
                item["gpt_judgment"] = "A"
            elif "MORE HELPFUL: B" in last_line or last_line == "B":
                item["gpt_judgment"] = "B"
            else:
                item["gpt_judgment"] = "Unclear"

            print(f"âœ… pair_id={pid} â†’ {item['gpt_judgment']}")
            break
        except Exception as e:
            item["error"] = str(e)
            print(f"âŒ pair_id={pid} â†’ Error: {str(e)}")
            time.sleep(sleep_time)
    else:
        item["gpt_judgment"] = "Error"
        print(f"âŒ pair_id={pid} â†’ Failed after {max_retries} attempts")

    results.append(item)
    time.sleep(sleep_time)

# === ä¿å­˜è¯„ä¼°ç»“æœ ===
with open(output_path, "w", encoding="utf-8") as f:
    for r in results:
        f.write(json.dumps(r, ensure_ascii=False) + "\n")

end_time = time.time()
print(f"\nâœ… è¯„ä¼°å®Œæˆï¼Œå…±è®¡ {len(results)} æ¡")
print(f"ğŸ•’ è€—æ—¶ï¼š{end_time - start_time:.1f} ç§’")
print(f"ğŸ“ è¾“å‡ºç»“æœè·¯å¾„ï¼š{output_path}")

import json
from collections import Counter
import pandas as pd
import matplotlib.pyplot as plt

# === æ›¿æ¢ä¸ºä½ çš„ç»“æœæ–‡ä»¶è·¯å¾„ ===
results_path = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/gpt4_judge_inputs/v10_pairwise_randomized_results.jsonl"

# === åŠ è½½æ•°æ® ===
with open(results_path, "r", encoding="utf-8") as f:
    data = [json.loads(line) for line in f]

# === ç»Ÿè®¡æ ¸å¿ƒå‡½æ•° ===
def judge_winner(item):
    judgment = item.get("gpt_judgment", "")
    a_origin = item.get("a_origin")
    b_origin = item.get("b_origin")

    if judgment == "Tie":
        return "Tie"
    elif judgment == "A":
        return a_origin  # è°åœ¨Aä½èƒœ
    elif judgment == "B":
        return b_origin  # è°åœ¨Bä½èƒœ
    elif judgment == "Unclear":
        return "Unclear"
    elif judgment == "Error":
        return "Error"
    else:
        return "Invalid"

# === åº”ç”¨ç»Ÿè®¡ ===
outcomes = [judge_winner(item) for item in data]
counter = Counter(outcomes)

# === æ•´ç†ä¸º DataFrame å±•ç¤º ===
total = sum(counter.values())
win_fdiv = counter.get("f-div", 0)
win_baseline = counter.get("baseline", 0)
tie = counter.get("Tie", 0)
unclear = counter.get("Unclear", 0)
error = counter.get("Error", 0)

summary = pd.DataFrame([{
    "Total": total,
    "f-div wins": win_fdiv,
    "baseline wins": win_baseline,
    "Ties": tie,
    "Unclear": unclear,
    "Error": error,
    "f-div win rate (%)": round(win_fdiv / total * 100, 2),
    "baseline win rate (%)": round(win_baseline / total * 100, 2),
    "Tie rate (%)": round(tie / total * 100, 2),
    "Unclear/Error rate (%)": round((unclear + error) / total * 100, 2)
}])

# === æ‰“å°ç»“æœ ===
print(summary)

# === å¯è§†åŒ–èƒœç‡æ¡å½¢å›¾ ===
summary_plot = {
    "f-div": win_fdiv,
    "baseline": win_baseline,
    "Tie": tie,
    "Unclear": unclear,
    "Error": error
}

# === å¸¦ç™¾åˆ†æ¯”æ ‡æ³¨çš„å¯è§†åŒ– ===
plt.figure(figsize=(8, 4))
bars = plt.bar(summary_plot.keys(), summary_plot.values(), color=["#2ca02c", "#1f77b4", "#ff7f0e", "#999999", "#d62728"])

plt.title("GPT-4o Judged Outcome (Randomized A/B)", fontsize=14)
plt.ylabel("Count")
plt.xticks(fontsize=10)
plt.grid(axis="y", linestyle="--", alpha=0.5)

# âœ… åœ¨æ¯ä¸ªæŸ±å­ä¸Šæ–¹æ ‡æ³¨ç™¾åˆ†æ¯”
for bar, label in zip(bars, summary_plot.keys()):
    count = bar.get_height()
    pct = count / total * 100
    plt.text(bar.get_x() + bar.get_width()/2, count + 2, f"{pct:.1f}%", ha="center", va="bottom", fontsize=10)

plt.tight_layout()
plt.show()

#v1 f-div

import numpy as np

v_main = np.array([1,0])  # ä¸»æ–¹å‘ vâ‚â‚€ = 0Â°
angle_offsets = [-60, -45, -30, -15, 0, 15, 30, 45, 60]

perturbed_vs = []
perturbed_angles = []

for offset in angle_offsets:
    angle_deg = 0 + offset  # å›´ç»• 0Â° åšæ‰°åŠ¨
    angle_rad = np.radians(angle_deg)
    v = np.array([np.cos(angle_rad), np.sin(angle_rad)])
    perturbed_vs.append(v)
    perturbed_angles.append(angle_deg)

def kl_divergence(p, q):
    p = np.abs(p) / np.sum(np.abs(p))  # å½’ä¸€åŒ–æˆä¼ªæ¦‚ç‡åˆ†å¸ƒ
    q = np.abs(q) / np.sum(np.abs(q))
    return np.sum(p * np.log(p / (q + 1e-12)))  # é˜²æ­¢é™¤é›¶

delta = 0.1 # KL æ•£åº¦å®¹å·®
kl_values = []

print(perturbed_vs)

for v in perturbed_vs:
    kl = kl_divergence(v, v_main)
    kl_values.append(kl)

# âœ… ç­›é€‰æ–¹å‘ï¼ŒKL â‰¤ Î´
valid_vs = []
valid_angles = []

for v, angle, kl in zip(perturbed_vs, perturbed_angles, kl_values):
    if kl <= delta:
        valid_vs.append(v)
        valid_angles.append(angle)



print(valid_vs)
print(valid_angles)

import numpy as np

v_main = np.array([(0.9962, 0.0872) ])  # ä¸»æ–¹å‘ vâ‚â‚€ = 0Â°
angle_offsets = [-60, -45, -30, -15, 15, 30, 45, 60]

perturbed_vs = []
perturbed_angles = []

for offset in angle_offsets:
    angle_deg = 5 + offset  # å›´ç»• 0Â° åšæ‰°åŠ¨
    angle_rad = np.radians(angle_deg)
    v = np.array([np.cos(angle_rad), np.sin(angle_rad)])
    perturbed_vs.append(v)
    perturbed_angles.append(angle_deg)

def kl_divergence(p, q):
    p = np.abs(p) / np.sum(np.abs(p))  # å½’ä¸€åŒ–æˆä¼ªæ¦‚ç‡åˆ†å¸ƒ
    q = np.abs(q) / np.sum(np.abs(q))
    return np.sum(p * np.log(p / (q + 1e-12)))  # é˜²æ­¢é™¤é›¶

delta = 0.1 # KL æ•£åº¦å®¹å·®
kl_values = []

print(perturbed_vs)

for v in perturbed_vs:
    kl = kl_divergence(v, v_main)
    kl_values.append(kl)

# âœ… ç­›é€‰æ–¹å‘ï¼ŒKL â‰¤ Î´
valid_vs = []
valid_angles = []

for v, angle, kl in zip(perturbed_vs, perturbed_angles, kl_values):
    if kl <= delta:
        valid_vs.append(v)
        valid_angles.append(angle)



print(valid_vs)
print(valid_angles)

import numpy as np

v_main = np.array([(0.9962, 0.0872) ])  # ä¸»æ–¹å‘ vâ‚â‚€ = 0Â°
angle_offsets = [-60, -45, -30, -15, 15, 30, 45, 60]

perturbed_vs = []
perturbed_angles = []

for offset in angle_offsets:
    angle_deg =  + offset  # å›´ç»• 0Â° åšæ‰°åŠ¨
    angle_rad = np.radians(angle_deg)
    v = np.array([np.cos(angle_rad), np.sin(angle_rad)])
    perturbed_vs.append(v)
    perturbed_angles.append(angle_deg)

def kl_divergence(p, q):
    p = np.abs(p) / np.sum(np.abs(p))  # å½’ä¸€åŒ–æˆä¼ªæ¦‚ç‡åˆ†å¸ƒ
    q = np.abs(q) / np.sum(np.abs(q))
    return np.sum(p * np.log(p / (q + 1e-12)))  # é˜²æ­¢é™¤é›¶

delta = 0.1 # KL æ•£åº¦å®¹å·®
kl_values = []

print(perturbed_vs)

for v in perturbed_vs:
    kl = kl_divergence(v, v_main)
    kl_values.append(kl)

# âœ… ç­›é€‰æ–¹å‘ï¼ŒKL â‰¤ Î´
valid_vs = []
valid_angles = []

for v, angle, kl in zip(perturbed_vs, perturbed_angles, kl_values):
    if kl <= delta:
        valid_vs.append(v)
        valid_angles.append(angle)



print(valid_vs)
print(valid_angles)















#Test-time Preference Shift

#Step 1ï½œå®šä¹‰å¤šä¸ªæ‰°åŠ¨æ–¹å‘

import numpy as np

# è®¾ç½®ä¸»æ–¹å‘è§’åº¦ï¼ˆv10 = 45Â°ï¼‰
main_angle = 45
offsets = [-30, -20, -10, 0, 10, 20, 30]

# æ„é€ åç§»è§’åº¦å¯¹åº”çš„æ–¹å‘å‘é‡å’Œè§’åº¦è®°å½•
valid_vs = []
valid_angles = []

for offset in offsets:
    angle = main_angle + offset
    rad = np.radians(angle)
    v = np.array([np.cos(rad), np.sin(rad)])
    valid_vs.append(v)
    valid_angles.append(angle)

print(valid_vs)
print(valid_angles)

#Step 2ï½œä½¿ç”¨å·²æœ‰è„šæœ¬ç”Ÿæˆæ¯ä¸ªè§’åº¦æ–¹å‘ä¸‹çš„å“åº”

from google.colab import drive
drive.mount('/content/drive')

import os
import numpy as np
import pandas as pd
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM
from tqdm.auto import tqdm
import torch
import time
import random

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
seed = 42
np.random.seed(seed)
random.seed(seed)
torch.manual_seed(seed)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(seed)

# âœ… STEP 2: ä¿å­˜è·¯å¾„
result_dir = "/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/v10_direction_sensitivity"
os.makedirs(result_dir, exist_ok=True)


# âœ… STEP 3: åŠ è½½å‰ 500 æ¡ UltraFeedback prompt
ds = load_dataset("HuggingFaceH4/ultrafeedback_binarized", split="test_prefs")
prompts = ds["prompt"][:100]
prompt_ids = list(range(100))

# âœ… STEP 4: åŠ è½½ DPA æ¨¡å‹å’Œ tokenizer
model = AutoModelForCausalLM.from_pretrained(
    "Haoxiang-Wang/DPA-v1-Mistral-7B",
    torch_dtype=torch.float16,
    device_map="auto"
).to(device)

tokenizer = AutoTokenizer.from_pretrained("Haoxiang-Wang/RewardModel-Mistral-7B-for-DPA-v1")
tokenizer.padding_side = "left"
if tokenizer.pad_token_id is None:
    tokenizer.pad_token = tokenizer.eos_token

# âœ… STEP 5: æ„é€  DPA Prompt
def build_input(prompt, v1, v2):
    h = int(np.round(v1 * 100))
    v = int(np.round(v2 * 100))
    sys_instruction = f"You are a helpful assistant. Your response should maximize weighted rating = helpfulness*{h} + verbosity*{v}."
    return [{"role": "user", "content": f"{sys_instruction}\n\n{prompt}"}]

# âœ… STEP 6: æ‰¹é‡ç”Ÿæˆ response
def generate_response_batch(prompts_batch, prompt_ids_batch, v1, v2):
    input_ids_list = []

    for prompt in prompts_batch:
        messages = build_input(prompt, v1, v2)
        input_ids = tokenizer.apply_chat_template(
            messages, add_generation_prompt=True, return_tensors="pt"
        )[0]
        input_ids_list.append(input_ids.to(device))

    input_ids_padded = torch.nn.utils.rnn.pad_sequence(
        input_ids_list, batch_first=True, padding_value=tokenizer.pad_token_id
    ).to(device)

    attention_mask = (input_ids_padded != tokenizer.pad_token_id).to(device)
    max_input_len = input_ids_padded.shape[1]
    max_new_tokens = min(2048, 4096 - max_input_len)

    with torch.no_grad():
        outputs = model.generate(
            input_ids=input_ids_padded,
            attention_mask=attention_mask,
            max_new_tokens=max_new_tokens,
            temperature=0.7,
            do_sample=True,
            num_return_sequences=1,
            pad_token_id=tokenizer.eos_token_id
        )

    responses = []
    for i, input_ids in enumerate(input_ids_list):
        generated_tokens = outputs[i][input_ids.shape[0]:]
        decoded = tokenizer.decode(generated_tokens, skip_special_tokens=True)
        responses.append({
            "prompt_id": prompt_ids_batch[i],
            "prompt": prompts_batch[i],
            "response": decoded
        })
    return responses

# âœ… STEP 7: ä½ è‡ªå·±çš„ valid_vs å’Œ valid_anglesï¼ˆè¯·æ›¿æ¢ä¸ºä½ çš„çœŸå®å€¼ï¼‰
valid_vs = [
    np.array([0.8660, 0.5000]),  # è§’åº¦ â‰ˆ 30Â°
    np.array([0.5000, 0.8660])   # è§’åº¦ â‰ˆ 60Â°
]

valid_angles = [30, 60]  # ä¸ valid_vs é¡ºåºä¸€è‡´

# âœ… ä¸»æ–¹å‘ï¼ˆv10ï¼‰
main_v1 = 0.7071
main_v2 = 0.7071
main_angle = 45  # âœ… è¡¥ä¸Šè¿™ä¸ªå®šä¹‰


# âœ… STEP 7: å®šä¹‰æ‰°åŠ¨æ–¹å‘ï¼ˆÂ±30Â°, Â±20Â°, Â±10Â°, 0Â°ï¼‰
# ä¸Šé¢ valid_vs å’Œ valid_angles å·²å‡†å¤‡å¥½
# âœ… STEP 7: å®šä¹‰æ‰°åŠ¨æ–¹å‘ï¼ˆÂ±30Â°, Â±20Â°, Â±10Â°, 0Â°, +10Â°, +20Â°, +30Â°ï¼‰
valid_vs = [
    np.array([0.96592583, 0.25881905]),  # 15Â°
    np.array([0.90630779, 0.42261826]),  # 25Â°
    np.array([0.81915204, 0.57357644]),  # 35Â°
    np.array([0.70710678, 0.70710678]),  # 45Â° (ä¸»æ–¹å‘)
    np.array([0.57357644, 0.81915204]),  # 55Â°
    np.array([0.42261826, 0.90630779]),  # 65Â°
    np.array([0.25881905, 0.96592583])   # 75Â°
]
valid_angles = [15, 25, 35, 45, 55, 65, 75]


# âœ… STEP 8: ç”Ÿæˆå“åº”
batch_size = 8
for i, (v_vec, angle_deg) in enumerate(zip(valid_vs, valid_angles)):
    v1, v2 = v_vec[0], v_vec[1]
    output_file = os.path.join(result_dir, f"v10_angle_{int(angle_deg)}.csv")
    print(f"\nğŸš€ Generating for direction {i}: angle = {angle_deg}Â°, v = ({v1:.4f}, {v2:.4f})")

    if os.path.exists(output_file):
        existing_df = pd.read_csv(output_file)
        done_prompt_ids = set(existing_df["prompt_id"].unique())
        results = existing_df.to_dict("records")
        print(f"ğŸ” Resuming from previous run: {len(done_prompt_ids)} prompts already completed.")
    else:
        done_prompt_ids = set()
        results = []

    for start in tqdm(range(0, len(prompts), batch_size), desc=f"Generating angle {angle_deg}Â°"):
        end = min(start + batch_size, len(prompts))
        batch_prompts_all = prompts[start:end]
        batch_ids_all = prompt_ids[start:end]

        unprocessed_indices = [j for j, pid in enumerate(batch_ids_all) if pid not in done_prompt_ids]
        if not unprocessed_indices:
            continue

        batch_prompts = [batch_prompts_all[j] for j in unprocessed_indices]
        batch_ids = [batch_ids_all[j] for j in unprocessed_indices]

        try:
            batch_outputs = generate_response_batch(batch_prompts, batch_ids, v1, v2)
            for item in batch_outputs:
                item.update({
                    "v1_p": round(v1, 4),
                    "v2_p": round(v2, 4),
                    "direction_index": i,
                    "angle_deg": round(angle_deg, 1),
                    "main_v1": round(np.cos(np.radians(main_angle)), 4),
                    "main_v2": round(np.sin(np.radians(main_angle)), 4)
                })
                results.append(item)

            pd.DataFrame(batch_outputs).to_csv(
                output_file, mode='a', index=False,
                header=not os.path.exists(output_file)
            )

        except Exception as e:
            print(f"âš ï¸ Error at batch {start}-{end}: {e}")

    print(f"âœ… Final saved {len(results)} responses to {output_file}")

import os
import pandas as pd
import numpy as np
import torch
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForCausalLM
from accelerate import Accelerator

# === æ¨¡å‹å‡†å¤‡ ===
device = "cuda" if torch.cuda.is_available() else "cpu"
accelerator = Accelerator()

model = AutoModelForCausalLM.from_pretrained(
    "Haoxiang-Wang/DPA-v1-Mistral-7B",
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
    device_map="auto" if torch.cuda.is_available() else None
)
model = accelerator.prepare(model)

tokenizer = AutoTokenizer.from_pretrained("Haoxiang-Wang/DPA-v1-Mistral-7B")
tokenizer.padding_side = "left"
if tokenizer.pad_token_id is None:
    tokenizer.pad_token = tokenizer.eos_token

# === åˆ¤æ–­æ˜¯å¦æ— æ•ˆ response ===
def is_invalid_response(resp):
    if pd.isna(resp):
        return True
    resp = str(resp).strip()
    if not resp:
        return True

    invalid_keywords = [
        "[A:]", "[/SOLUTION]", "[ASS]", "[A]:", "[/CONV]", "[/CODE]", "[/USER]",
        "[OUTPUT]", "[CC]", "[OUT]", "[/DATA]", "[/ASST]", "[/ME]", "[/REST]",
        "[RESP]", "<[user]>", "<|user|>", "[Response]", "[Assistant]", "[Answer]",
        "[End of Response]", "<human>", "<|endoftext|>", "<<", "[INST]", "[/INST]",
        "[]", "[USER]"
    ]
    tokens = resp.split()
    if all(token.strip() in invalid_keywords for token in tokens):
        return True
    tag_like_count = sum(1 for t in tokens if (t.startswith("[") and t.endswith("]")) or (t.startswith("<") and t.endswith(">")))
    if len(tokens) > 0 and tag_like_count / len(tokens) > 0.8:
        return True
    return False

# === æ„é€ è¾“å…¥ prompt ===
def build_input(prompt, v1, v2):
    h = int(np.round(v1 * 100))
    v = int(np.round(v2 * 100))
    sys_instruction = f"You are a helpful assistant. Your response should maximize weighted rating = helpfulness*{h} + verbosity*{v}."
    return [{"role": "system", "content": sys_instruction}, {"role": "user", "content": prompt}]

# === ç”¨æ¨¡å‹é‡æ–°ç”Ÿæˆ response ===
def regenerate_response(prompt, v1, v2):
    try:
        messages = build_input(prompt, v1, v2)
        input_ids = tokenizer.apply_chat_template(
            messages, add_generation_prompt=True, return_tensors="pt"
        ).to(device)

        if input_ids.ndim != 2 or input_ids.shape[1] == 0:
            print(f"âš ï¸ Invalid input_ids shape {input_ids.shape}, skipping: {prompt[:60]}")
            return ""

        outputs = model.generate(
            input_ids=input_ids,
            temperature=0.7,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
        generated = outputs[0][input_ids.shape[1]:]
        return tokenizer.decode(generated, skip_special_tokens=True).strip()

    except Exception as e:
        print(f"âŒ Error: {e} \nPrompt: {prompt[:60]}")
        return ""

# === ä¿®å¤å•ä¸ª CSV æ–‡ä»¶ ===
def fix_csv_responses(csv_path, save_dir):
    print(f"\nğŸ“‚ æ­£åœ¨æ£€æŸ¥: {csv_path}")

    if not os.path.exists(csv_path):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {csv_path}")
        return

    df = pd.read_csv(csv_path)
    if "v1_p" not in df.columns or "v2_p" not in df.columns:
        print(f"âŒ ç¼ºå°‘ v1_p/v2_p åˆ—ï¼Œè·³è¿‡: {csv_path}")
        return

    bad_mask = df["response"].apply(is_invalid_response)
    num_bad = bad_mask.sum()

    if num_bad == 0:
        print("âœ… æ— éœ€ä¿®å¤ï¼Œè·³è¿‡ã€‚")
        return

    print(f"ğŸ”§ å‘ç° {num_bad} æ¡æ— æ•ˆ responseï¼Œå¼€å§‹é‡ç”Ÿæˆ...")
    for i in tqdm(bad_mask[bad_mask].index, total=num_bad, desc=os.path.basename(csv_path), dynamic_ncols=True):
        row = df.loc[i]
        new_response = regenerate_response(row["prompt"], row["v1_p"], row["v2_p"])
        df.at[i, "response"] = new_response

    os.makedirs(save_dir, exist_ok=True)
    file_name = os.path.basename(csv_path).replace(".csv", "_fixed.csv")
    save_path = os.path.join(save_dir, file_name)
    df.to_csv(save_path, index=False)
    print(f"âœ… ä¿®å¤å®Œæˆ â†’ å·²ä¿å­˜: {save_path}")

# === ä¸»é€»è¾‘ï¼šä¿®å¤æ‰€æœ‰ CSV ===
input_dir = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/v10_direction_sensitivity"
output_dir = os.path.join(input_dir, "fixed")

for file in os.listdir(input_dir):
    if file.endswith(".csv") and not file.endswith("_fixed.csv"):
        fix_csv_responses(os.path.join(input_dir, file), output_dir)

import pandas as pd
import os

# ç»“æœç›®å½•
result_dir = "/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/v10_direction_sensitivity/fixed"
merged_path = os.path.join(result_dir, "merged_all_angles.csv")

# åˆå¹¶æ‰€æœ‰ CSV æ–‡ä»¶
all_data = []
for fname in os.listdir(result_dir):
    if fname.endswith(".csv") and not fname.startswith(".") and "merged" not in fname:
        fpath = os.path.join(result_dir, fname)
        try:
            df = pd.read_csv(fpath)
            all_data.append(df)
            print(f"ğŸ“„ Loaded {fname} with {len(df)} rows.")
        except Exception as e:
            print(f"âš ï¸ Failed to load {fname}: {e}")

if all_data:
    df_all = pd.concat(all_data, ignore_index=True)
    df_all.to_csv(merged_path, index=False)
    print(f"\nâœ… Merged total {len(df_all)} responses.")
    print(f"ğŸ“ Saved to: {merged_path}")
else:
    print("âŒ No valid CSV files found to merge.")

import os
import time
import pandas as pd
import numpy as np
import torch
from tqdm.auto import tqdm
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from google.colab import drive

# === æŒ‚è½½ Google Drive ===
drive.mount('/content/drive')

# === è®¾ç½®è·¯å¾„ ===
input_path = "/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/v10_direction_sensitivity/fixed/merged_all_angles.csv"
output_path = "/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/v10_direction_sensitivity/merged_all_angles_scored.csv"

# === åŠ è½½åˆå¹¶æ•°æ® ===
df = pd.read_csv(input_path)
print(f"ğŸ“¦ Loaded {len(df)} samples")

# === åŠ è½½ Reward Model ===
print("ğŸ¤– Loading Reward Model...")
device = "cuda" if torch.cuda.is_available() else "cpu"
rm = AutoModelForSequenceClassification.from_pretrained(
    "Haoxiang-Wang/RewardModel-Mistral-7B-for-DPA-v1", trust_remote_code=True
).to(device)
tokenizer = AutoTokenizer.from_pretrained("Haoxiang-Wang/RewardModel-Mistral-7B-for-DPA-v1")

def score_response(prompt, response):
    template = "[INST] You must read the following conversation carefully and rate the assistant's response from score 0-100 in these aspects: helpfulness, correctness, coherence, honesty, complexity, verbosity\n\nUser: {prompt}\n\nAssistant: {response} [/INST]"
    inputs = tokenizer(template.format(prompt=prompt, response=response), return_tensors="pt").to(device)
    with torch.no_grad():
        logits = rm(**inputs).logits.squeeze().cpu().numpy()
    return logits[9], logits[4]  # helpfulness, verbosity

# === æ‰¹é‡æ‰“åˆ† ===
help_scores, verb_scores = [], []

print("ğŸ§  Scoring all responses...")
for i, row in tqdm(df.iterrows(), total=len(df)):
    try:
        h, v = score_response(row["prompt"], row["response"])
    except Exception as e:
        print(f"âš ï¸ Error on row {i}: {e}")
        h, v = None, None
    help_scores.append(h)
    verb_scores.append(v)

df["reward_score_helpfulness"] = help_scores
df["reward_score_verbosity"] = verb_scores

# === ä¿å­˜æ‰“åˆ†ç»“æœ ===
df.to_csv(output_path, index=False)
print(f"âœ… All scored results saved to {output_path}")

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# === åŠ è½½æ‰“åˆ†åçš„ CSV ===
input_path = "/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/v10_direction_sensitivity/merged_all_angles_scored.csv"
df = pd.read_csv(input_path)

# === èšåˆä¸åŒæ–¹å‘çš„å¹³å‡åˆ† ===
df_grouped = df.groupby("angle_deg")[["reward_score_helpfulness", "reward_score_verbosity"]].agg(["mean", "std"]).reset_index()
df_grouped.columns = ["angle", "help_mean", "help_std", "verb_mean", "verb_std"]

# === å¯è§†åŒ–ï¼šhelpfulness å’Œ verbosity éšè§’åº¦å˜åŒ– ===
plt.figure(figsize=(10, 6))
plt.errorbar(df_grouped["angle"], df_grouped["help_mean"], yerr=df_grouped["help_std"], label="Helpfulness", fmt='-o')
plt.errorbar(df_grouped["angle"], df_grouped["verb_mean"], yerr=df_grouped["verb_std"], label="Verbosity", fmt='-o', color='orange')
plt.axvline(x=45, color='gray', linestyle='--', label="Main Direction (45Â°)")
plt.xlabel("Angle (degrees)")
plt.ylabel("Reward Score")
plt.title("Reward Score vs Direction Angle (Â±pertuation)")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

#v11

main_v1 = 0.9500
main_v2 = -0.3100
main_angle = np.degrees(np.arctan2(main_v2, main_v1))  # å¯è§†åŒ–æ—¶ç”¨

angles = [-30, -20, -10, 0, 10, 20, 30]  # ç›¸å¯¹ä¸»æ–¹å‘
valid_vs = []
valid_angles = []

for delta in angles:
    angle = np.arctan2(main_v2, main_v1) + np.radians(delta)
    v = np.array([np.cos(angle), np.sin(angle)])
    valid_vs.append(v)
    valid_angles.append(np.degrees(angle))  # å®é™…è§’åº¦

print(valid_vs)
print(valid_angles)
print(main_angle)

#Step 2ï½œä½¿ç”¨å·²æœ‰è„šæœ¬ç”Ÿæˆæ¯ä¸ªè§’åº¦æ–¹å‘ä¸‹çš„å“åº”

from google.colab import drive
drive.mount('/content/drive')

import os
import numpy as np
import pandas as pd
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM
from tqdm.auto import tqdm
import torch
import time
import random

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
seed = 42
np.random.seed(seed)
random.seed(seed)
torch.manual_seed(seed)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(seed)

# âœ… STEP 2: ä¿å­˜è·¯å¾„
result_dir = "/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/v11_direction_sensitivity"
os.makedirs(result_dir, exist_ok=True)


# âœ… STEP 3: åŠ è½½å‰ 500 æ¡ UltraFeedback prompt
ds = load_dataset("HuggingFaceH4/ultrafeedback_binarized", split="test_prefs")
prompts = ds["prompt"][:100]
prompt_ids = list(range(100))

# âœ… STEP 4: åŠ è½½ DPA æ¨¡å‹å’Œ tokenizer
model = AutoModelForCausalLM.from_pretrained(
    "Haoxiang-Wang/DPA-v1-Mistral-7B",
    torch_dtype=torch.float16,
    device_map="auto"
).to(device)

tokenizer = AutoTokenizer.from_pretrained("Haoxiang-Wang/RewardModel-Mistral-7B-for-DPA-v1")
tokenizer.padding_side = "left"
if tokenizer.pad_token_id is None:
    tokenizer.pad_token = tokenizer.eos_token

# âœ… STEP 5: æ„é€  DPA Prompt
def build_input(prompt, v1, v2):
    h = int(np.round(v1 * 100))
    v = int(np.round(v2 * 100))
    sys_instruction = f"You are a helpful assistant. Your response should maximize weighted rating = helpfulness*{h} + verbosity*{v}."
    return [{"role": "user", "content": f"{sys_instruction}\n\n{prompt}"}]

# âœ… STEP 6: æ‰¹é‡ç”Ÿæˆ response
def generate_response_batch(prompts_batch, prompt_ids_batch, v1, v2):
    input_ids_list = []

    for prompt in prompts_batch:
        messages = build_input(prompt, v1, v2)
        input_ids = tokenizer.apply_chat_template(
            messages, add_generation_prompt=True, return_tensors="pt"
        )[0]
        input_ids_list.append(input_ids.to(device))

    input_ids_padded = torch.nn.utils.rnn.pad_sequence(
        input_ids_list, batch_first=True, padding_value=tokenizer.pad_token_id
    ).to(device)

    attention_mask = (input_ids_padded != tokenizer.pad_token_id).to(device)
    max_input_len = input_ids_padded.shape[1]
    max_new_tokens = min(2048, 4096 - max_input_len)

    with torch.no_grad():
        outputs = model.generate(
            input_ids=input_ids_padded,
            attention_mask=attention_mask,
            max_new_tokens=max_new_tokens,
            temperature=0.7,
            do_sample=True,
            num_return_sequences=1,
            pad_token_id=tokenizer.eos_token_id
        )

    responses = []
    for i, input_ids in enumerate(input_ids_list):
        generated_tokens = outputs[i][input_ids.shape[0]:]
        decoded = tokenizer.decode(generated_tokens, skip_special_tokens=True)
        responses.append({
            "prompt_id": prompt_ids_batch[i],
            "prompt": prompts_batch[i],
            "response": decoded
        })
    return responses

# âœ… ä¸»æ–¹å‘ï¼ˆv10ï¼‰
main_v1 = 0.9500
main_v2 = -0.3100
main_angle = np.degrees(np.arctan2(main_v2, main_v1)) # âœ… è¡¥ä¸Šè¿™ä¸ªå®šä¹‰


# âœ… STEP 7: å®šä¹‰æ‰°åŠ¨æ–¹å‘ï¼ˆÂ±30Â°, Â±20Â°, Â±10Â°, 0Â°ï¼‰
# ä¸Šé¢ valid_vs å’Œ valid_angles å·²å‡†å¤‡å¥½
# âœ… STEP 7: å®šä¹‰æ‰°åŠ¨æ–¹å‘ï¼ˆÂ±30Â°, Â±20Â°, Â±10Â°, 0Â°, +10Â°, +20Â°, +30Â°ï¼‰
valid_vs = [
    np.array([0.66819203, -0.74398885]),  # -30Â°
    np.array([0.78723300, -0.61665566]),  # -20Â°
    np.array([0.88235429, -0.47058570]),  # -10Â°
    np.array([0.95066570, -0.31021723]),  # 0Â° ä¸»æ–¹å‘
    np.array([0.99009161, -0.14042297]),  # +10Â°
    np.array([0.99943408, +0.03363798]),  # +20Â°
    np.array([0.97840926, +0.20667685])   # +30Â°
]

valid_angles = [
    np.float64(-48.07232214895949),
    np.float64(-38.0723221489595),
    np.float64(-28.072322148959497),
    np.float64(-18.072322148959497),
    np.float64(-8.072322148959499),
    np.float64(1.9276778510405013),
    np.float64(11.9276778510405)
]



# âœ… STEP 8: ç”Ÿæˆå“åº”
batch_size = 8
for i, (v_vec, angle_deg) in enumerate(zip(valid_vs, valid_angles)):
    v1, v2 = v_vec[0], v_vec[1]
    output_file = os.path.join(result_dir, f"v10_angle_{int(angle_deg)}.csv")
    print(f"\nğŸš€ Generating for direction {i}: angle = {angle_deg}Â°, v = ({v1:.4f}, {v2:.4f})")

    if os.path.exists(output_file):
        existing_df = pd.read_csv(output_file)
        done_prompt_ids = set(existing_df["prompt_id"].unique())
        results = existing_df.to_dict("records")
        print(f"ğŸ” Resuming from previous run: {len(done_prompt_ids)} prompts already completed.")
    else:
        done_prompt_ids = set()
        results = []

    for start in tqdm(range(0, len(prompts), batch_size), desc=f"Generating angle {angle_deg}Â°"):
        end = min(start + batch_size, len(prompts))
        batch_prompts_all = prompts[start:end]
        batch_ids_all = prompt_ids[start:end]

        unprocessed_indices = [j for j, pid in enumerate(batch_ids_all) if pid not in done_prompt_ids]
        if not unprocessed_indices:
            continue

        batch_prompts = [batch_prompts_all[j] for j in unprocessed_indices]
        batch_ids = [batch_ids_all[j] for j in unprocessed_indices]

        try:
            batch_outputs = generate_response_batch(batch_prompts, batch_ids, v1, v2)
            for item in batch_outputs:
                item.update({
                    "v1_p": round(v1, 4),
                    "v2_p": round(v2, 4),
                    "direction_index": i,
                    "angle_deg": round(angle_deg, 1),
                    "main_v1": round(np.cos(np.radians(main_angle)), 4),
                    "main_v2": round(np.sin(np.radians(main_angle)), 4)
                })
                results.append(item)

            pd.DataFrame(batch_outputs).to_csv(
                output_file, mode='a', index=False,
                header=not os.path.exists(output_file)
            )

        except Exception as e:
            print(f"âš ï¸ Error at batch {start}-{end}: {e}")

    print(f"âœ… Final saved {len(results)} responses to {output_file}")

import os
import pandas as pd
import numpy as np
import torch
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForCausalLM
from accelerate import Accelerator

# === æ¨¡å‹å‡†å¤‡ ===
device = "cuda" if torch.cuda.is_available() else "cpu"
accelerator = Accelerator()

model = AutoModelForCausalLM.from_pretrained(
    "Haoxiang-Wang/DPA-v1-Mistral-7B",
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
    device_map="auto" if torch.cuda.is_available() else None
)
model = accelerator.prepare(model)

tokenizer = AutoTokenizer.from_pretrained("Haoxiang-Wang/DPA-v1-Mistral-7B")
tokenizer.padding_side = "left"
if tokenizer.pad_token_id is None:
    tokenizer.pad_token = tokenizer.eos_token

# === åˆ¤æ–­æ˜¯å¦æ— æ•ˆ response ===
def is_invalid_response(resp):
    if pd.isna(resp):
        return True
    resp = str(resp).strip()
    if not resp:
        return True

    invalid_keywords = [
        "[A:]", "[/SOLUTION]", "[ASS]", "[A]:", "[/CONV]", "[/CODE]", "[/USER]",
        "[OUTPUT]", "[CC]", "[OUT]", "[/DATA]", "[/ASST]", "[/ME]", "[/REST]",
        "[RESP]", "<[user]>", "<|user|>", "[Response]", "[Assistant]", "[Answer]",
        "[End of Response]", "<human>", "<|endoftext|>", "<<", "[INST]", "[/INST]",
        "[]", "[USER]"
    ]
    tokens = resp.split()
    if all(token.strip() in invalid_keywords for token in tokens):
        return True
    tag_like_count = sum(1 for t in tokens if (t.startswith("[") and t.endswith("]")) or (t.startswith("<") and t.endswith(">")))
    if len(tokens) > 0 and tag_like_count / len(tokens) > 0.8:
        return True
    return False

# === æ„é€ è¾“å…¥ prompt ===
def build_input(prompt, v1, v2):
    h = int(np.round(v1 * 100))
    v = int(np.round(v2 * 100))
    sys_instruction = f"You are a helpful assistant. Your response should maximize weighted rating = helpfulness*{h} + verbosity*{v}."
    return [{"role": "system", "content": sys_instruction}, {"role": "user", "content": prompt}]

# === ç”¨æ¨¡å‹é‡æ–°ç”Ÿæˆ response ===
def regenerate_response(prompt, v1, v2):
    try:
        messages = build_input(prompt, v1, v2)
        input_ids = tokenizer.apply_chat_template(
            messages, add_generation_prompt=True, return_tensors="pt"
        ).to(device)

        if input_ids.ndim != 2 or input_ids.shape[1] == 0:
            print(f"âš ï¸ Invalid input_ids shape {input_ids.shape}, skipping: {prompt[:60]}")
            return ""

        outputs = model.generate(
            input_ids=input_ids,
            temperature=0.7,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
        generated = outputs[0][input_ids.shape[1]:]
        return tokenizer.decode(generated, skip_special_tokens=True).strip()

    except Exception as e:
        print(f"âŒ Error: {e} \nPrompt: {prompt[:60]}")
        return ""

# === ä¿®å¤å•ä¸ª CSV æ–‡ä»¶ ===
def fix_csv_responses(csv_path, save_dir):
    print(f"\nğŸ“‚ æ­£åœ¨æ£€æŸ¥: {csv_path}")

    if not os.path.exists(csv_path):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {csv_path}")
        return

    df = pd.read_csv(csv_path)
    if "v1_p" not in df.columns or "v2_p" not in df.columns:
        print(f"âŒ ç¼ºå°‘ v1_p/v2_p åˆ—ï¼Œè·³è¿‡: {csv_path}")
        return

    bad_mask = df["response"].apply(is_invalid_response)
    num_bad = bad_mask.sum()

    if num_bad == 0:
        print("âœ… æ— éœ€ä¿®å¤ï¼Œè·³è¿‡ã€‚")
        return

    print(f"ğŸ”§ å‘ç° {num_bad} æ¡æ— æ•ˆ responseï¼Œå¼€å§‹é‡ç”Ÿæˆ...")
    for i in tqdm(bad_mask[bad_mask].index, total=num_bad, desc=os.path.basename(csv_path), dynamic_ncols=True):
        row = df.loc[i]
        new_response = regenerate_response(row["prompt"], row["v1_p"], row["v2_p"])
        df.at[i, "response"] = new_response

    os.makedirs(save_dir, exist_ok=True)
    file_name = os.path.basename(csv_path).replace(".csv", "_fixed.csv")
    save_path = os.path.join(save_dir, file_name)
    df.to_csv(save_path, index=False)
    print(f"âœ… ä¿®å¤å®Œæˆ â†’ å·²ä¿å­˜: {save_path}")

# === ä¸»é€»è¾‘ï¼šä¿®å¤æ‰€æœ‰ CSV ===
input_dir = "drive/MyDrive/llm_pre_project/dpa_outputs_v9/v11_direction_sensitivity"
output_dir = os.path.join(input_dir, "fixed")

for file in os.listdir(input_dir):
    if file.endswith(".csv") and not file.endswith("_fixed.csv"):
        fix_csv_responses(os.path.join(input_dir, file), output_dir)

import pandas as pd
import os

# ç»“æœç›®å½•
result_dir = "/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/v11_direction_sensitivity/fixed"
merged_path = os.path.join(result_dir, "merged_all_angles.csv")

# åˆå¹¶æ‰€æœ‰ CSV æ–‡ä»¶
all_data = []
for fname in os.listdir(result_dir):
    if fname.endswith(".csv") and not fname.startswith(".") and "merged" not in fname:
        fpath = os.path.join(result_dir, fname)
        try:
            df = pd.read_csv(fpath)
            all_data.append(df)
            print(f"ğŸ“„ Loaded {fname} with {len(df)} rows.")
        except Exception as e:
            print(f"âš ï¸ Failed to load {fname}: {e}")

if all_data:
    df_all = pd.concat(all_data, ignore_index=True)
    df_all.to_csv(merged_path, index=False)
    print(f"\nâœ… Merged total {len(df_all)} responses.")
    print(f"ğŸ“ Saved to: {merged_path}")
else:
    print("âŒ No valid CSV files found to merge.")

import os
import time
import pandas as pd
import numpy as np
import torch
from tqdm.auto import tqdm
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from google.colab import drive

# === æŒ‚è½½ Google Drive ===
drive.mount('/content/drive')

# === è®¾ç½®è·¯å¾„ ===
input_path = "/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/v11_direction_sensitivity/fixed/merged_all_angles.csv"
output_path = "/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/v11_direction_sensitivity/merged_all_angles_scored.csv"

# === åŠ è½½åˆå¹¶æ•°æ® ===
df = pd.read_csv(input_path)
print(f"ğŸ“¦ Loaded {len(df)} samples")

# === åŠ è½½ Reward Model ===
print("ğŸ¤– Loading Reward Model...")
device = "cuda" if torch.cuda.is_available() else "cpu"
rm = AutoModelForSequenceClassification.from_pretrained(
    "Haoxiang-Wang/RewardModel-Mistral-7B-for-DPA-v1", trust_remote_code=True
).to(device)
tokenizer = AutoTokenizer.from_pretrained("Haoxiang-Wang/RewardModel-Mistral-7B-for-DPA-v1")

def score_response(prompt, response):
    template = "[INST] You must read the following conversation carefully and rate the assistant's response from score 0-100 in these aspects: helpfulness, correctness, coherence, honesty, complexity, verbosity\n\nUser: {prompt}\n\nAssistant: {response} [/INST]"
    inputs = tokenizer(template.format(prompt=prompt, response=response), return_tensors="pt").to(device)
    with torch.no_grad():
        logits = rm(**inputs).logits.squeeze().cpu().numpy()
    return logits[9], logits[4]  # helpfulness, verbosity

# === æ‰¹é‡æ‰“åˆ† ===
help_scores, verb_scores = [], []

print("ğŸ§  Scoring all responses...")
for i, row in tqdm(df.iterrows(), total=len(df)):
    try:
        h, v = score_response(row["prompt"], row["response"])
    except Exception as e:
        print(f"âš ï¸ Error on row {i}: {e}")
        h, v = None, None
    help_scores.append(h)
    verb_scores.append(v)

df["reward_score_helpfulness"] = help_scores
df["reward_score_verbosity"] = verb_scores

# === ä¿å­˜æ‰“åˆ†ç»“æœ ===
df.to_csv(output_path, index=False)
print(f"âœ… All scored results saved to {output_path}")

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# === åŠ è½½æ‰“åˆ†åçš„ CSV ===
input_path = "/content/drive/MyDrive/llm_pre_project/dpa_outputs_v9/v11_direction_sensitivity/merged_all_angles_scored.csv"
df = pd.read_csv(input_path)

# === èšåˆä¸åŒæ–¹å‘çš„å¹³å‡åˆ† ===
df_grouped = df.groupby("angle_deg")[["reward_score_helpfulness", "reward_score_verbosity"]].agg(["mean", "std"]).reset_index()
df_grouped.columns = ["angle", "help_mean", "help_std", "verb_mean", "verb_std"]

# === å¯è§†åŒ–ï¼šhelpfulness å’Œ verbosity éšè§’åº¦å˜åŒ– ===
plt.figure(figsize=(10, 6))
plt.errorbar(df_grouped["angle"], df_grouped["help_mean"], yerr=df_grouped["help_std"], label="Helpfulness", fmt='-o')
plt.errorbar(df_grouped["angle"], df_grouped["verb_mean"], yerr=df_grouped["verb_std"], label="Verbosity", fmt='-o', color='orange')
plt.axvline(x=-18.072322148959497, color='gray', linestyle='--', label="Main Direction")
plt.xlabel("Angle (degrees)")
plt.ylabel("Reward Score")
plt.title("Reward Score vs Direction Angle (Â±pertuation)")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# è§’åº¦è®¾ç½®ï¼š0Â° åˆ° 45Â°
angles_deg = np.arange(0, 46, 5)
angles_rad = np.radians(angles_deg)

# è®¡ç®—å•ä½æ–¹å‘å‘é‡åæ ‡
x = np.cos(angles_rad)
y = np.sin(angles_rad)

# åˆ›å»ºå›¾å½¢
fig, ax = plt.subplots(figsize=(6, 6))
ax.set_aspect('equal')

# ç»˜åˆ¶å•ä½åœ†
theta_full = np.linspace(0, 2 * np.pi, 500)
circle_x = np.cos(theta_full)
circle_y = np.sin(theta_full)
ax.plot(circle_x, circle_y, color='lightgray', linestyle='--', linewidth=1)

# ç»˜åˆ¶æ–¹å‘å‘é‡å’Œè§’åº¦æ ‡æ³¨
for i in range(len(x)):
    ax.arrow(0, 0, x[i], y[i],
             head_width=0.03, head_length=0.05,
             fc='black', ec='black', length_includes_head=True)
    ax.text(x[i] * 1.15, y[i] * 1.15,
            f'{angles_deg[i]}Â°',
            ha='center', va='center', fontsize=10)

# è®¾ç½®åæ ‡è½´èŒƒå›´å’Œæ ‡ç­¾
ax.set_xlim(-1.1, 1.1)
ax.set_ylim(-1.1, 1.1)
ax.set_xlabel(r'$V_{\text{helpfulness}}$', fontsize=14)
ax.set_ylabel(r'$V_{\text{verbosity}}$', fontsize=14)
ax.set_title('Preference Directions on the Unit Circle', fontsize=13)
ax.axhline(0, color='gray', linewidth=0.5)
ax.axvline(0, color='gray', linewidth=0.5)
ax.grid(True, linestyle=':', linewidth=0.5)
ax.set_xticks([])
ax.set_yticks([])

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# é‡‡æ ·è§’åº¦ï¼š-45Â° åˆ° 0Â°
angles_deg = np.linspace(-45, 0, 30)  # å¯è°ƒé‡‡æ ·å¯†åº¦
angles_rad = np.radians(angles_deg)

# å•ä½å‘é‡åæ ‡
x = np.cos(angles_rad)
y = np.sin(angles_rad)

# åˆ›å»ºå›¾å½¢
fig, ax = plt.subplots(figsize=(6, 6))
ax.set_aspect('equal')

# ç»˜åˆ¶å•ä½åœ†
theta_full = np.linspace(0, 2 * np.pi, 500)
circle_x = np.cos(theta_full)
circle_y = np.sin(theta_full)
ax.plot(circle_x, circle_y, color='lightgray', linestyle='--', linewidth=1)

# ç»˜åˆ¶è®­ç»ƒæ–¹å‘å‘é‡ (v1, v2)
for i in range(len(x)):
    ax.arrow(0, 0, x[i], y[i],
             head_width=0.025, head_length=0.04,
             fc='darkblue', ec='darkblue', alpha=0.7,
             length_includes_head=True)

# å¯è§†åŒ–é‡‡æ ·æ‰‡å½¢åŒºåŸŸè¾¹ç•Œ
arc_angles = np.radians(np.linspace(-45, 0, 100))
arc_x = np.cos(arc_angles)
arc_y = np.sin(arc_angles)
ax.plot(arc_x, arc_y, color='blue', linestyle=':', linewidth=1)
ax.fill_between(arc_x, arc_y, 0, where=(arc_y <= 0), color='blue', alpha=0.05)

# æ ‡æ³¨
ax.text(0.8, -0.1, r'$v = (1, 0)$', fontsize=10)
ax.text(0.45, -0.55, r'$v = \left(\frac{\sqrt{2}}{2}, -\frac{\sqrt{2}}{2}\right)$', fontsize=10)
ax.text(0.3, -0.3, r'DPA training range', fontsize=11, color='blue')

# åæ ‡è½´è®¾ç½®
ax.set_xlim(-1.05, 1.05)
ax.set_ylim(-1.05, 1.05)
ax.axhline(0, color='gray', linewidth=0.5)
ax.axvline(0, color='gray', linewidth=0.5)
ax.set_xlabel(r'$V_{\text{helpfulness}}$', fontsize=14)
ax.set_ylabel(r'$V_{\text{verbosity}}$', fontsize=14)
ax.set_title('Training-Time Direction Sampling in DPA', fontsize=13)
ax.grid(True, linestyle=':', linewidth=0.5)
ax.set_xticks([])
ax.set_yticks([])

plt.tight_layout()
plt.show()





























