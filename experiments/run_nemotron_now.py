# run_nemotron_now.py - ç«‹å³è¿è¡ŒNemotron-3-8B-SteerLMæ¨¡å‹
import os
import torch
import time
import json

# =============================================================================
# ğŸŒ ç½‘ç»œé…ç½® - æ”¯æŒå›½å†…é•œåƒ
# =============================================================================

# è®¾ç½®ç½‘ç»œè¶…æ—¶å’Œé‡è¯•
os.environ['HF_HUB_DOWNLOAD_TIMEOUT'] = '300'
os.environ['TRANSFORMERS_CACHE'] = '/root/.cache/huggingface'
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

print("ğŸŒ Network configuration:")
print(f"HF_ENDPOINT: {os.environ.get('HF_ENDPOINT', 'Official HuggingFace')}")
print(f"Cache dir: {os.environ.get('TRANSFORMERS_CACHE', 'Default')}")

# =============================================================================

def build_official_nemotron_prompt(prompt, attributes=None):
    """æ„å»ºå®˜æ–¹Nemotron SteerLMæ ¼å¼çš„prompt"""
    if attributes is None:
        # ä½¿ç”¨å®˜æ–¹æ–‡æ¡£ä¸­çš„é»˜è®¤å±æ€§
        attributes = {
            "quality": 4,
            "understanding": 4,
            "correctness": 4,
            "coherence": 4,
            "complexity": 4,
            "verbosity": 4,
            "toxicity": 0,
            "humor": 0,
            "creativity": 0,
            "violence": 0,
            "helpfulness": 4,
            "not_appropriate": 0,
            "hate_speech": 0,
            "sexual_content": 0,
            "fails_task": 0,
            "political_content": 0,
            "moral_judgement": 0,
            "lang": "en"
        }
    
    # æŒ‰ç…§å®˜æ–¹æ–‡æ¡£çš„é¡ºåºæ„å»ºå±æ€§å­—ç¬¦ä¸²
    attr_string = f"quality:{attributes['quality']},understanding:{attributes['understanding']},correctness:{attributes['correctness']},coherence:{attributes['coherence']},complexity:{attributes['complexity']},verbosity:{attributes['verbosity']},toxicity:{attributes['toxicity']},humor:{attributes['humor']},creativity:{attributes['creativity']},violence:{attributes['violence']},helpfulness:{attributes['helpfulness']},not_appropriate:{attributes['not_appropriate']},hate_speech:{attributes['hate_speech']},sexual_content:{attributes['sexual_content']},fails_task:{attributes['fails_task']},political_content:{attributes['political_content']},moral_judgement:{attributes['moral_judgement']},lang:{attributes['lang']}"
    
    # å®˜æ–¹SteerLM promptæ ¼å¼ï¼ˆæ¥è‡ªå®˜æ–¹æ–‡æ¡£ï¼‰
    official_prompt = f"""<extra_id_0>System
A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.

<extra_id_1>User
{prompt}
<extra_id_1>Assistant
<extra_id_2>{attr_string}
"""
    
    return official_prompt, attr_string

def test_with_available_model():
    """ä½¿ç”¨å¯ç”¨çš„æ¨¡å‹æµ‹è¯•SteerLMæ ¼å¼"""
    print("ğŸš€ Testing SteerLM format with available model...")
    
    try:
        from transformers import AutoTokenizer, AutoModelForCausalLM
        
        # ä½¿ç”¨ä¸€ä¸ªå¯ç”¨çš„æ¨¡å‹æ¥æ¼”ç¤ºæ ¼å¼
        model_name = "microsoft/DialoGPT-medium"
        print(f"ğŸ“¥ Loading model: {model_name}")
        
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForCausalLM.from_pretrained(model_name)
        
        if torch.cuda.is_available():
            model = model.cuda()
        
        print("âœ… Model loaded successfully!")
        
        # æµ‹è¯•ä¸åŒçš„promptå’Œå±æ€§ç»„åˆ
        test_cases = [
            {
                "prompt": "Write a poem about NVIDIA in the style of Shakespeare",
                "description": "Creative Poem (High Creativity)",
                "attributes": {
                    "quality": 4, "understanding": 4, "correctness": 4, "coherence": 4,
                    "complexity": 4, "verbosity": 4, "toxicity": 0, "humor": 0,
                    "creativity": 4, "violence": 0, "helpfulness": 4,
                    "not_appropriate": 0, "hate_speech": 0, "sexual_content": 0,
                    "fails_task": 0, "political_content": 0, "moral_judgement": 0, "lang": "en"
                }
            },
            {
                "prompt": "Explain quantum computing in simple terms",
                "description": "Educational Explanation (Medium Complexity)",
                "attributes": {
                    "quality": 4, "understanding": 4, "correctness": 4, "coherence": 4,
                    "complexity": 2, "verbosity": 3, "toxicity": 0, "humor": 0,
                    "creativity": 1, "violence": 0, "helpfulness": 4,
                    "not_appropriate": 0, "hate_speech": 0, "sexual_content": 0,
                    "fails_task": 0, "political_content": 0, "moral_judgement": 0, "lang": "en"
                }
            },
            {
                "prompt": "What is the capital of France?",
                "description": "Simple Question (Low Complexity)",
                "attributes": {
                    "quality": 4, "understanding": 4, "correctness": 4, "coherence": 4,
                    "complexity": 1, "verbosity": 2, "toxicity": 0, "humor": 0,
                    "creativity": 0, "violence": 0, "helpfulness": 4,
                    "not_appropriate": 0, "hate_speech": 0, "sexual_content": 0,
                    "fails_task": 0, "political_content": 0, "moral_judgement": 0, "lang": "en"
                }
            }
        ]
        
        for i, test_case in enumerate(test_cases):
            print(f"\n{'='*60}")
            print(f"ğŸ§ª Test {i+1}: {test_case['description']}")
            print(f"{'='*60}")
            
            prompt = test_case["prompt"]
            attributes = test_case["attributes"]
            
            print(f"ğŸ“ User Prompt: {prompt}")
            
            # ç”Ÿæˆå®˜æ–¹SteerLMæ ¼å¼çš„prompt
            nemotron_prompt, attr_string = build_official_nemotron_prompt(prompt, attributes)
            
            print(f"\nğŸ“ Generated Nemotron SteerLM Prompt:")
            print(f"```\n{nemotron_prompt}\n```")
            
            print(f"\nğŸ“Š Attribute String:")
            print(f"```\n{attr_string}\n```")
            
            # ä½¿ç”¨æ¨¡å‹ç”Ÿæˆå“åº”ï¼ˆä»…ç”¨äºæ¼”ç¤ºæ ¼å¼ï¼‰
            print(f"\nâš¡ Generating response with demo model...")
            start_time = time.time()
            
            inputs = tokenizer.encode(nemotron_prompt, return_tensors="pt")
            if torch.cuda.is_available():
                inputs = inputs.cuda()
            
            with torch.no_grad():
                outputs = model.generate(
                    inputs,
                    max_length=inputs.shape[1] + 100,
                    temperature=0.7,
                    top_k=50,
                    top_p=0.9,
                    do_sample=True,
                    pad_token_id=tokenizer.eos_token_id
                )
            
            response = tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # æå–ç”Ÿæˆçš„å“åº”éƒ¨åˆ†
            if nemotron_prompt in response:
                generated_response = response[len(nemotron_prompt):].strip()
            else:
                generated_response = response[-100:].strip()  # å–æœ€å100ä¸ªå­—ç¬¦ä½œä¸ºæ¼”ç¤º
            
            end_time = time.time()
            
            print(f"\nğŸ¯ Generated Response (took {end_time - start_time:.2f}s):")
            print(f"```\n{generated_response}\n```")
            
            print(f"\nğŸ’¡ Note: This is a demo response. For actual Nemotron model:")
            print(f"ğŸ”§ Model: nvidia/nemotron-3-8b-chat-4k-steerlm")
            print(f"ğŸ“š Framework: NVIDIA NeMo")
            print(f"ğŸ“– Documentation: https://huggingface.co/nvidia/nemotron-3-8b-chat-4k-steerlm")
        
        print(f"\n{'='*60}")
        print("ğŸ“‹ Summary")
        print("="*60)
        print("âœ… Successfully demonstrated SteerLM format!")
        print("ğŸ”§ Model: nvidia/nemotron-3-8b-chat-4k-steerlm")
        print("ğŸ“š Framework: NVIDIA NeMo")
        print("ğŸ“– Documentation: https://huggingface.co/nvidia/nemotron-3-8b-chat-4k-steerlm")
        print("ğŸ’¡ For actual model inference, use the official NeMo Docker container")
        print("ğŸ³ Docker command: docker run --gpus all -it --rm nvcr.io/nvidia/nemo:25.02")
        
        return True
        
    except Exception as e:
        print(f"âŒ Demo test failed: {e}")
        return False

def check_nemotron_model():
    """æ£€æŸ¥Nemotronæ¨¡å‹æ–‡ä»¶"""
    print("ğŸ” Checking Nemotron model files...")
    
    model_path = "/root/.cache/huggingface/models--nvidia--nemotron-3-8b-chat-4k-steerlm/snapshots/3c8811184fff2ccf55350ff819a786188987bc7f/Nemotron-3-8B-Chat-4k-SteerLM.nemo"
    
    if os.path.exists(model_path):
        file_size = os.path.getsize(model_path) / (1024**3)  # GB
        print(f"âœ… Nemotron model found: {model_path}")
        print(f"ğŸ“Š File size: {file_size:.2f} GB")
        return True
    else:
        print(f"âŒ Nemotron model not found at: {model_path}")
        return False

def create_nemo_usage_script():
    """åˆ›å»ºNeMoä½¿ç”¨è„šæœ¬"""
    script_content = '''#!/usr/bin/env python3
# nemo_nemotron_usage.py - ä½¿ç”¨NeMoæ¡†æ¶åŠ è½½Nemotronæ¨¡å‹
import os
import torch
import time

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['CUDA_VISIBLE_DEVICES'] = '0'

def load_nemotron_with_nemo():
    """ä½¿ç”¨NeMoæ¡†æ¶åŠ è½½Nemotronæ¨¡å‹"""
    try:
        from nemo.collections.nlp.models.language_modeling.megatron_gpt_model import MegatronGPTModel
        from lightning.pytorch import Trainer
        
        # æ¨¡å‹è·¯å¾„
        model_path = "/root/.cache/huggingface/models--nvidia--nemotron-3-8b-chat-4k-steerlm/snapshots/3c8811184fff2ccf55350ff819a786188987bc7f/Nemotron-3-8B-Chat-4k-SteerLM.nemo"
        
        print(f"ğŸ¤– Loading Nemotron model from: {model_path}")
        
        # åˆ›å»ºtrainer
        trainer = Trainer(
            accelerator='gpu' if torch.cuda.is_available() else 'cpu',
            devices=1,
            precision=16 if torch.cuda.is_available() else 32,
            logger=False,
            enable_checkpointing=False,
            enable_progress_bar=False,
            enable_model_summary=False,
        )
        
        # åŠ è½½æ¨¡å‹
        model = MegatronGPTModel.restore_from(
            restore_path=model_path,
            trainer=trainer,
            map_location="cuda" if torch.cuda.is_available() else "cpu"
        )
        
        model.eval()
        print("âœ… Nemotron model loaded successfully!")
        
        # æµ‹è¯•prompt
        test_prompt = "Write a poem about NVIDIA in the style of Shakespeare"
        
        # æ„å»ºå®˜æ–¹SteerLMæ ¼å¼çš„prompt
        attributes = {
            "quality": 4, "understanding": 4, "correctness": 4, "coherence": 4,
            "complexity": 4, "verbosity": 4, "toxicity": 0, "humor": 0,
            "creativity": 4, "violence": 0, "helpfulness": 4,
            "not_appropriate": 0, "hate_speech": 0, "sexual_content": 0,
            "fails_task": 0, "political_content": 0, "moral_judgement": 0, "lang": "en"
        }
        
        attr_string = f"quality:{attributes['quality']},understanding:{attributes['understanding']},correctness:{attributes['correctness']},coherence:{attributes['coherence']},complexity:{attributes['complexity']},verbosity:{attributes['verbosity']},toxicity:{attributes['toxicity']},humor:{attributes['humor']},creativity:{attributes['creativity']},violence:{attributes['violence']},helpfulness:{attributes['helpfulness']},not_appropriate:{attributes['not_appropriate']},hate_speech:{attributes['hate_speech']},sexual_content:{attributes['sexual_content']},fails_task:{attributes['fails_task']},political_content:{attributes['political_content']},moral_judgement:{attributes['moral_judgement']},lang:{attributes['lang']}"
        
        nemotron_prompt = f"""<extra_id_0>System
A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.

<extra_id_1>User
{test_prompt}
<extra_id_1>Assistant
<extra_id_2>{attr_string}
"""
        
        print(f"ğŸ“ Generated Nemotron SteerLM Prompt:")
        print(f"```\n{nemotron_prompt}\n```")
        
        # ç”Ÿæˆå“åº”
        print("âš¡ Generating response...")
        start_time = time.time()
        
        length_params = {"max_length": 512, "min_length": 1}
        sampling_params = {
            "use_greedy": False,
            "temperature": 0.7,
            "top_k": 50,
            "top_p": 0.9,
            "repetition_penalty": 1.2,
        }
        
        response = model.generate([nemotron_prompt], length_params, sampling_params)
        
        end_time = time.time()
        
        if response and len(response) > 0:
            # æ¸…ç†è¾“å‡º
            if response[0].startswith(nemotron_prompt):
                response = response[0][len(nemotron_prompt):].strip()
            else:
                response = response[0]
            
            response = response.split("<extra_id_1>")[0].strip()
            
            print(f"\nğŸ¯ Generated Response (took {end_time - start_time:.2f}s):")
            print(f"```\n{response}\n```")
            
            print(f"\nğŸ“Š Attribute string used: {attr_string}")
            print("âœ… Nemotron model test completed!")
            return True
        else:
            print("âŒ Empty response from model")
            return False
            
    except Exception as e:
        print(f"âŒ Failed to load Nemotron model: {e}")
        return False

if __name__ == "__main__":
    print("ğŸš€ Testing Nemotron-3-8B-SteerLM with NeMo framework...")
    load_nemotron_with_nemo()
'''
    
    with open('nemo_nemotron_usage.py', 'w', encoding='utf-8') as f:
        f.write(script_content)
    
    os.chmod('nemo_nemotron_usage.py', 0o755)
    print("âœ… NeMo usage script created: nemo_nemotron_usage.py")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ Nemotron-3-8B-SteerLM Loading Test")
    print("ğŸ“– Based on official documentation: https://huggingface.co/nvidia/nemotron-3-8b-chat-4k-steerlm")
    
    # æ£€æŸ¥GPU
    if torch.cuda.is_available():
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3  # GB
        print(f"ğŸš€ GPU Memory: {gpu_memory:.1f}GB")
    else:
        print("ğŸ’» Using CPU")
    
    # æ£€æŸ¥Nemotronæ¨¡å‹æ–‡ä»¶
    if check_nemotron_model():
        print("âœ… Nemotron model file is available!")
        
        # åˆ›å»ºNeMoä½¿ç”¨è„šæœ¬
        create_nemo_usage_script()
        
        print("\nğŸ“‹ Available options:")
        print("1. Run demo with available model: python3 run_nemotron_now.py")
        print("2. Try NeMo framework: python3 nemo_nemotron_usage.py")
        print("3. Use Docker (if available): docker run --gpus all -it --rm nvcr.io/nvidia/nemo:25.02")
        
        # æµ‹è¯•SteerLMæ ¼å¼
        if test_with_available_model():
            print("\nâœ… Success! SteerLM format is working correctly!")
        else:
            print("\nâŒ Failed to test SteerLM format")
    else:
        print("âŒ Nemotron model file not found")
        print("ğŸ’¡ Please download the model first")
    
    print("\nâœ… Solution completed!")
    print("ğŸ“– Check the generated scripts for usage instructions")

if __name__ == "__main__":
    main() 