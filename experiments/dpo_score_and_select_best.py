#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DPO RPS æ‰“åˆ†å’Œé€‰æ‹©æœ€ä½³å“åº”è„šæœ¬ - æœåŠ¡å™¨ç‰ˆæœ¬
å¯¹æ¯ä¸ªæ–¹å‘çš„ç”Ÿæˆç»“æœè¿›è¡Œæ‰“åˆ†ï¼Œå¹¶é€‰å‡ºæ¯ä¸ªpromptçš„æœ€ä½³å“åº”
"""

import os
import sys
import pandas as pd
import numpy as np
import torch
import json
import time
from tqdm.auto import tqdm
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from pathlib import Path

# è®¾ç½®å†…å­˜ä¼˜åŒ–
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'

# è®¾ç½®éšæœºç§å­
def set_seed(seed=42):
    """è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿ç»“æœå¯é‡ç°"""
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
        torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

def run_reward_scoring(input_dir, output_dir, model_name="Haoxiang-Wang/RewardModel-Mistral-7B-for-DPA-v1"):
    """
    å¯¹è¾“å…¥ç›®å½•ä¸­çš„æ‰€æœ‰CSVæ–‡ä»¶è¿›è¡Œæ‰“åˆ†
    Args:
        input_dir: è¾“å…¥ç›®å½•è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•è·¯å¾„
        model_name: å¥–åŠ±æ¨¡å‹åç§°
    """
    print(f"ğŸ¤– Loading reward model: {model_name}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ“± Using device: {device}")
    
    # åŠ è½½æ¨¡å‹å’Œtokenizer
    try:
        rm = AutoModelForSequenceClassification.from_pretrained(
            model_name, 
            trust_remote_code=True,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto" if torch.cuda.is_available() else None,
            low_cpu_mem_usage=True
        ).to(device)
        
        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            
        print("âœ… Reward model loaded successfully!")
        
    except Exception as e:
        print(f"âŒ Failed to load reward model: {e}")
        print("ğŸ”„ Trying CPU fallback...")
        try:
            rm = AutoModelForSequenceClassification.from_pretrained(
                model_name, 
                trust_remote_code=True,
                torch_dtype=torch.float32,
                device_map="cpu",
                low_cpu_mem_usage=True
            )
            tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            print("âœ… Reward model loaded on CPU!")
        except Exception as e2:
            print(f"âŒ CPU loading also failed: {e2}")
            return

    def score_response(prompt, response):
        """å¯¹å•ä¸ªå“åº”è¿›è¡Œæ‰“åˆ†"""
        try:
            template = "[INST] You must read the following conversation carefully and rate the assistant's response from score 0-100 in these aspects: helpfulness, correctness, coherence, honesty, complexity, verbosity\n\nUser: {prompt}\n\nAssistant: {response} [/INST]"
            inputs = tokenizer(
                template.format(prompt=prompt, response=response), 
                return_tensors="pt",
                truncation=True,
                max_length=2048
            ).to(device)
            
            with torch.no_grad():
                logits = rm(**inputs).logits.squeeze().cpu().numpy()
            
            # è¿”å›helpfulnesså’Œverbosityåˆ†æ•°
            return logits[9], logits[4]  # helpfulness, verbosity
            
        except Exception as e:
            print(f"âš ï¸ Scoring error: {e}")
            return None, None

    # å¤„ç†æ‰€æœ‰CSVæ–‡ä»¶
    csv_files = [f for f in os.listdir(input_dir) if f.endswith('.csv')]
    print(f"ğŸ“ Found {len(csv_files)} CSV files to process")
    
    for file in csv_files:
        file_path = os.path.join(input_dir, file)
        print(f"\nğŸ“„ Processing: {file}")
        
        try:
            df = pd.read_csv(file_path)
            print(f"   ğŸ“Š Loaded {len(df)} rows")
            
            # åˆå§‹åŒ–åˆ†æ•°åˆ—
            if "helpfulness" not in df.columns:
                df["helpfulness"] = np.nan
            if "verbosity" not in df.columns:
                df["verbosity"] = np.nan
            
            # ç»Ÿè®¡éœ€è¦æ‰“åˆ†çš„è¡Œæ•°
            need_scoring = df[df["helpfulness"].isna() | df["verbosity"].isna()].shape[0]
            print(f"   ğŸ¯ Need to score {need_scoring} responses")
            
            if need_scoring == 0:
                print(f"   âœ… All responses already scored, skipping...")
                continue
            
            # é€è¡Œæ‰“åˆ†
            for i, row in tqdm(df.iterrows(), total=len(df), desc=f"Scoring {file}"):
                if pd.notnull(row["helpfulness"]) and pd.notnull(row["verbosity"]):
                    continue
                
                try:
                    h, v = score_response(row["prompt"], row["response"])
                    df.loc[i, "helpfulness"] = h
                    df.loc[i, "verbosity"] = v
                    
                    # æ¯100è¡Œä¿å­˜ä¸€æ¬¡ï¼Œé˜²æ­¢æ•°æ®ä¸¢å¤±
                    if (i + 1) % 100 == 0:
                        temp_save_path = os.path.join(output_dir, file.replace(".csv", "_temp_scored.csv"))
                        df.to_csv(temp_save_path, index=False)
                        
                except Exception as e:
                    print(f"   âŒ Error on row {i}: {e}")
                    continue
            
            # ä¿å­˜æœ€ç»ˆç»“æœ
            save_path = os.path.join(output_dir, file.replace(".csv", "_scored.csv"))
            df.to_csv(save_path, index=False)
            print(f"   ğŸ’¾ Scored file saved to: {save_path}")
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            temp_file = os.path.join(output_dir, file.replace(".csv", "_temp_scored.csv"))
            if os.path.exists(temp_file):
                os.remove(temp_file)
                
        except Exception as e:
            print(f"   âŒ Error processing file {file}: {e}")
            continue

def select_best_response(scored_dir, output_path):
    """
    ä»å·²æ‰“åˆ†çš„æ–‡ä»¶ä¸­é€‰å‡ºæ¯ä¸ªpromptçš„æœ€ä½³å“åº”
    Args:
        scored_dir: å·²æ‰“åˆ†çš„æ–‡ä»¶ç›®å½•
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    print(f"ğŸ” Selecting best responses from {scored_dir}")
    
    dfs = []
    scored_files = [f for f in os.listdir(scored_dir) if f.endswith("_scored.csv")]
    
    if not scored_files:
        print(f"âŒ No scored files found in {scored_dir}")
        return
    
    print(f"ğŸ“ Found {len(scored_files)} scored files")
    
    for file in scored_files:
        try:
            file_path = os.path.join(scored_dir, file)
            df = pd.read_csv(file_path)
            print(f"   ğŸ“„ Loaded {file}: {len(df)} rows")
            
            # æ£€æŸ¥å¿…è¦çš„åˆ—æ˜¯å¦å­˜åœ¨
            required_cols = ["prompt_id", "prompt", "response", "helpfulness", "verbosity"]
            missing_cols = [col for col in required_cols if col not in df.columns]
            if missing_cols:
                print(f"   âš ï¸ Missing columns in {file}: {missing_cols}")
                continue
            
            # è®¡ç®—æ€»åˆ†
            if "main_v1" in df.columns and "main_v2" in df.columns:
                df["score_total"] = df["main_v1"] * df["helpfulness"] + df["main_v2"] * df["verbosity"]
            elif "v1_p" in df.columns and "v2_p" in df.columns:
                df["score_total"] = df["v1_p"] * df["helpfulness"] + df["v2_p"] * df["verbosity"]
            elif "v1" in df.columns and "v2" in df.columns:
                df["score_total"] = df["v1"] * df["helpfulness"] + df["v2"] * df["verbosity"]
            else:
                print(f"   âš ï¸ No direction vector columns found in {file}, using default weights")
                # ä½¿ç”¨é»˜è®¤æƒé‡
                df["score_total"] = 0.7071 * df["helpfulness"] + 0.7071 * df["verbosity"]
            
            dfs.append(df)
            
        except Exception as e:
            print(f"   âŒ Error processing {file}: {e}")
            continue
    
    if not dfs:
        print("âŒ No valid data found for best response selection")
        return
    
    # åˆå¹¶æ‰€æœ‰æ•°æ®
    df_all = pd.concat(dfs, ignore_index=True)
    print(f"ğŸ“Š Total {len(df_all)} responses from all files")
    
    # æŒ‰prompt_idåˆ†ç»„ï¼Œé€‰å‡ºå¾—åˆ†æœ€é«˜çš„å“åº”
    df_best = df_all.loc[df_all.groupby("prompt_id")["score_total"].idxmax()].copy()
    df_best = df_best.rename(columns={"response": "best_response"})
    
    # ä¿å­˜ç»“æœ
    df_best.to_csv(output_path, index=False)
    print(f"âœ… Best responses saved to: {output_path}")
    print(f"ï¿½ï¿½ Selected {len(df_best)} best responses from {len(df_all)} total responses")

def main():
    """ä¸»å‡½æ•°"""
    # è®¾ç½®éšæœºç§å­
    set_seed(42)
    
    # è®¾ç½®è·¯å¾„
    base_dir = "/root/rps/data/dpo_rps_generated"
    output_dir = "/root/rps/data"
    
    # æ£€æŸ¥è¾“å…¥ç›®å½•æ˜¯å¦å­˜åœ¨
    if not os.path.exists(base_dir):
        print(f"âŒ Input directory not found: {base_dir}")
        sys.exit(1)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    # è¦å¤„ç†çš„æ–¹å‘
    directions = ["v3", "v4", "v5", "v6", "v7", "v8", "v9", "v10"]
    
    print("ğŸš€ Starting DPO RPS scoring and best response selection")
    print(f"ï¿½ï¿½ Input directory: {base_dir}")
    print(f"ğŸ“‚ Output directory: {output_dir}")
    print(f"ğŸ¯ Directions to process: {directions}")
    
    start_time = time.time()
    
    for i, direction in enumerate(directions, 1):
        print(f"\n{'='*60}")
        print(f"ğŸ“Š Processing direction {i}/{len(directions)}: {direction}")
        print(f"{'='*60}")
        
        input_dir = os.path.join(base_dir, direction)
        if not os.path.exists(input_dir):
            print(f"âš ï¸ Directory not found: {input_dir}, skipping...")
            continue
        
        scored_dir = os.path.join(input_dir, "scored")
        
        try:
            # Step 1: æ‰“åˆ†
            print(f"ğŸ¯ Step 1: Scoring responses for {direction}")
            run_reward_scoring(input_dir, scored_dir)
            
            # Step 2: é€‰å‡ºbest response
            print(f"ğŸ† Step 2: Selecting best responses for {direction}")
            output_path = os.path.join(output_dir, f"{direction}_best_response.csv")
            select_best_response(scored_dir, output_path)
            
            print(f"âœ… Completed processing {direction}")
            
        except Exception as e:
            print(f"âŒ Error processing {direction}: {e}")
            continue
    
    end_time = time.time()
    total_time = end_time - start_time
    
    print(f"\n{'='*60}")
    print("ğŸ‰ All processing completed!")
    print(f"â±ï¸ Total time: {total_time:.2f} seconds ({total_time/60:.2f} minutes)")
    print(f"ğŸ“‚ Results saved to: {output_dir}")
    
    # åˆ—å‡ºç”Ÿæˆçš„æ–‡ä»¶
    output_files = [f for f in os.listdir(output_dir) if f.endswith("_best_response.csv")]
    print(f"ğŸ“„ Generated {len(output_files)} best response files:")
    for file in sorted(output_files):
        file_path = os.path.join(output_dir, file)
        file_size = os.path.getsize(file_path) / 1024  # KB
        print(f"   - {file} ({file_size:.1f} KB)")

if __name__ == "__main__":
    main()