# dpo_baseline_generation_optimized.py
# ä¼˜åŒ–ç‰ˆæœ¬çš„DPOæ¨¡å‹å“åº”ç”Ÿæˆè„šæœ¬

import os
import numpy as np
import pandas as pd
import math
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification
from tqdm.auto import tqdm
import torch
import time
import random
from concurrent.futures import ThreadPoolExecutor
import gc
from datetime import datetime, timedelta

# ğŸ‡¨ğŸ‡³ è®¾ç½®å›½å†…é•œåƒï¼Œè§£å†³ç½‘ç»œè®¿é—®é—®é¢˜
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
os.environ['HUGGINGFACE_HUB_CACHE'] = '/root/autodl-tmp/hf_cache'
os.environ['HF_HOME'] = '/root/autodl-tmp/hf_cache'
os.environ['TRANSFORMERS_CACHE'] = '/root/autodl-tmp/hf_cache'
os.environ['HF_DATASETS_CACHE'] = '/root/autodl-tmp/hf_cache'

print("ğŸŒ å·²è®¾ç½®Hugging Faceå›½å†…é•œåƒ: https://hf-mirror.com")
print("ğŸ’¾ æ¨¡å‹ç¼“å­˜ç›®å½•: /root/autodl-tmp/hf_cache (150GBæ•°æ®ç›˜)")

# å®šä¹‰v3-v10çš„æ–¹å‘å‘é‡ï¼ˆåŸºäºè®ºæ–‡Tableï¼‰
PREFERENCE_DIRECTIONS = {
    "v3": {"vector": (0.9848, 0.1736), "angle": 10},
    "v4": {"vector": (0.9659, 0.2588), "angle": 15},
    "v5": {"vector": (0.9397, 0.3420), "angle": 20},
    "v6": {"vector": (0.9063, 0.4226), "angle": 25},
    "v7": {"vector": (0.8660, 0.5000), "angle": 30},
    "v8": {"vector": (0.8192, 0.5736), "angle": 35},
    "v9": {"vector": (0.7660, 0.6428), "angle": 40},
    "v10": {"vector": (0.7071, 0.7071), "angle": 45},
}

def setup_environment():
    """è®¾ç½®ç¯å¢ƒå’Œéšæœºç§å­"""
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ”§ Using device: {device}")
    
    # è®¾ç½®éšæœºç§å­
    seed = 42
    np.random.seed(seed)
    random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    
    return device

def load_models_optimized(device):
    """ä¼˜åŒ–çš„æ¨¡å‹åŠ è½½ï¼Œå‡å°‘å†…å­˜å ç”¨"""
    cache_dir = '/root/autodl-tmp/hf_cache'
    print(f"ğŸ¤– Loading DPO model from mirror... (cache: {cache_dir})")
    
    try:
        # ğŸ”§ ä¼˜åŒ–æ¨¡å‹åŠ è½½ï¼Œä½¿ç”¨æ›´æ¿€è¿›çš„é‡åŒ–
        dpo_model = AutoModelForCausalLM.from_pretrained(
            "stabilityai/stablelm-zephyr-3b",
            torch_dtype=torch.float16,
            load_in_4bit=True,  # ğŸ”§ ä½¿ç”¨4bité‡åŒ–è¿›ä¸€æ­¥å‡å°‘å†…å­˜
            device_map="auto",
            trust_remote_code=True,
            cache_dir=cache_dir,
            low_cpu_mem_usage=True,
            use_cache=True,  # ğŸ”§ å¯ç”¨KVç¼“å­˜
        )
        print("âœ… DPO model loaded successfully with 4bit quantization!")
        
        dpo_tokenizer = AutoTokenizer.from_pretrained(
            "stabilityai/stablelm-zephyr-3b",
            trust_remote_code=True,
            cache_dir=cache_dir
        )
        dpo_tokenizer.padding_side = "left"
        
        if dpo_tokenizer.pad_token_id is None:
            if hasattr(dpo_tokenizer, 'unk_token') and dpo_tokenizer.unk_token is not None:
                dpo_tokenizer.pad_token = dpo_tokenizer.unk_token
            else:
                dpo_tokenizer.add_special_tokens({'pad_token': '<|pad|>'})
                dpo_model.resize_token_embeddings(len(dpo_tokenizer))
        print("âœ… DPO tokenizer loaded successfully!")
        
    except Exception as e:
        print(f"âŒ Error loading DPO model: {e}")
        raise e
    
    # ğŸ”§ æ¸…ç†GPUç¼“å­˜
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    print("ğŸ† Loading Reward model from mirror...")
    try:
        # ğŸ”§ Rewardæ¨¡å‹å¼ºåˆ¶æ”¾åœ¨CPUä¸Šï¼Œå‡å°‘GPUå†…å­˜å‹åŠ›
        reward_model = AutoModelForSequenceClassification.from_pretrained(
            "Haoxiang-Wang/RewardModel-Mistral-7B-for-DPA-v1",
            trust_remote_code=True,
            cache_dir=cache_dir,
            torch_dtype=torch.float32,
            device_map="cpu",  # ğŸ”§ å¼ºåˆ¶CPU
            low_cpu_mem_usage=True,
        )
        print("âœ… Reward model loaded on CPU!")
        
        reward_tokenizer = AutoTokenizer.from_pretrained(
            "Haoxiang-Wang/RewardModel-Mistral-7B-for-DPA-v1",
            trust_remote_code=True,
            cache_dir=cache_dir
        )
        print("âœ… Reward tokenizer loaded successfully!")
        
    except Exception as e:
        print(f"âŒ Error loading Reward model: {e}")
        raise e
    
    return dpo_model, dpo_tokenizer, reward_model, reward_tokenizer

def build_dpa_input(prompt, v1, v2):
    """æ„é€ DPAæ¨¡å‹çš„è¾“å…¥æ ¼å¼"""
    h = int(np.round(v1 * 100))
    v = int(np.round(v2 * 100))
    sys_instruction = f"You are a helpful assistant. Your response should maximize weighted rating = helpfulness*{h} + verbosity*{v}."
    
    return [{"role": "user", "content": f"{sys_instruction}\n\n{prompt}"}]

def generate_responses_batch_optimized(prompts, prompt_ids, direction_name, direction_info, 
                                     dpo_model, dpo_tokenizer, device, num_responses=3, max_batch_size=4):
    """ğŸ”§ ä¼˜åŒ–çš„æ‰¹é‡ç”Ÿæˆï¼ŒçœŸæ­£ä½¿ç”¨æ‰¹å¤„ç†"""
    try:
        v1, v2 = direction_info["vector"]
        angle = direction_info["angle"]
        
        all_responses = []
        
        # ğŸ”§ åˆ†æ‰¹å¤„ç†ï¼Œé¿å…æ˜¾å­˜æº¢å‡º
        for batch_start in range(0, len(prompts), max_batch_size):
            batch_end = min(batch_start + max_batch_size, len(prompts))
            batch_prompts = prompts[batch_start:batch_end]
            batch_ids = prompt_ids[batch_start:batch_end]
            
            print(f"    ğŸ”„ Processing micro-batch {batch_start//max_batch_size + 1}, prompts {batch_start}-{batch_end-1} ({len(batch_prompts)} prompts)")
            
            # ğŸ”§ ä¸ºæ¯ä¸ªresponseç”Ÿæˆå•ç‹¬å¤„ç†ï¼ˆå› ä¸ºä¸åŒçš„éšæœºæ€§ï¼‰
            for resp_idx in range(num_responses):
                print(f"      ğŸ¯ Generating response {resp_idx + 1}/{num_responses} for {len(batch_prompts)} prompts")
                
                # æ‰¹é‡æ„å»ºè¾“å…¥
                batch_messages = [build_dpa_input(prompt, v1, v2) for prompt in batch_prompts]
                batch_inputs = []
                
                for messages in batch_messages:
                    tokenized = dpo_tokenizer.apply_chat_template(
                        messages, add_generation_prompt=True, return_tensors="pt",
                        padding=True, return_attention_mask=True, truncation=True
                    )
                    batch_inputs.append(tokenized)
                
                # ğŸ”§ çœŸæ­£çš„æ‰¹å¤„ç†ï¼šå°†æ‰€æœ‰è¾“å…¥åˆå¹¶
                if batch_inputs:
                    # åˆå¹¶input_idså’Œattention_mask
                    input_ids_list = [inp['input_ids'] if isinstance(inp, dict) else inp for inp in batch_inputs]
                    attention_masks = [inp.get('attention_mask') if isinstance(inp, dict) else None for inp in batch_inputs]
                    
                    # Padåˆ°ç›¸åŒé•¿åº¦
                    max_len = max(ids.shape[1] for ids in input_ids_list)
                    padded_input_ids = []
                    padded_attention_masks = []
                    
                    for i, ids in enumerate(input_ids_list):
                        pad_len = max_len - ids.shape[1]
                        padded_ids = torch.nn.functional.pad(ids, (pad_len, 0), value=dpo_tokenizer.pad_token_id)
                        padded_input_ids.append(padded_ids)
                        
                        if attention_masks[i] is not None:
                            padded_mask = torch.nn.functional.pad(attention_masks[i], (pad_len, 0), value=0)
                        else:
                            padded_mask = torch.ones_like(padded_ids)
                        padded_attention_masks.append(padded_mask)
                    
                    # åˆå¹¶ä¸ºbatch
                    batch_input_ids = torch.cat(padded_input_ids, dim=0).to(device)
                    batch_attention_mask = torch.cat(padded_attention_masks, dim=0).to(device)
                    
                    # ğŸ”§ æ‰¹é‡ç”Ÿæˆ
                    with torch.no_grad():
                        batch_outputs = dpo_model.generate(
                            input_ids=batch_input_ids,
                            attention_mask=batch_attention_mask,
                            max_new_tokens=2048,  # ğŸ”§ ä¸è®ºæ–‡å®éªŒä¿æŒä¸€è‡´
                            temperature=0.7,
                            do_sample=True,
                            pad_token_id=dpo_tokenizer.pad_token_id,
                            eos_token_id=dpo_tokenizer.eos_token_id,
                            use_cache=True,  # ğŸ”§ ä½¿ç”¨KVç¼“å­˜
                        )
                    
                    # è§£ç ç»“æœ
                    for i, output in enumerate(batch_outputs):
                        original_len = batch_input_ids[i].shape[0]
                        generated_tokens = output[original_len:]
                        decoded = dpo_tokenizer.decode(generated_tokens, skip_special_tokens=True)
                        
                        all_responses.append({
                            "prompt_id": batch_ids[i],
                            "prompt": batch_prompts[i],
                            "direction_name": direction_name,
                            "direction_vector": f"({v1:.4f}, {v2:.4f})",
                            "angle_degrees": angle,
                            "response_id": resp_idx + 1,
                            "response": decoded
                        })
                    
                    # ğŸ”§ æ¸…ç†æ˜¾å­˜
                    del batch_outputs, batch_input_ids, batch_attention_mask
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
        
        return all_responses
    
    except Exception as e:
        print(f"âš ï¸ Error in batch generation: {e}")
        return []

def score_responses_batch(responses, reward_model, reward_tokenizer):
    """ğŸ”§ æ‰¹é‡è¯„åˆ†å“åº”"""
    try:
        scored_responses = []
        
        # ğŸ”§ æ‰¹é‡è¯„åˆ†ï¼Œå‡å°‘æ¨¡å‹è°ƒç”¨æ¬¡æ•°
        batch_size = 8  # reward modelæ‰¹å¤„ç†å¤§å°
        
        for i in range(0, len(responses), batch_size):
            batch_responses = responses[i:i + batch_size]
            
            # å‡†å¤‡æ‰¹é‡è¾“å…¥
            batch_texts = []
            for resp in batch_responses:
                template = "[INST] You must read the following conversation carefully and rate the assistant's response from score 0-100 in these aspects: helpfulness, correctness, coherence, honesty, complexity, verbosity\n\nUser: {prompt}\n\nAssistant: {response} [/INST]"
                batch_texts.append(template.format(prompt=resp["prompt"], response=resp["response"]))
            
            # æ‰¹é‡tokenize
            inputs = reward_tokenizer(
                batch_texts,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=2048
            )
            
            # ç§»åŠ¨åˆ°reward modelçš„è®¾å¤‡
            model_device = next(reward_model.parameters()).device
            inputs = inputs.to(model_device)
            
            # æ‰¹é‡æ¨ç†
            with torch.no_grad():
                logits = reward_model(**inputs).logits.cpu().numpy()
            
            # å¤„ç†ç»“æœ
            for j, resp in enumerate(batch_responses):
                resp_logits = logits[j]
                helpfulness = resp_logits[9]  # r_help(x, y)
                verbosity = resp_logits[4]    # r_verb(x, y)
                
                # è·å–directionå‘é‡
                direction_vector = resp["direction_vector"]
                v1_str, v2_str = direction_vector.strip("()").split(", ")
                v1, v2 = float(v1_str), float(v2_str)
                
                # DPAå¥–åŠ±å‡½æ•°
                dpa_score = v1 * helpfulness + v2 * verbosity
                
                resp.update({
                    "helpfulness": float(helpfulness),
                    "verbosity": float(verbosity),
                    "v1": float(v1),
                    "v2": float(v2),
                    "dpa_score": float(dpa_score)
                })
                
                scored_responses.append(resp)
        
        return scored_responses
    
    except Exception as e:
        print(f"âš ï¸ Error in batch scoring: {e}")
        return responses  # è¿”å›æœªè¯„åˆ†çš„responses

def select_best_responses(scored_responses):
    """ğŸ”§ é€‰æ‹©æ¯ä¸ªpromptçš„æœ€ä½³å“åº”"""
    # æŒ‰prompt_idåˆ†ç»„
    from collections import defaultdict
    prompt_groups = defaultdict(list)
    
    for resp in scored_responses:
        prompt_groups[resp["prompt_id"]].append(resp)
    
    best_responses = []
    for prompt_id, responses in prompt_groups.items():
        if responses:
            # é€‰æ‹©DPAå¾—åˆ†æœ€é«˜çš„
            best_response = max(responses, key=lambda x: x["dpa_score"])
            best_response["selected_as_best"] = True
            best_response["all_dpa_scores"] = [r["dpa_score"] for r in responses]
            best_response["num_candidates"] = len(responses)
            best_responses.append(best_response)
    
    return best_responses

# ä¿®å¤åçš„è¿›åº¦æ¡é€»è¾‘
def process_direction_optimized(direction_name, direction_info, prompts, prompt_ids, 
                              dpo_model, dpo_tokenizer, reward_model, reward_tokenizer, 
                              device, output_dir, batch_size=8, num_responses=3):
    """ğŸ”§ ä¼˜åŒ–çš„å•æ–¹å‘å¤„ç†"""
    print(f"\nğŸ¯ Processing direction {direction_name}: {direction_info['vector']} ({direction_info['angle']}Â°)")
    
    output_file = os.path.join(output_dir, f"dpo_responses_{direction_name}.csv")
    
    # æ£€æŸ¥å·²æœ‰ç»“æœ
    done_prompt_ids = set()
    if os.path.exists(output_file):
        try:
            existing_df = pd.read_csv(output_file)
            done_prompt_ids = set(existing_df["prompt_id"].unique())
            print(f"ğŸ” Found existing results for {len(done_prompt_ids)} prompts in {direction_name}")
        except Exception as e:
            print(f"âš ï¸ Error loading existing file for {direction_name}: {e}")
    
    # è®¡ç®—å‰©ä½™prompts
    remaining_indices = [i for i, pid in enumerate(prompt_ids) if pid not in done_prompt_ids]
    remaining_prompts = [prompts[i] for i in remaining_indices]
    remaining_ids = [prompt_ids[i] for i in remaining_indices]
    
    if not remaining_prompts:
        print(f"âœ… {direction_name} already complete!")
        return
    
    print(f"ğŸ“Š {direction_name}: å·²å¤„ç† {len(done_prompt_ids)} ä¸ªï¼Œå‰©ä½™ {len(remaining_prompts)} ä¸ª")
    
    # ğŸ”§ åˆ†æ‰¹å¤„ç†
    all_direction_results = []
    total_batches = (len(remaining_prompts) + batch_size - 1) // batch_size
    direction_start_time = time.time()
    
    # ğŸ•’ æ­£ç¡®çš„è¿›åº¦æ¡åˆ›å»º - å®Œå…¨ä¿®å¤ç‰ˆæœ¬
    pbar = tqdm(total=total_batches, 
                desc=f"ğŸ“Š {direction_name}",
                unit="batch",
                bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} batches [{elapsed}<{remaining}, {rate_fmt}] {postfix}")
    
    # ğŸ”§ æ‰‹åŠ¨æ§åˆ¶æ‰¹å¤„ç†å¾ªç¯
    for batch_idx in range(total_batches):
        start = batch_idx * batch_size
        end = min(start + batch_size, len(remaining_prompts))
        batch_prompts = remaining_prompts[start:end]
        batch_ids = remaining_ids[start:end]
        
        # ğŸ”§ æ‰¹é‡ç”Ÿæˆå“åº”
        responses = generate_responses_batch_optimized(
            batch_prompts, batch_ids, direction_name, direction_info,
            dpo_model, dpo_tokenizer, device, num_responses, max_batch_size=2
        )
        
        if not responses:
            pbar.update(1)  # ğŸ”§ å³ä½¿å¤±è´¥ä¹Ÿè¦æ›´æ–°è¿›åº¦æ¡
            continue
        
        # ğŸ”§ æ‰¹é‡è¯„åˆ†
        scored_responses = score_responses_batch(responses, reward_model, reward_tokenizer)
        
        # ğŸ”§ é€‰æ‹©æœ€ä½³å“åº”
        best_responses = select_best_responses(scored_responses)
        
        # ä¿å­˜æ‰¹å¤„ç†ç»“æœ
        if best_responses:
            df_batch = pd.DataFrame(best_responses)
            
            if not os.path.exists(output_file):
                df_batch.to_csv(output_file, index=False)
            else:
                df_batch.to_csv(output_file, mode='a', header=False, index=False)
            
            all_direction_results.extend(best_responses)
            progress_prompts = len(done_prompt_ids) + len(all_direction_results)
            
            # ğŸ•’ è®¡ç®—æ—¶é—´ç»Ÿè®¡
            elapsed_time = time.time() - direction_start_time
            avg_time_per_batch = elapsed_time / (batch_idx + 1)
            remaining_batches = total_batches - (batch_idx + 1)
            estimated_remaining = avg_time_per_batch * remaining_batches
            
            # ğŸ”§ æ­£ç¡®æ›´æ–°è¿›åº¦æ¡
            pbar.set_postfix({
                'prompts': f"{progress_prompts}/2000",
                'avg_time': f"{avg_time_per_batch:.1f}s/batch",
                'eta': f"{timedelta(seconds=int(estimated_remaining))}" if estimated_remaining > 0 else "0:00:00"
            })
        
        # ğŸ”§ æ‰‹åŠ¨æ›´æ–°è¿›åº¦æ¡
        pbar.update(1)
        
        # ğŸ”§ å¼ºåˆ¶åƒåœ¾å›æ”¶
        del responses, scored_responses, best_responses
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
    
    # å…³é—­è¿›åº¦æ¡
    pbar.close()
    
    print(f"âœ… Completed direction {direction_name}: {len(all_direction_results)} responses")
    return all_direction_results

def main():
    """ä¸»å‡½æ•°"""
    # è®¾ç½®è¾“å‡ºç›®å½•
    result_dir = "/root/rps/data/dpo_outputs"
    os.makedirs(result_dir, exist_ok=True)
    
    # æ£€æŸ¥å·²æœ‰æ•°æ®
    existing_files = []
    for direction_name in PREFERENCE_DIRECTIONS.keys():
        output_file = os.path.join(result_dir, f"dpo_responses_{direction_name}.csv")
        if os.path.exists(output_file):
            try:
                df = pd.read_csv(output_file)
                existing_files.append((direction_name, len(df)))
                print(f"ğŸ“ Found existing data for {direction_name}: {len(df)} responses")
            except Exception as e:
                print(f"âš ï¸ Error reading {direction_name}: {e}")
    
    if existing_files:
        print(f"\nâœ… Found existing data in {result_dir}")
        print(f"ğŸ“Š Summary:")
        total_responses = 0
        for direction_name, count in existing_files:
            print(f"  {direction_name}: {count} responses")
            total_responses += count
        print(f"  Total: {total_responses} responses across {len(existing_files)} directions")
        
        user_input = input("\nğŸ¤” Data already exists. Continue anyway? (y/N): ").strip().lower()
        if user_input != 'y':
            print("ğŸ›‘ Stopping execution.")
            return
    
    # è®¾ç½®ç¯å¢ƒ
    device = setup_environment()
    
    # åŠ è½½æ•°æ®é›†
    print("ğŸ“¦ Loading prompts from UltraFeedback...")
    try:
        ds = load_dataset("HuggingFaceH4/ultrafeedback_binarized", split="test_prefs")
        prompts = ds["prompt"][:2000]  # ğŸ”§ å¯ä»¥è°ƒæ•´æ•°é‡è¿›è¡Œæµ‹è¯•
        prompt_ids = list(range(len(prompts)))
        print(f"âœ… Loaded {len(prompts)} prompts successfully!")
    except Exception as e:
        print(f"âŒ Error loading dataset: {e}")
        return
    
    # ğŸ”§ ä¼˜åŒ–çš„æ¨¡å‹åŠ è½½
    dpo_model, dpo_tokenizer, reward_model, reward_tokenizer = load_models_optimized(device)
    
    # æ˜¾ç¤ºå¤„ç†è®¡åˆ’
    print(f"\nğŸ“ Will process {len(PREFERENCE_DIRECTIONS)} directions:")
    for name, info in PREFERENCE_DIRECTIONS.items():
        print(f"  {name}: {info['vector']} ({info['angle']}Â°)")
    
    # ğŸ”§ æŒ‰æ–¹å‘ä¸²è¡Œå¤„ç†ï¼Œä½†å†…éƒ¨æ‰¹é‡åŒ–
    start_time = time.time()
    total_directions = len(PREFERENCE_DIRECTIONS)
    
    print(f"\nğŸš€ Starting generation for {len(prompts)} prompts across {total_directions} directions...")
    print(f"â° Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    for i, (direction_name, direction_info) in enumerate(PREFERENCE_DIRECTIONS.items(), 1):
        direction_start = time.time()
        print(f"\nğŸŒŸ ã€{i}/{total_directions}ã€‘ Starting direction {direction_name}")
        
        process_direction_optimized(
            direction_name=direction_name,
            direction_info=direction_info,
            prompts=prompts,
            prompt_ids=prompt_ids,
            dpo_model=dpo_model,
            dpo_tokenizer=dpo_tokenizer,
            reward_model=reward_model,
            reward_tokenizer=reward_tokenizer,
            device=device,
            output_dir=result_dir,
            batch_size=16,  # ğŸ”§ å¢å¤§æ‰¹å¤„ç†å¤§å°
            num_responses=3
        )
        
        # ğŸ•’ è®¡ç®—å¹¶æ˜¾ç¤ºæ•´ä½“è¿›åº¦
        direction_elapsed = time.time() - direction_start
        total_elapsed = time.time() - start_time
        avg_time_per_direction = total_elapsed / i
        remaining_directions = total_directions - i
        estimated_total_remaining = avg_time_per_direction * remaining_directions
        
        print(f"âœ… Direction {direction_name} completed in {timedelta(seconds=int(direction_elapsed))}")
        print(f"ğŸ“Š Overall progress: {i}/{total_directions} directions ({i/total_directions*100:.1f}%)")
        if remaining_directions > 0:
            print(f"â±ï¸  Estimated remaining time: {timedelta(seconds=int(estimated_total_remaining))}")
            completion_time = datetime.now() + timedelta(seconds=int(estimated_total_remaining))
            print(f"ğŸ¯ Estimated completion: {completion_time.strftime('%Y-%m-%d %H:%M:%S')}")
    
    elapsed_time = time.time() - start_time
    print(f"\nğŸ All directions completed in {elapsed_time:.1f} seconds")
    
    # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
    print(f"ğŸ“ˆ Final statistics:")
    total_responses = 0
    for direction_name in PREFERENCE_DIRECTIONS.keys():
        output_file = os.path.join(result_dir, f"dpo_responses_{direction_name}.csv")
        if os.path.exists(output_file):
            df = pd.read_csv(output_file)
            print(f"  {direction_name}: {len(df)} responses, avg DPA score: {df['dpa_score'].mean():.2f}")
            total_responses += len(df)
    
    print(f"ğŸ“Š Total responses: {total_responses}")
    print(f"âš¡ Average time per response: {elapsed_time/total_responses:.2f}s")

if __name__ == "__main__":
    main() 